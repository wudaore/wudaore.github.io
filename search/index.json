[{"content":"目录 PCA基本原理 PCA数学推导 PCA代码实现 PCA接口实现 总结 ps. 1. PCA基本原理 太高维度的数据处理时会大大增加计算难度和时间.所以必要时需要对特征进行降维.\nPCA，主成分分析 是一种数据降维的方法.\n首先，拿二维数据距离.对于一批数据点，如果其在x和y轴上的分布比较均匀，那么就难以舍去一些特征，做到降维了.此时要做的就是沿着覆盖数据的最大方差位置旋转坐标轴.\n如下图所示，经过旋转后，y轴的分散程度大大提高，对数据有更好的识别能力\n旋转前\n旋转后\n接下来要开始降维.对于输入的数据集矩阵，设为Xm*n,即m条数据，有n个维度.现在想要将它降到K维.设降维后的矩阵为Zm*k\n要实现上述降维，只需要让X乘以一个n*k的矩阵即可.设该矩阵为Wn*k\nZm*k = Xm*n * Wn*k\n接下来就是一系列数学优化过程.详情跳转\n2. PCA数学推导 对于新的坐标轴 Z ，W的取值要让其方差最大化，也即数据最分散，提高识别度.\n这里需要限制W为单位向量，即||W|| = 1.这么做是为了后面的优化.不然直接取W等于无穷大就能满足最大了 .没有实际意义.\n要求最大值，可以用我们数值的拉格朗日乘子法求解.\n求解的过程涉及二次型求导，点击查看证明\n最终得到解：\n令Cov(x) = S，SW−αW=0正好是特征值的定义.\n接着推，可知，我们需要的是最大的（若干个）特征值.\n最后得出PCA的基本步骤：\n整理原始矩阵Xm*n ​ 求原始矩阵X的协防差阵S=Cov(X)\n求解协防差阵的特征值和特征向量。\n选取最大的K(人为给定)个特征值所对应的特征向量组成构成矩阵Wn*k ​ 直接进行矩阵计算 Z = XW\n3. PCA代码实现 # lowDDataMat:降维后的数据, reconMat：重构后的原始数据\rdef pca(dataMat, topNfeat=9999999):\rmeanVals = mean(dataMat, axis=0)\rmeanRemoved = dataMat - meanVals # 计算并减去原始数据集的平均值\rcovMat = cov(meanRemoved, rowvar=0)# 计算协方差矩阵\reigVals,eigVects = linalg.eig(mat(covMat))# 计算特征值和特征向量\r# print('特征值')\r# print(eigVals)\r# print('特征向量')\r# print(eigVects)\reigValInd = argsort(eigVals) # 根据特征值排序结果的逆序就可以得到topNfeat个最大的特征向量.这里得到的是下标\r# print('eigValInd is',eigValInd)\reigValInd = eigValInd[:-(topNfeat+1):-1] # 去掉不需要的数据\r# print('val after dele', eigValInd)\rredEigVects = eigVects[:,eigValInd] # 重组,得到N个最大的特征向量.reorganize eig vects largest to smallest\r# print(redEigVects)\rlowDDataMat = meanRemoved * redEigVects# 将数据转换为新的维度\rreconMat = (lowDDataMat * redEigVects.T) + meanVals\rreturn lowDDataMat, reconMat\r 书上的代码先将数据减去了均值，最后又将重构的数据加上了均值\n4. PCA接口实现 详见之前的博客\n5. 总结 PCA其实是有很多限制的。比如，可以做PCA降维的前提必须保证数据是线性分布的，如果数据不是线性的，就不适用.\n6. p.s. 参考文章\n","date":"2022-06-24","permalink":"https://wudaore.github.io/post/pca/","tags":["机器学习","algo"],"title":"特征降维-主成分分析PCA"},{"content":"目录 FP树基本原理 FP树源码 总结 ps. 1. FP树基本原理 在寻找频繁项集时，Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁.在数据量大时运行会很慢.\n而FP-groth算法通过构造FP树，只需要扫描数据集两遍即可.FP树如下图所示.\n区别于传统的树，FP树可以出现重复内容，并且对于重复元素，使用“node link”进行连接，方便进行回溯.\n1.1 FP树节点结构 包括节点名，节点次数，linknode（指向同名节点），节点双亲和节点孩子.\n有inc和disp方法.inc方法用于修改节点次数，disp方法则用于打印该节点和该节点的孩子组成的树\n跳转至代码\n1.2 FP树的构建 首先对于所有符合频率要求（达到指定次数）的元素，构建一个头指针表，存放这些元素.头指针表同样需要使用linknode连接树中的同名元素.头指针表需要根据元素频度进行排序\n第一次遍历数据集会获得每个元素项的出现频率。接下来，去掉不满足最小支持度的元素项\n第二次遍历之前，需要对集合项进行排序，让频度高的排在前面.\n然后开始构建FP树.如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝.\n跳转至代码\n1.3 条件模式基 FP树已经构造完毕.那么要如何查看频繁项集呢？引入条件模式基的概念.条件模式基即是以所查找元素项为结尾的路径集合.FP树中加入父亲节点就是为了方便回溯，构造路径.\n跳转至代码\n1.4 条件FP树 光构造FP树并不足以帮助我们获取所有的频繁项集.\n总结一下过程，对于每个元素a的FP树H中的每个元素b都与a的组合频繁，而在a元素对应的H中，有与b频繁的元素c，那么{a，b，c}也频繁。\n同理可以处理c及其他元素。之所以可以这样做是应为元素a与其对应的H中的所有元素都频繁出现，而在该H中，b又与c频繁出现，c在该H中，所以a，b，c频繁。每一个元素对应不同的H，每一次递归时，对应的频繁项集的长度就会增加，H范围会在上一递归层H_old的范围的基础上缩小。\n代码具体过程如下：\n对于每一个频繁项，都要创建一棵条件FP树.某个元素是频繁项，但是在指定元素的条件FP树中，它就不一定是了.\n对于输入的大批量数据，先调用createTree方法构建头结点表和树节点；先将头结点表中的元素按照出现频率从小到大进行排序，然后遍历头节点表，找到它每一个元素的条件模式基.\n而后，将条件基作为数据集输入到建树函数createTree中构建条件树.最后，如果树中有元素项的话，递归调用mineTree()函数.\n跳转至代码\n参考博客\n这篇博客举了树上了例子，很好的帮助我们了解条件FP树\n2. FP树建树，更新树源码 2.1 FP树节点. # 节点值，count变量增加值，父节点\rclass treeNode:\rdef __init__(self, nameValue, numOccur, parentNode):\rself.name = nameValue\rself.count = numOccur\rself.nodeLink = None\rself.parent = parentNode #needs to be updated\rself.children = {} def inc(self, numOccur):\rself.count += numOccur\rdef disp(self, ind=1):\rprint(' '*ind, self.name, ' ', self.count)\rfor child in self.children.values():\rchild.disp(ind+1)\r 2.2 构建FP树. 函数updateTree用于递归增长树，updateHeader将同名元素使用LinkNode连接.主函数createTree的返回值一是树节点，二是头结点表.\ndef createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine\r# print('data is', dataSet)\rheaderTable = {}\r#go over dataSet twice\rfor trans in dataSet:#first pass counts frequency of occurance)\rfor item in trans:\rheaderTable[item] = headerTable.get(item, 0) + dataSet[trans]\r# print('header table is', headerTable)\rfor k in list(headerTable.keys()): #remove items not meeting minSup\rif headerTable[k] \u0026lt; minSup: del(headerTable[k])\rfreqItemSet = set(headerTable.keys())# 过滤后只剩下频繁项\r# print('fre is ', freqItemSet)\r#print 'freqItemSet: ',freqItemSet\rif len(freqItemSet) == 0: return None, None # 没有频繁项 直接结束\rfor k in headerTable:\rheaderTable[k] = [headerTable[k], None]# 将headerTable 用链表节点重新格式化\r# print('uodate table is ', headerTable)# {'z': [5, None], 'r': [3, None],\r# 'y': [3, None], 't': [3, None], 'x': [4, None], 's': [3, None]}\rretTree = treeNode('Null Set', 1, None) # 建树\rfor tranSet, count in dataSet.items(): # 第二遍遍历数据\rlocalD = {}\rfor item in tranSet: # 将事务项按顺序排列\rif item in freqItemSet:\rlocalD[item] = headerTable[item][0]\r# print('localD is', localD)\rif len(localD) \u0026gt; 0:\rorderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)] # 按频率排序.\r# 非常漂亮的列表生成式+lambda\rupdateTree(orderedItems, retTree, headerTable, count)# 用有序的频率项集填充树\rreturn retTree, headerTable #return tree and header table\r# 传入参数：按频率排序以后的列表，树，头指针列表，数据集键值对中的值（\r# 初始都为1，在init函数中设置，也即每次检测到存在就加一）\rdef updateTree(items, inTree, headerTable, count):\r# print('+++')\r# print('item ', items)\r# print('tree', inTree)\r# print('header table', headerTable)\r# print('count',count)\rif items[0] in inTree.children:# 检查 Items[0] 是否在 retTree.children中\rinTree.children[items[0]].inc(count) # 如果在，增加计数\relse: # 如果不在，就将 items[0] 加入到 inTree.children\rinTree.children[items[0]] = treeNode(items[0], count, inTree)\rif headerTable[items[0]][1] == None: # 更新 header table.更新后的格式：{'z': # [5, \u0026lt;__main__.treeNode object at 0x00000208CEE05B20\u0026gt;]}，值中第二个元素是一个树节点\rheaderTable[items[0]][1] = inTree.children[items[0]]\relse: # 如果已经有值了 要确保节点链接指向树中该元素项的每一个实例\rupdateHeader(headerTable[items[0]][1], inTree.children[items[0]])\r# print('second update is ', headerTable)\rif len(items) \u0026gt; 1:# 用剩余的有序项递归调用updateTree()\rupdateTree(items[1::], inTree.children[items[0]], headerTable, count)\r# print('+++')\rdef updateHeader(nodeToTest, targetNode): #对于结点inTree.children[items[0]]和其对应元素,\r# 此时在头指针表里该元素有指针存在（即树中已经有了该元素的存在），把这个元素所在结点和该元素上一次出现的结点“连”起来\rwhile (nodeToTest.nodeLink != None): # nodeLink,是同一元素之间的连接\rnodeToTest = nodeToTest.nodeLink\rnodeToTest.nodeLink = targetNode\r 2.3 条件模式基. 非常易懂的代码，不做过多解释.\ndef ascendTree(leafNode, prefixPath): if leafNode.parent != None:\rprefixPath.append(leafNode.name)\rascendTree(leafNode.parent, prefixPath)\r# 构建条件基\rdef findPrefixPath(basePat, treeNode): #treeNode comes from header table\rcondPats = {}\rwhile treeNode != None:\rprefixPath = []\rascendTree(treeNode, prefixPath)\rif len(prefixPath) \u0026gt; 1: condPats[frozenset(prefixPath[1:])] = treeNode.count\rtreeNode = treeNode.nodeLink\rreturn condPats\r 2.4 条件FP树. def mineTree(inTree, headerTable, minSup, preFix, freqItemList):\rbigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]#(sort header table)\r# print('bigL is ', bigL)\rfor basePat in bigL: # 从标题表的底部开始\rnewFreqSet = preFix.copy()\rnewFreqSet.add(basePat)\r# print('finalFrequent Item: ',newFreqSet)\rfreqItemList.append(newFreqSet)\r# print('freqItemList is', freqItemList)\rcondPattBases = findPrefixPath(basePat, headerTable[basePat][1])\r# print('condPattBases :',basePat, condPattBases)\r#2. construct cond FP-tree from cond. pattern base\rmyCondTree, myHead = createTree(condPattBases, minSup) #将条件基设为数据集输入到建树函数中.因为如果某元素是频繁的，那么它的条件FP树上的元素和它的组合也是频繁的.而不在该树上的元素就不一定了\r# print('head from conditional tree: ', myHead)\rif myHead != None: #3. mine cond. FP-tree\r# print('conditional tree for: ',newFreqSet)\rmyCondTree.disp(1)\r# 如若树中有元素项，则递归调用\rmineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\r 3. 总结 FP-growth算法只需要两遍遍历数据集\u0026ndash;第一遍获取元素频率，过滤不符合要求的元素；第二遍直接构建FP树，速度快于Apriori算法.\n为了和Apriori算法一样获取频繁项集，FP-growth需要对于每个频繁元素，获取它的条件模式基和它条件模式基的条件模式基\u0026hellip;..如此迭代.原理是对于某个频繁元素，其条件FP树上与该元素的组合也是频繁的.当然，条件FP树需要再一次过滤频率不符合的元素.因为对于整体频繁，对于某元素不一定频繁.\n4. p.s. dict.get(key, default=None)作用是返回字典中键的值，如若不存在则返回默认值.\nsorted的key参数可以传入一个函数.使用lambda更方便.示例\n","date":"2022-06-23","permalink":"https://wudaore.github.io/post/fp-growth/","tags":["机器学习","algo"],"title":"FP-growth算法"},{"content":"目录 Apriori关联分析基本原理 关联分析《实战》源码 关联规则基本原理 关联规则《实战》源码 总结 ps. 1. Apriori关联分析基本原理 频繁项集是指那些经常出现在一起的物品集合.使用频繁项集和关联规则，商家可以更好地理解他们的顾客.\n**支持度（support）**被定义为数据集中包含该项集的记录所占的比例\n**可信度或置信度（confidence）**针对一条关联规则来定义.\n支持度和可信度是用来量化关联分析是否成功的方法.\n当物品数量多时，针对每一种组合，统计其出现的频繁程度速度很慢.比如对于N种物品，有2n-1种项集组合.当N很大时遍历穷举显然不合适.此时引出Apriori关联分析减少计算量\n在学习Apriori算法之前，需要知道Apriori原理.也即，某个项集是频繁的，那么它的所有子集也是频繁的.逆否定理即子集不频繁，项集频繁.这就可以通过子集来对超集进行剪枝.\n算法流程如下：\n当集合中项的个数大于0时\n构建一个k个项组成的候选项集的列表\r检查数据以确认每个项集都是频繁的\r保留频繁项集并构建k+1项组成的候选项集的列表\r 2. 关联分析《实战》源码 首先需要创建一个辅助函数扫描数据集\n# 构建集合C1\rdef createC1(dataSet):\rC1 = []\rfor transaction in dataSet:\rfor item in transaction:\rif not [item] in C1:\rC1.append([item])\rC1.sort()\rreturn list(map(frozenset, C1)) # 需要使用不可修改的frozen set # 扫描\rdef scanD(D, Ck, minSupport):\r# ssCnt实际上就是统计D中的每个值在ck中出现的次数\r# retList是筛选以后的支持度达到最小支持度的Ck列表\r# supportData是ck中每一个值出现的频率，也即支持度\rssCnt = {}\rfor tid in D:\rfor can in Ck:\rif can.issubset(tid): # 如果ck中的元素都包括在D中\rif not can in ssCnt:\rssCnt[can]=1\relse:\rssCnt[can] += 1\rnumItems = float(len(D))\rretList = []\rsupportData = {}\rfor key in ssCnt:\rsupport = ssCnt[key]/numItems\rif support \u0026gt;= minSupport:\rretList.insert(0,key)\rsupportData[key] = support\rreturn retList, supportData\r createC1构建了C1，也即大小为1的所有候选项集的集合，格式为{[1],[2]\u0026hellip;}\n而scanD有三个输入参数，分别是数据集、候选项集列表Ck以及感兴趣项集的最小支持度minSupport.该函数的核心作用就是筛选达到最小支持度的Ck列表，并返回ck中每一个值出现的频率，也即支持度.\n然后是Apriori算法的具体实现.Apriori算法的优化之处就在于对于支持度低的子集，舍弃其超集\ndef aprioriGen(Lk, k): #creates Ck\rretList = []\rlenLk = len(Lk)\rfor i in range(lenLk):\rfor j in range(i+1, lenLk):\r# print('+++')\rL1 = list(Lk[i])[:k-2]\rL2 = list(Lk[j])[:k-2]\rL1.sort(); L2.sort()\r# print(Lk)\r# print(k)\r# print(L1)\r# print(L2)\r# print('+++')\rif L1==L2: # 输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck\rretList.append(Lk[i] | Lk[j]) # 合并两个集合\r# print('ret is', retList)\rreturn retList\r# 获得频繁集项和其支持度\rdef apriori(dataSet, minSupport = 0.5):\rC1 = createC1(dataSet)\rD = list(map(set, dataSet))\rL1, supportData = scanD(D, C1, minSupport)\rL = [L1]\r# print('L is', L)\rk = 2\rwhile (len(L[k-2]) \u0026gt; 0):\rCk = aprioriGen(L[k-2], k)\r# print('-----')\r# print(L[k-2])\r# print(Ck)\r# print('-----')\rLk, supK = scanD(D, Ck, minSupport)# Ck是候选集列表.需要通过scanD来筛选掉频率低于最小支持度的项\rsupportData.update(supK)\rL.append(Lk)# 低位集合不频繁那么它的超集一定不频繁.L中只会添加频繁的集合\rk += 1\rreturn L, supportData\r 厘清CK，LK，supportData和L的关系.\nCK是将L输入到aprioriGen后得出的产物，其含义是输入元素的各种合并后的项集.AprioriGen函数的输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck.“输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck”的思想有效避免了计算的过度繁杂.\nLK是将CK输入到scanD后得出的产物.其含义是上述项集后筛选掉支持度小于最小支持度后的结果.\nsupportData是每次scanD返回的supportData的合集.里面存放每一个组合的支持度.\nL初始只有L1（即初始数据集扫描以后）中的内容.随着迭代不断进行，L中只会添加频繁的集合，直到达到迭代结束的条件.\n总的来说，apriori就是不断迭代数据，每次迭代产生一系列项集，筛选掉不频繁的，留下频繁的项集就是apriori的输出结果.\n3. 关联规则基本原理 关联规则，差不多可以理解为某个元素或者某个元素集合是否可能会推导出另一个元素.\n一条规则P ➞ H的可信度定义为support(P | H)/support(P)，其中(P | H)的含义是P和H的并集.在构造关联规则时，我们先生成一个可能的规则列表，然后测试每条规则的可信度。如果可信度不满足最小要求，则去掉该规则.\n与关联分析类似，如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求，这可以大大减少计算量.\n4.关联规则《实战》源码 # 频繁项集列表、包含那些频繁项集支持数据的字典、最小可信度阈值\rdef generateRules(L, supportData, minConf=0.7): #supportData is a dict coming from scanD\rbigRuleList = []\r# print('L is', L)\r# print('supp is,', suppData)\r# print('conf is', minConf)\rfor i in range(1, len(L)):# 只获取有两个或更多元素的集合.L[0]都只有一个元素 所以被舍弃\rfor freqSet in L[i]:\rH1 = [frozenset([item]) for item in freqSet]\r# print('---')\r# print('h1 is', H1)\r# print('freqSet is', freqSet)\rif (i \u0026gt; 1):\rrulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) # freqSet：频繁项集；H1：可以出现在规则右部的元素列表\relse:\rcalcConf(freqSet, H1, supportData, bigRuleList, minConf)\rreturn bigRuleList # 对规则进行评估\rdef calcConf(freqSet, H, supportData, brl, minConf=0.7):\rprunedH = [] #create new list to return\rfor conseq in H:\rconf = supportData[freqSet]/supportData[freqSet-conseq] # 计算可信度（一条规则P ➞ H的可信度定义为support(P | H)/support(P)。）\rif conf \u0026gt;= minConf: # 如果某条规则满足最小可信度值，那么将这些规则输出到屏幕显示。通过检查的规则也会被返回，并被用在下一个函数rulesFromConseq()中。同时也需要对列表brl进行填充，而brl是前面通过检查的bigRuleList。\rprint(freqSet-conseq,'--\u0026gt;',conseq,'conf:',conf)\rbrl.append((freqSet-conseq, conseq, conf))\rprunedH.append(conseq)\rreturn prunedH\r# 从最初的项集中生成更多的关联规则\rdef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\rm = len(H[0])# m为频繁集大小\r# print('m is', m)\rif (len(freqSet) \u0026gt; (m + 1)): # 频繁集大到可以移除为m的子集\rHmp1 = aprioriGen(H, m+1)# 生成H元素中的无重复组合.hmp1即下一次迭代的H列表，包含所有可能的规则\r# print('hmp1 is', Hmp1)\rHmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)# 所有通过检查的规则\rif (len(Hmp1) \u0026gt; 1): # 如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则。\rrulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\r 三个函数.generateRules是主函数，输入值为频繁项集列表、包含那些频繁项集支持数据的字典、最小可信度阈值.freqSet：频繁项集；H1：可以出现在规则右部的元素列表.\ncalcConf是对规则进行评估的函数.conf = supportData[freqSet]/supportData[freqSet-conseq]，即通过上述公式support(P | H)/support(P)计算可信度.如果某条规则满足最小可信度值，那么将这些规则输出到屏幕显示。通过检查的规则也会被返回，并被用在下一个函数rulesFromConseq()中。同时也需要对列表brl进行填充，而brl是前面通过检查的bigRuleList.\nrulesFromConseq的作用是从最初的项集中生成更多的关联规则.通过aprioriGen生成新的项集，并通过calcConf对项集进行筛选.如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则.\n回到主函数中.因为只获取有两个或更多元素的集合.L[0]都只有一个元素 所以被舍弃，故而循环是从L[1]开始的.从L[1]之后，不断从输入的项集中生成更多的关联规则.如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则.\n5. 总结 Apriori算法可以发现元素间的不同组合，并通过Apriori原理减少计算量.\n但是Apriori算法会重新扫描整个数据集。当数据集很大时，这会显著降低频繁项集发现的速度。\n6. p.s. ","date":"2022-06-22","permalink":"https://wudaore.github.io/post/apriori/","tags":["机器学习","algo"],"title":"Apriori算法实现关联分析"},{"content":"目录 基本原理 《实战》源码 模型评估 算法优化 特征降维 API接口实现 案例-消费预测 总结 ps. 1. 基本原理 原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点为簇的均值点，重新遍历\n2. 《实战》源码 2.1 普通k均值聚类 # 计算欧式距离\rdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)\r# 随机形成质心\rdef randCent(dataSet, k):\rn = shape(dataSet)[1]\rcentroids = mat(zeros((k,n)))#create centroid mat\rfor j in range(n):#create random cluster centers, within bounds of each dimension\rminJ = min(dataSet[:,j])\rrangeJ = float(max(dataSet[:,j]) - minJ)\rcentroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) # 通过最小值+（最大值和最小值的）差值*一个0-1的数的方式，随机生成一个区间内的质心\rreturn centroids\r# k均值算法\rdef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\rm = shape(dataSet)[0]\rclusterAssment = mat(zeros((m,2)))# 簇结果分配矩阵.第一列记录簇索引值，第二列记录误差\rcentroids = createCent(dataSet, k)\rclusterChanged = True\rwhile clusterChanged:\rclusterChanged = False\rfor i in range(m):# 对于每一行（每一个数据），计算距离， 直到不变为止\rminDist = inf; minIndex = -1\rfor j in range(k):\rdistJI = distMeas(centroids[j,:],dataSet[i,:])\rif distJI \u0026lt; minDist:\rminDist = distJI; minIndex = j\rif clusterAssment[i,0] != minIndex: clusterChanged = True\rclusterAssment[i,:] = minIndex,minDist**2\rfor cent in range(k):#recalculate centroids\rptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]# 得到这个簇的所有点\rcentroids[cent,:] = mean(ptsInClust, axis=0) # 将质心重置为均值\rreturn centroids, clusterAssment\r 返回值中centroids代表质心，clusterAssment用于记录簇的聚类结果和误差\n2.2 二分k均值聚类 普通k均值算法只能达到局部最优解.初始点的选择对算法的影响很大.无法达到全局最优.此时就需要进行后处理.\n聚类算法的指标是SSE（误差平方和），也即clusterAssment第二列之和，越小 代表数据点越接近质心，聚类效果好.后处理中降低SSE的方法，可以将最大的类再进行二分的聚类，然后为了保持簇的总数不变，需要合并两个使SSE增幅最小的簇的质心.对于高维数据并不好用.\nSSE在k-means中的计算如下：\n引入二分k均值聚类.初始将所有数据看做一个簇，然后循环.对于每个簇，先计算总误差， 然后将簇二分k均值后计算误差.最后选择使误差最小的簇进行划分.当簇的数量到达k时，退出循环.\n\rdef biKmeans(dataSet, k, distMeas=distEclud):\rm = shape(dataSet)[0]\rclusterAssment = mat(zeros((m,2)))# 记录最佳聚类的类别和距离，随迭代不断更新\rcentroid0 = mean(dataSet, axis=0).tolist()[0]\rcentList =[centroid0] #create a list with one centroid\rprint(centList)\rfor j in range(m):#calc initial Error\rclusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2\rwhile (len(centList) \u0026lt; k):\rlowestSSE = inf\rfor i in range(len(centList)):\rptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]# 获得簇i的所有数据点\rcentroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)# splitClustAss:簇分配矩阵，第一列索引第二列偏差；centroidMat：距离矩阵\rsseSplit = sum(splitClustAss[:,1])# 计算分割后的误差平方和（该公式上面已经提及）\rsseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])# 计算未分割的误差平方和\rprint (\u0026quot;sseSplit, and notSplit: \u0026quot;,sseSplit,sseNotSplit)\rif (sseSplit + sseNotSplit) \u0026lt; lowestSSE:\rbestCentToSplit = i\rbestNewCents = centroidMat\rbestClustAss = splitClustAss.copy()\rlowestSSE = sseSplit + sseNotSplit\rprint(nonzero(bestClustAss[:,0].A == 1))\rbestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)# 在二分的结果中找出类别“1”，将其替换到新的聚类类序号（如第三 类，第四类）\rbestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit# 在二分的结果中找出类别“0”，将其替换为最优分割点的类别\rprint ('the bestCentToSplit is: ',bestCentToSplit)\rprint ('the len of bestClustAss is: ', len(bestClustAss))\rcentList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]# 将质心替换为最优聚类时的质心\rcentList.append(bestNewCents[1,:].tolist()[0])\rclusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss# 重新分配簇和sse\rreturn mat(centList), clusterAssment\r 2.3 二分k均值聚类处理实际数据 《实战》中使用雅虎数据集测试算法.这里就不陈列获取数据的代码了，直接上画图的代码\ndef clusterClubs(numClust=5):\rdatList = []\rfor line in open('places.txt').readlines():\rlineArr = line.split('\\t')\rdatList.append([float(lineArr[4]), float(lineArr[3])])\rdatMat = mat(datList)\rmyCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)\rfig = plt.figure()\rrect=[0.1,0.1,0.8,0.8]# 该列表的作用：第一个左右移动，第二个上下移动，第三个左右缩放，第四个上下缩放\rscatterMarkers=['s', 'o', '^', '8', 'p', \\\r'd', 'v', 'h', '\u0026gt;', '\u0026lt;']\raxprops = dict(xticks=[], yticks=[])\rax0=fig.add_axes(rect, label='ax0', **axprops)\rimgP = plt.imread('Portland.png')\rax0.imshow(imgP)\rax1=fig.add_axes(rect, label='ax1', frameon=False)\rfor i in range(numClust):\rptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]\rmarkerStyle = scatterMarkers[i % len(scatterMarkers)]\rax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)\rax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=300)\rplt.show()\r 3. 模型评估 3.1 确定K的值-肘方法（Elbow method） 对于每个K的取值，一次计算其到簇中心的平方和.随着簇的增加，平方和会逐渐减少.\n下降过程中忽然出现拐点（下降速度变慢），被认为是最佳K值\n3.2 聚类的评估-轮廓系数法（Silhouette Coefficient， SC系数） 结合聚类的凝聚度和分离度，对聚类效果进行评估\n目的是实现内部距离最小化，外部距离最大化\n范围在-1-1之间，越接近1效果越好\n3.3 聚类的评估-CH系数法（Calinski-harabasz ） CH系数追求用尽量少的类别聚类尽量多的样本，并获取较好的效果\n计算方式如下图\ns值越大聚类效果越好\n4.算法优化 首先对传统kmeans进行总结：\n优点如下：\n1.原理简单实现容易\n2.空间复杂度为O(N)，时间复杂度为O(IKN)， k为簇数，I为迭代次数\n缺点如下：\n聚类效果依赖K的选择和初始点的选择\n中心店易偏移，对利群点，噪声敏感\n很难发现大小差别很大的簇进行增量计算\n只能得到局部最优解\n除了二分kmeans，还有其他优化算法\n4.1 使用canopy算法确定初始点 canopy的原理如下：选择随机初始点，以该点为圆心，不同的值（T1，T2）为半径画同心圆.然后不断寻找圆以外的点为圆心 ，不同的值为半径画同心圆，直到所有点都能包含在一个圆中.这么做可以使初始点尽可能的远.\ncanopy的优点如下：\nkmeans的抗干扰能比比较弱，使用canopy可以直接去掉较小数据点的簇，有利于抗干扰\ncanopy选出的中心点更精确\ncanopy的缺点如下：\n算法中T1，T2的确立，仍然会落入局部最优的问题\n4.2 使用kmeans++算法确定初始点 K-Means++算法使用距离平方求解，尽可能保证下一个质心到当前质心距离最远.\nK-Means++算法的初始化过程如下所示：\n从输入的数据点集合中随机选择一个点作为第一个聚类中心\n对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x).根据D（x）计算距离.距离计算公式如下:\n选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大（并非绝对，而是将距离映射到一个概率.详情参考博客）\n重复2和3直到k个聚类中心被选出来\n利用这k个初始的聚类中心来运行标准的k-means算法\n相比于kmeans，kmeans++的主要区别就在第三步.kmeans更新中心点为簇的均值点，而++更新中心为距离相对较远的点.\n4.3 使用二分kmeans算法设置阈值进行划分 当SSE达到阈值时或k值达到要求时，停止划分\n《实战》源码中已经提及和代码原理注释，不过多赘述\n4.4 使用kmedoids算法选取质心 与kmeans的区别在于，kmeans选取新的簇中心使用平均值，而kmedoids算法比较麻烦，需要计算除原中心点外的其他点成为中心点时，代价函数（如距离和）的值.选取最小值以成为新的中心点\n这么做，相较于kmeans，提高了抗噪声能力.但是样本多的时候运行非常慢.\n4.5 其他 Kernel kmeans通过将每一个样本投射到高维空间（SVM核函数），再将处理好的数据使用普通kmeans进行聚类\nISODATA类似于《实战》中的后处理方法，类别的数目会随着聚类的进行而改变.合并（样本数少，两个类距离太近）和分裂（类内方差过大）\nMinibatch 适用于大批量数据（10000+）.原理是从大批量数据中每次选取一批进行聚类，进行多次.是大数据集的分批聚类\n5. 特征降维 包括特征选择和主成分分析（可以理解成一种特征提取的方式）.\n5.1 特征选择 5.1.1 Filter(过滤式) 主要探究特征本身，特征与特征之间和目标值之间的关联\n5.1.1.1 低方差特征过滤 两列数据方差小，代表某个特征大部分的值比较接近.需要过滤.\nAPI接口\n5.1.1.2 相关系数 包括皮尔逊相关系数和斯皮尔曼相关系数\n皮尔逊相关系数\n非常麻烦的式子，计算公式如下图：\n其中x和y为两列，即两个特征.n为个数\n相关系数r的范围是-1到1.当|r|\u0026lt;0.4为低度相关,当0.4\u0026lt;|r|\u0026lt;0.7为显著性相关,当0.7\u0026lt;|r|\u0026lt;1为高度线性相关\nAPI接口\n斯皮尔曼相关系数\n计算公式如下，其中di为二列成队变量的等级差值：\nAPI接口\n5.1.2 Embedded（嵌入式） 算法自动选择特征（特征与目标值之间的关联）\n决策树，包括信息熵，信息增益,点击跳转\n正则化，包括L1，L2，线性回归中有提到,点击跳转\n深度学习（卷积）\n5.2 主成分分析（PCA） 高维信息转低维.此过程中可能舍弃原有数据，创建新的变量.应用在回归分析或聚类分析中.\n与特征选择的区别是特征选择删除特征，没有产生新特征.而PCA产生新特征\nAPI接口\n6. API接口实现 6.1 kmeans 接口非常简单\n由于聚类算法不同于分类 ，fit和predict可以一步实现\n打分标准和分类（监督学习）也不一样，通过CH方法进行评估.\ncalinski_harabaz_score(X, y_pre)\r 来实现.分数越高，分类效果越好.\n6.2 低方差特征过滤 默认0.0的参数是方差，必须设置，比较基本不会有完全相同的两个特征\ndef var_thr():\r\u0026quot;\u0026quot;\u0026quot;\r特征选择：低方差特征过滤\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rdata = pd.read_csv(\u0026quot;./data/factor_returns.csv\u0026quot;)\rprint(data)\rtransfer = VarianceThreshold(threshold=10)\rtrans_data = transfer.fit_transform(data.iloc[:, 1:10])\rprint(\u0026quot;之前数据的形状：\\n\u0026quot;,data.iloc[:, 1:10].shape)\rprint(\u0026quot;之后数据的形状：\\n\u0026quot;,trans_data.shape)\rprint(trans_data)\r 6.3 皮尔逊, 斯皮尔曼相关系数 from scipy.stats import pearsonr, spearmanr\r# 皮尔逊演示\rx1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]\rx2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]\rret = pearsonr(x1, x2)\rprint(\u0026quot;这两列数据的皮尔逊相关系数为：\\n\u0026quot;, ret)\rret = spearmanr(x1, x2)\rprint(\u0026quot;这两列数据的斯皮尔曼相关系数为：\\n\u0026quot;, ret)\r 皮尔逊计算得出两个值.第一个值越大越相关.第二个值越小越相关.当样本数大于500时，第二个数更实用.斯皮尔曼的结果与其类似\n6.4 主成分分析 from sklearn.decomposition import PCA\rdata = [[2, 8, 4, 5],\r[6, 3, 0, 8],\r[5, 4, 9, 1]]\r# 1.保留到多少维度\r# transfer = PCA(n_components=2)\r# trans_data = transfer.fit_transform(data)\r# print(trans_data)\r# 2.保留信息的百分比\rtransfer = PCA(n_components=0.95)\rtransfer_data = transfer.fit_transform(data)\rprint(transfer_data)\r 7. 案例-消费预测 现有四张表：order表，product表，order_product表，aisle表.首先需要将多表合并，然后通过特征降维获取输入数据，机器学习后得出模型\n# 1.获取数据\r# 2.数据基本处理\r# 2.1 合并表格\r# 2.2 交叉表合并\r# 2.3 数据截取\r# 3.特征工程 — pca\r# 4.机器学习（k-means）\r# 5.模型评估\rimport pandas as pd\rfrom sklearn.decomposition import PCA\rfrom sklearn.cluster import KMeans\rfrom sklearn.metrics import silhouette_score\r# 1.获取数据\rorder_product = pd.read_csv(\u0026quot;./data/instacart/order_products__prior.csv\u0026quot;)\rproducts = pd.read_csv(\u0026quot;./data/instacart/products.csv\u0026quot;)\rorders = pd.read_csv(\u0026quot;./data/instacart/orders.csv\u0026quot;)\raisles = pd.read_csv(\u0026quot;./data/instacart/aisles.csv\u0026quot;)\r# 2.数据基本处理\r# 2.1 合并表格\rtable1 = pd.merge(order_product, products, on=[\u0026quot;product_id\u0026quot;, \u0026quot;product_id\u0026quot;])\rtable2 = pd.merge(table1, orders, on=[\u0026quot;order_id\u0026quot;, \u0026quot;order_id\u0026quot;])\rtable = pd.merge(table2, aisles, on=[\u0026quot;aisle_id\u0026quot;, \u0026quot;aisle_id\u0026quot;])\r# 2.2 交叉表合并\rdata = pd.crosstab(table[\u0026quot;user_id\u0026quot;], table[\u0026quot;aisle\u0026quot;])\r# 2.3 数据截取\rnew_data = data[:1000]\r# 3.特征工程 — pca\rtransfer = PCA(n_components=0.9)\rtrans_data = transfer.fit_transform(new_data)\r# 4.机器学习（k-means）\restimator = KMeans(n_clusters=5)\ry_pre = estimator.fit_predict(trans_data)\r# 5.模型评估\rsilhouette_score(trans_data, y_pre)\r 8. 总结 本博客介绍了k均值算法，并对其不足之处陈列了一系列改进算法.同时介绍了特征降维两种方法，即特征提取和主成分分析.最后，通过一个消费预测的实例测试了算法\n9. p.s. pd.merge\n","date":"2022-06-20","permalink":"https://wudaore.github.io/post/k-means/","tags":["机器学习","algo"],"title":"k均值聚类算法"},{"content":"目录 基本原理 bagging boosting 总结 ps. 1. 基本原理 即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测.\n对于欠拟合问题，使用boosting逐步增强学习；对于过拟合问题，使用bagging采样学习集成.\n2. bagging 2.1 bagging 集成原理和过程 1.采样.从所有版本中，采样一部分\n2.学习.训练弱学习器\n3.集成.使用平权投票\n2.2 随机森林 随机森林，即bagging+决策树.其实现流程如下：\n1.随机选k条数据（一次随机选一个样本，有放回的抽样）\n2.随机选m个特征（m远小于特征总数）\n3.训练决策树（默认CART（基尼系数）决策树）\n4.重复1-3步构造n棵弱决策树\n5.平权投票集成所有决策树\n注：随机抽样是为了让每棵树的训练集不一样\n抽样必须是有放回的，不然可能会导致每棵树训练出来的结果有很大的差异.而随机森林取决于每棵树的投票\n2.3 随机森林sklearn接口 在使用随机森林时，需要对树的个数，树的深度等超参数进行调整.泰坦尼克号样例代码如下（与决策树代码类似，改一下模型就可）：\n# 4.机器学习（模型训练）\restimator = RandomForestClassifier()\rparam_grid = {\u0026quot;n_estimators\u0026quot;: [120,200,300,500,800,1200], \u0026quot;max_depth\u0026quot;: [5, 8, 15, 25, 30]}\restimator = GridSearchCV(estimator, param_grid=param_grid, cv=5)\restimator.fit(x_train, y_train)\rprint(estimator.score(x_train, y_train))\rprint(estimator.best_estimator_) # 打印最佳参数选择\r 2.4 bagging总结 bagging集成学习方法 = Bagging+决策树/逻辑回归深度学习\u0026hellip;\n经过上面方式组成的集成方法，可以提高一定的泛化准确率，并简单，方便，通用\n3. boosting 1.训练学习器\n2.调整数据分布（对于分类争取的数据减少其权重，分类正确则增加其权重），将训练注意力集中在错误数据上\n3.重复1,2.直到达到结束条件（《实战》中的结束条件为迭代次数到了或者再也没有错误分类了）\n4.各个学习器加权投票，得出结果\n3.1 AdaBoost\u0026ndash;《实战》源码 AdaBoost将结合《实战》中的代码进行学习.AdaBoost即自适应Boost（adaptive Boost）.其基本流程如下：\n1.初始化训练数据权重相等，训练第一个学习器\n D = mat(ones((m,1))/m) #init D to all equal\r 2.计算该学习器在训练数据中的错误率\n3.计算该学习器的投票权重αt.αt的计算公式如下：\nalpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0\rbestStump['alpha'] = alpha weakClassArr.append(bestStump) # 将最佳单层决策树存入数组中\r 4.根据投票权重，调整学习器的注意力到错误数据上，对训练数据重新赋权值.\nexpon = multiply(-1*alpha*mat(classLabels).T,classEst) # 权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1\rD = multiply(D,exp(expon)) # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值\rD = D/D.sum()\r 5.重复执行1-4步n次\n6.对所有学习器投票计算权重\n\r训练模型代码\r\rdef adaBoostTrainDS(dataArr,classLabels,numIt=40):\rweakClassArr = []\rm = shape(dataArr)[0]\rD = mat(ones((m,1))/m) #init D to all equal\raggClassEst = mat(zeros((m,1)))\rfor i in range(numIt):\rbestStump,error,classEst = buildStump(dataArr,classLabels,D)# 最优单层决策树的基本信息（特征下标，阈值和符号），最小误差，分类结果\r#print('result = ', bestStump)\r#print(error)\r#print(classEst)\r#print(\"D:\",D.T)\ralpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0\rbestStump['alpha'] = alpha weakClassArr.append(bestStump) # 将最佳单层决策树存入数组中\rexpon = multiply(-1*alpha*mat(classLabels).T,classEst) # 权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1\rD = multiply(D,exp(expon)) # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值\rD = D/D.sum()\r#calc training error of all classifiers, if this is 0 quit for loop early (use break)\raggClassEst += alpha*classEst\r#print(\"aggClassEst: \",aggClassEst.T)\r# addErrors存放分类错误的特征.分错为1分对为0\raggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\r#print('aggerr', aggErrors)\rerrorRate = aggErrors.sum()/m\rprint(\"total error: \",errorRate)\rif errorRate == 0.0: break\rreturn weakClassArr,aggClassEst\r\r\r\r利用模型分类 代码\r\rdef adaClassify(datToClass,classifierArr):\r# print(classifierArr)\r# 输入：由一个或多个待分类样例daToClass以及多个弱分类器组成的数组classifierArr\r# 输出：aggClassEst的符号，即aggClassEst大于0则返回+1，小于0则返回-1\rdataMatrix = mat(datToClass)\rm = shape(dataMatrix)[0]\raggClassEst = mat(zeros((m,1)))\rfor i in range(len(classifierArr)):\rclassEst = stumpClassify(dataMatrix,classifierArr[0][i]['dim'],\\\rclassifierArr[0][i]['thresh'],\\\rclassifierArr[0][i]['ineq'])\raggClassEst += classifierArr[0][i]['alpha']*classEst # 。输出的类别估计值乘上该单层决策树的alpha权重然后累加到aggClassEst上\r# print(aggClassEst)\rreturn sign(aggClassEst)\r\r\r3.2 bagging和boosting的区别 1.数据方面\nBagging：对数据进行采样训练\nBoosting：根据前一轮学习结果调整数据重要性\n2.投票方面\nBagging：所有学习器平权投票\nBoosting：所有学习器加权投票\n3.学习顺序\nBagging：Bagging是并行的，每个学习器没有依赖关系\nBoosting：学习是串行，有先后顺序\n4.主要作用\nBagging：解决过拟合（降低方差，提高泛化性能）\nBoosting：解决欠拟合（提高训练精度，降低偏差）\n3.3 GBDT GBDT = 梯度下降+Boosting+决策树\nGBDT使用梯度下降法优化代价函数，使用一层决策树作为弱学习器，负梯度作为目标值，最后利用boosting思想进行集成\nGBDT举例\n   编号 年龄 体重 身高     1 5 20 1.1   2 7 30 1.3   3 21 70 1.7   4 30 60 1.8   5 25 65 ?    预测身高，这是一个回归问题.回归问题的损失函数可以使用平方误差（二分类使用log，多分类使用softmax）\n第一步，计算损失函数，并求出第一个预测值.\n第二步，使用误差值替换目标值.求解划分点，使得方差最小\n第三步，通过划分点划分，求解得出h1x\ny,的求法类似于上面的h0x，即对损失函数求导使其等于0\n第四步，求解h2x\nh2x的求法也类似于h1x.首先求出误差值，使用误差值替换目标值.h2x的误差值等于上一轮的目标值-预测值.\n最后经过一系列计算，得出最后的预测模型：\n3.4 GBDT和AdaBoost的区别 GBDT与Adboost最主要的区别在于两者如何识别模型的问题。\nAdaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。\nGBDT通过负梯度来识别问题，通过计算负梯度来改进模型。\nGBDT每一轮训练时所关注的重点是本轮产生结果的残差，下一轮以本轮残差作为输入，尽量去拟合这个残差，使下一轮输出的残差不断变小。所以GBDT可以做到每一轮一定向损失函数减小的梯度方向变化，而传统的boosting算法只能是尽量向梯度方向减小，这是GBDT与传统boosting算法最大的区别，这也是为什么GBDT相比传统boosting算法可以用更少的树个数与深度达到更好的效果。\n和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足，而GBDT是通过算梯度来定位模型的不足。因此相比AdaBoost，GBDT可以使用更多种类的目标函数。\n3.5 XGBoost（简单了解） 基本原理：二阶泰勒展开，boosting，正则化，决策树\n","date":"2022-06-08","permalink":"https://wudaore.github.io/post/ensemble-learning/","tags":["机器学习","algo"],"title":"集成学习"},{"content":"目录 基本原理 度量方法 常用的剪枝方法 特征工程-特征提取 决策树的实现 决策树绘制 总结 ps. 1. 基本原理 决策树是一种树形结构，每个内部节点代表一个属性上的判断.每个叶节点代表一种分类结果.它的本质就是基于数据，通过问一系列的问题(if-else)去预测结果。\n2. 度量方法 所谓熵即混乱程度.越有序熵越低.\n2.1 香农熵 计算代码如下：\ndef calcShannonEnt(dataSet):\rnumEntries = len(dataSet)\rlabelCounts = {}\rfor featVec in dataSet: #the the number of unique elements and their occurance\rcurrentLabel = featVec[-1]\rif currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\rlabelCounts[currentLabel] += 1\rshannonEnt = 0.0\rfor key in labelCounts:\rprob = float(labelCounts[key])/numEntries\rshannonEnt -= prob * log(prob,2) #log base 2\rreturn shannonEnt\r 2.2 信息增益 信息增益，即以某特征值划分前后信息熵的差值.特征值A对数据集D的增益g(D,A) = H(D) - H(D|A)\n计算代码如下：\ndef chooseBestFeatureToSplit(dataSet):\rnumFeatures = len(dataSet[0]) - 1 #the last column is used for the labels\rbaseEntropy = calcShannonEnt(dataSet)\rbestInfoGain = 0.0; bestFeature = -1\rfor i in range(numFeatures): # 对于每一个特征\rfeatList = [example[i] for example in dataSet] # 提取出这个特征所有的值\runiqueVals = set(featList) # unique这个值\rnewEntropy = 0.0\rfor value in uniqueVals: # 对于某个特征的所有值（类别）\rsubDataSet = splitDataSet(dataSet, i, value)\rprob = len(subDataSet)/float(len(dataSet))\rnewEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # 计算信息增益\rif (infoGain \u0026gt; bestInfoGain): #compare this to the best gain so far\rbestInfoGain = infoGain #if better than current best, set to best\rbestFeature = i\rreturn bestFeature #returns an integer\r 举例 探究某论坛用户流失和性别，活跃度的相关性：\n   编号 性别 活跃度 是否流失     1 男 高 0   2 女 中 0   3 男 低 1   4 女 高 0   5 男 高 0   6 男 中 0   7 男 中 1   8 女 中 0   9 女 低 1   10 女 中 0   11 女 高 0   12 男 低 1   13 女 低 1   14 男 高 0   15 男 高 0    整理后的数据如下：\n    positive negative 汇总     整体 5 10 15   男 3 5 8   女 2 5 7   高活跃 0 6 6   中活跃 1 4 5   低活跃 4 0 4    首先计算整体熵.E(S) = -( 5/15 * log(5/15) + 10/15 * log(10/15) ) = 0.9182\n然后计算性别熵.根据条件熵的公式，需要依次计算男性和女性的熵值.\nE(male) = -( 3/8 * log(3/8) + 5/8 * log(5/8) ) = 0.9543\nE(female) = -( 2/7 * log(2/7) + 5/7 * log(5/7) ) = 0.8631\n因此，性别的信息增益GAIN = E(S) - 8/15 * E(male) - 7/15 * E(female) = 0.0064\n同理，可以求出活跃度的信息增益GAIN = 0.6776 ，远远大于性别的信息增益.也即活跃度对用户流失的影响比性别大.在特征选择和特征分析的时候要更多的考虑.\n表现在决策树上，也即信息增益高的因素在树上会被优先划分.\n2.3 信息增益比 不难发现，信息增益会更偏重分类多的属性(如上面的例子，若将编号也算作类别的话，编号的信息增益将会最大，这显然是不合理的.)，因此引入信息增益比.\n首先引入属性分裂信息度量H.对于每个属性（性别，是否活跃），都会有若干个可能的取值（性别能取男，女，是否活跃能取活跃，一般，不活跃.）令p(xi)为某个取值的占比，属性分裂信息度量的计算公式为：\n而信息增益比 = 信息增益 / 属性分裂信息度量.\n2.4 基尼增益. 与信息增益同样是度量的指标.基尼增益最大的属性作为决策树的根节点.基尼增益=总体基尼值-基尼指数\n2.4.1 基尼值 GiNi(D)代表从数据集D中随机抽取两个样本，计算其不一致的概率.基尼值越小，数据集纯度越高\n2.4.2 基尼指数 一般选择划分后基尼系数最小的属性为最优划分属性.\n举例：是否拖欠贷款和是否有房，年收入和婚姻状况的关系.数据表格如下(s代表singal单身，m代表married，已结婚,d表示divorced，已离婚)：\n   编号 是否有房 婚姻状况 年收入 是否拖欠贷款     1 1 s 125 0   2 0 m 100 0   3 0 s 70 0   4 1 m 120 0   5 0 d 95 1   6 0 m 60 0   7 1 d 220 0   8 0 s 85 1   9 0 m 75 0   10 0 s 90 1    与计算信息增益类似，基尼增益需要先算出分类以前的基尼值.然后根据各个属性的不同类别（比如婚姻状况属性就有三个类别），依次计算出他们的基尼值.最后用分类以前的基尼值依次进去[ 概率（这里指的是总概率，比如这个案例中的是否拖欠贷款）* 各个类别的基尼值 ]\n以本案例来说，根节点基尼系数为0.42.\n根据是否有房，计算基尼系数增益为0.077：\n婚姻状况不同于是否有房，其并不是一个是或否的问题.需要根据婚姻状况这个属性的三种类别分别划分数据，计算基尼增益.注意划分后只有某类别与其他类别.计算过程如下：\n其中最大的增益为0.12\n年收入是数值型数据，处理方法不同于前面两种.其基尼增益的求法为从小到大排序，两两之间求出相邻值中点，以此求基尼增益.\n以60和70中间点65为例，以该点为分割时，Gini(年收入) = 0.42 - [1/10 * (1-0-1)] - { 9/10 * [ 1-(3/9)2 - (6/9)2 ] } = 0.02\n其中 1/10是小于65的点的概率；3/9是大于65的点中分类为positive的点的占比.依次通过每个中间点分割数据，得到最大的基尼增益同样是0.12\n婚姻状况\u0026ndash;married的基尼增益也是0.12，因为先被计算出来所以先用它来分割根节点，**注意：**每次分割后剩余的数据需要重新计算基尼增益，来决定下一次由谁来分割.比如分割掉married后，是否有房的基尼增益变为最大，需要根据它来分.具体流程如下图：\n2.5 小结. 决策树的变量可以有数字型和名称型.\n数字型，如上文提到的工资，需要使用“\u0026gt;=,\u0026lt;”等比值符号进行分割.而名称型，如上文提到的婚姻状况，使用“=”分割.\n决策树构建的基本步骤如下：\n1.开始将所有记录看做一个节点\n2.遍历每个变量的每一种分割方式，找到最适合的分割方式，分割数据\n3.对分割后的数据重复分割，知道最后的节点足够纯为止\n根据使用信息增益，信息增益率和基尼指数，决策树分为ID3决策树，C4.5决策树和CART决策树\n   决策树类别 分支方式 优点 缺点 备注     ID3 信息增益  只能对离散属性的数据集构造决策树，偏向取值多的属性    C4.5 信息增益率 分类结果易于理解准确率高.可以处理连续数据.对于缺失值的处理.采用了后剪枝，避免树高无限增长，避免过拟合 对数据多次扫描排序，效率低.另外，只能处理训练集全部读入内存的情况.数据集过大就不好处理 优化后解决了ID3偏向取值多的属性.   CART 基尼指数 可以进行分类和回归.可以处理离散也可以处理连续  根据上面的例子可知，CART每次分类都只是二分    3. 常用的剪枝方法 由于噪声，样本冲突，或特征即属性不能完全作为分类标准，或因为数据量不够大造成的巧合的规律性，不剪枝的决策树容易过拟合.\n3.1 预剪枝 即一边构造树时一边剪枝.主要通过(1)限制节点包含的最小样本数(2)指定树的高度或深度(3)指定节点的熵小于某个值\n3.2 后剪枝 构造完树后进行从下往上的剪枝.C4.5就是后剪枝.因为要构造完树才能剪枝，当数据量太大导致无法全部读入内存时，C4.5无法运行\n4. 特征工程-特征提取 特征提取，即将数据转换为能使机器学习的数字特征.分为三类，字典特征提取（特征离散化），文本特征提取和图像特征提取（深度学习）.本博客介绍前两类.\nsklearn中特征提取的API为feature_extraction.点击跳转\n4.1 字典特征提取 对于特征中存在的类别信息，我们一般都会one-hot编码\n具体用法和标准化的接口非常类似\ndef dict_demo():\r\u0026quot;\u0026quot;\u0026quot;\r字典特征提取\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rdata = [{'city': '北京', 'temperature': 100},\r{'city': '上海', 'temperature': 60},\r{'city': '深圳', 'temperature': 30}]\r# 字典特征提取\r# 1.实例化\rtransfer = DictVectorizer(sparse=False)\r# 2.调用fit_transform\rtrans_data = transfer.fit_transform(data)\rprint(\u0026quot;特征名字是：\\n\u0026quot;, transfer.get_feature_names())\rprint(trans_data)\r 运行结果如下：\n而在数据量特别大时，sparse设置为True会更直观，提高读取效率并节省内存\n4.2 文本特征提取 4.2.1 英文文本特征提取 def english_count_text_demo():\r\u0026quot;\u0026quot;\u0026quot;\r文本特征提取 -- 英文\r:return: NOne\r\u0026quot;\u0026quot;\u0026quot;\rdata = [\u0026quot;life is is short,i like python\u0026quot;,\r\u0026quot;life is too long,i dislike python\u0026quot;]\r# 1.实例化\r# transfer = CountVectorizer(sparse=False) # 注意，没有sparse这个参数\rtransfer = CountVectorizer(stop_words=[\u0026quot;dislike\u0026quot;])\r# 2.调用fit_transform\rtransfer_data = transfer.fit_transform(data)\rprint(transfer_data)\rprint(transfer.get_feature_names())\rprint(transfer_data.toarray())\r 4.2.2 中文文本特征提取 需要先进行分词处理.本篇使用jieba分词，后续可用分词效果更好的模型如hanlp\n需要先分词后，使用\u0026quot; \u0026ldquo;.join转换成用空格分开的句子.转换后再代入.\ndef cut_word(sen):\r\u0026quot;\u0026quot;\u0026quot;\r中文分词\r:return: sen\r\u0026quot;\u0026quot;\u0026quot;\r# print(\u0026quot; \u0026quot;.join(list(jieba.cut(sen))))\rreturn \u0026quot; \u0026quot;.join(list(jieba.cut(sen)))\r def chinese_count_text_demo2():\r\u0026quot;\u0026quot;\u0026quot;\r文本特征提取 -- 中文\r:return: NOne\r\u0026quot;\u0026quot;\u0026quot;\rdata = [\u0026quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\u0026quot;,\r\u0026quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\u0026quot;,\r\u0026quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\u0026quot;]\rlist = []\rfor temp in data:\r# print(temp)\rlist.append(cut_word(temp))\rprint(list)\r# 1.实例化\rtransfer = CountVectorizer(stop_words=[\u0026quot;一种\u0026quot;, \u0026quot;还是\u0026quot;])\r# 2.调用fit_transform\rtransfer_data = transfer.fit_transform(list)\rprint(transfer.get_feature_names())\rprint(transfer_data.toarray())\r 4.3 tf-idf 作用：评估一个词在一个文件中的重要作用.某次在本文章中出现的概率高（tf，词频率），在其他文件中出现的概率低（idf，逆向文档频率），那么就适合分类.\ntfidf = tf * idf.\ntf即词频，idf = log10(总文件数/出现该词的文件数)\n5. 决策树的实现 5.1 《实战》源码实现 《实战》决策树分类中只讲了ID3决策树的实现.实现选择信息增益最大的特征的函数已经在上面列出（chooseBestFeatureToSplit）.\n下面是分割数据集的函数：\ndef splitDataSet(dataSet, axis, value): # 分割特征下标为axis，值为value的数据\rretDataSet = []\rfor featVec in dataSet:\rif featVec[axis] == value:\rreducedFeatVec = featVec[:axis] #chop out axis used for splitting\rreducedFeatVec.extend(featVec[axis+1:])\rretDataSet.append(reducedFeatVec)\rreturn retDataSet\r 票选函数：\ndef majorityCnt(classList):\rclassCount={}\rfor vote in classList:\rif vote not in classCount.keys(): classCount[vote] = 0\rclassCount[vote] += 1\rsortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\rreturn sortedClassCount[0][0]\r 最后是递归构造决策树的函数.递归停止条件一为只剩一个类别了.二位没有特征可以继续分割了\ndef createTree(dataSet,labels):\rclassList = [example[-1] for example in dataSet] # yes or no 的一个list\rif classList.count(classList[0]) == len(classList): return classList[0] # 当只剩一种类别时，停止递归\rif len(dataSet[0]) == 1: # 没有特征可以分割了，停止递归\rreturn majorityCnt(classList) # 选取类别最多的作为结果\rbestFeat = chooseBestFeatureToSplit(dataSet)\rbestFeatLabel = labels[bestFeat]\rmyTree = {bestFeatLabel:{}}\rdel(labels[bestFeat])\rfeatValues = [example[bestFeat] for example in dataSet]\runiqueVals = set(featValues)\rfor value in uniqueVals:\rsubLabels = labels[:] #copy all of labels, so trees don't mess up existing labels\rmyTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\rreturn myTree  5.2 sklearn实现 具体接口信息点击跳转\n案例：泰坦尼克号乘客生存预测\n\r点击查看\r\r1.获取数据 2.数据基本处理 2.1 确定特征值,目标值 2.2 缺失值处理 2.3 数据集划分 3.特征工程(字典特征抽取) 4.机器学习(决策树) 5.模型评估 import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.feature_extraction import DictVectorizer\n1.获取数据 data = pd.read_csv(\u0026rdquo;./titanic.csv\u0026quot;)\n2.数据基本处理 -缺失值替换 2.1 确定特征值,目标值 train = data[[\u0026ldquo;pclass\u0026rdquo;,\u0026ldquo;age\u0026rdquo;,\u0026ldquo;sex\u0026rdquo;]] label = data[\u0026ldquo;survived\u0026rdquo;] print(train.isnull().sum())\n2.2 缺失值处理 train[\u0026lsquo;age\u0026rsquo;].fillna(value=train[\u0026lsquo;age\u0026rsquo;].mean(), inplace=True) train\n2.3 数据集划分 x_train, x_test, y_train, y_test = train_test_split(train, label, random_state=22, test_size=0.2)\n3.特征工程(字典特征抽取) 字典特征抽取需要先将输入转换为字典样式 orient参数可以有很多值，用到的时候再查 x_train = x_train.to_dict(orient=\u0026ldquo;records\u0026rdquo;) x_test = x_test.to_dict(orient=\u0026ldquo;records\u0026rdquo;) transfer = DictVectorizer() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test)\n4.机器学习(决策树) estimator = DecisionTreeClassifier(max_depth=5) estimator.fit(x_train, y_train) estimator.predict(x_test)\n5.模型评估 print(estimator.score(x_test, y_test))\n\n\r6. 决策树的绘制 6.1 《实战》源码 \r获取叶子节点个数\r\rdef getNumLeafs(myTree):\rnumLeafs = 0\rfirstStr = myTree.keys()[0]\rsecondDict = myTree[firstStr]\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rnumLeafs += getNumLeafs(secondDict[key])\relse: numLeafs +=1\rreturn numLeafs\r\r\r\r计算树的高度\r\rdef getTreeDepth(myTree):\rmaxDepth = 0\rfirstStr = myTree.keys()[0]\rsecondDict = myTree[firstStr]\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rthisDepth = 1 + getTreeDepth(secondDict[key])\relse: thisDepth = 1\rif thisDepth  maxDepth: maxDepth = thisDepth\rreturn maxDepth\r\r\r\r使用文本注解绘制树节点\r\rdef plotNode(nodeTxt, centerPt, parentPt, nodeType):\rcreatePlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',\rxytext=centerPt, textcoords='axes fraction',\rva=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args )\r\r\r\r绘制分支上的值，计算父节点和子节点的中间位置，添加简单的文本信息\r\rdef plotMidText(cntrPt, parentPt, txtString):\rxMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]\ryMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\rcreatePlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\r\r\r\r绘制树\r\rdef plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on\rnumLeafs = getNumLeafs(myTree) #this determines the x width of this tree\rdepth = getTreeDepth(myTree)\rfirstStr = list(myTree.keys())[0] #the text label for this node should be this\rcntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\rplotMidText(cntrPt, parentPt, nodeTxt)\rplotNode(firstStr, cntrPt, parentPt, decisionNode)\rsecondDict = myTree[firstStr]\r# 更新下一个节点的位置的Y值\rplotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rplotTree(secondDict[key],cntrPt,str(key)) #recursion\relse: #it's a leaf node print the leaf node\rplotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\rplotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\rplotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\rplotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\r\r\r注：该段代码比较阴间，需要注解\n设该树所有叶节点水平位置上之间的距离 d=（ 1/叶节点的个数）。\ny的坐标很好计算，只需要使用上个y的坐标-1/树高 就可以了.以下只考虑x坐标的计算。分两种情况讨论，一是非叶节点，二是叶节点。\n对于非叶节点，每个非叶节点的x位置都可以分别用它左边最近的叶节点进行求解。设某非叶结点 A 有n个叶节点，它左边最近叶节点 a 位置为（xOff，yOff），那么A应位于属于它的所有叶节点的中间位置，即水平位置上离 a 的间隔是（n+1）/2。那么它的 x坐标=xOff+d*（n+1）/2。\n对于叶节点，每个叶节点的x位置也可以用它左边最近的叶节点进行求解。左边最近的叶节点坐标位置（xOff，yOff），即 某叶节点x坐标是 xOff+d。\n\r主函数，调用 绘制树 函数\r\rdef createPlot(inTree):\rfig = plt.figure(1, facecolor='white')\rfig.clf()\raxprops = dict(xticks=[], yticks=[])\rcreatePlot.ax1 = plt.subplot(111, frameon=False, **axprops) #no ticks\r#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses plotTree.totalW = float(getNumLeafs(inTree))\rplotTree.totalD = float(getTreeDepth(inTree))\r# 使用两个全局变量plotTree.xOff和plotTree.yOff追踪已经绘制的节点位置，以及放置下一\r# 个节点的恰当位置\rplotTree.xOff = -0.5/plotTree.totalW\rplotTree.yOff = 1.0;\rprint(plotTree.xOff)\rprint(plotTree.yOff)\rplotTree(inTree, (0.5, 1.0), '')\rplt.show()\r\r\r6.2 sklearn实现 运用上面的泰坦尼克号的案例.sklearn提供了可视化决策树的接口\n\r决策树可视化\r\rfrom sklearn.tree import export_graphviz\rexport_graphviz(estimator, out_file=\"./data/tree.dot\", feature_names=['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', '女性', '男性'])\r\r\r运行后导出一个dot文件.可以直接查看但是比较糊.可以将文件内容输入到这个网站进行查看\n6. 总结 决策树有两个优点：\n一是得到的模型很容易可视化，非专家也很容易理解（至少对于较小的树而言）；\n二是算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。\n特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好\n决策树的主要缺点在于：\n即使做了预剪枝，它也经常会过拟合，泛化性能很差。\n7. p.s. 决策树可视化时特征名字和训练集不一致，最好用代码看一下训练的特征顺序，才好判定\n\rfor i in x_train:\rprint(i.toarray())\r ","date":"2022-06-05","permalink":"https://wudaore.github.io/post/decision-tree/","tags":["机器学习","algo"],"title":"决策树学习"},{"content":"目录 基本原理 逻辑回归的损失函数 逻辑回归的优化 随机梯度上升 分类评估方法 sklearn实现 总结 ps. 1. 基本原理 虽然叫回归但是其实解决的是二分类问题.逻辑回归的输入值为线性回归的输出值，即\n之所以能解决二分类问题，是因为逻辑回归将输入赋予sigmod函数，并设置阈值，可以达到分类的效果\nsigmod函数：\n逻辑回归的工作流程如下：\n2. 对数似然函数 区别于线性回归的平方损失函数，逻辑回归使用对数似然函数.\n也可以这么写（综合完整损失函数.）\n为什么要使用这个损失函数呢？试想：当样本值（y）为1而线性回归预估值比较小（比如h(θ)接近0）时，通过对数似然函数进行计算，会得到一个很大的值，给予模型最大的惩罚力度；当样本值（y）为1而线性回归预估值比较大（比如h(θ)接近1）时，计算得出0，不给模型惩罚.就很巧妙.\n3. 逻辑回归的优化 根据上面的综合完整损失函数，可以推导出损失函数的优化是这样的：要使求和的值最小，就要提升原本是类别1的概率，降低原本是类别0的概率.\n同样可以使用梯度下降（上升）进行优化.\n需要注意的是 《实战》中并没有具体推导对数似然函数的优化过程.以下是推导过程：\n于是，《实战》中梯度上升的代码就能被解释了.\n# 梯度上升迭代参数\rdef gradAscent(dataMatIn, classLabels):\rdataMatrix = mat(dataMatIn) #convert to NumPy matrix\rlabelMat = mat(classLabels).transpose() #转置.\rm,n = shape(dataMatrix)\ralpha = 0.001\rmaxCycles = 500\rweights = ones((n,1))\rfor k in range(maxCycles): #maxCycles 迭代次数\rh = sigmoid(dataMatrix*weights) #matrix mult\rerror = (labelMat - h) #vector subtraction\rweights = weights + alpha * dataMatrix.transpose()* error # 上面推导出的结果\rreturn weights\r # 画出最优曲线\rdef plotBestFit(weights):\rimport matplotlib.pyplot as plt\rdataMat,labelMat=loadDataSet()\rdataArr = array(dataMat)\rn = shape(dataArr)[0] xcord1 = []; ycord1 = []\rxcord2 = []; ycord2 = []\rfor i in range(n):\rif int(labelMat[i])== 1:\rxcord1.append(dataArr[i,1])\rycord1.append(dataArr[i,2])\relse:\rxcord2.append(dataArr[i,1])\rycord2.append(dataArr[i,2])\rfig = plt.figure()\rax = fig.add_subplot(111)\rax.scatter(xcord1, ycord1, s=30, c='blue', marker='s')\rax.scatter(xcord2, ycord2, s=30, c='green')\rx = arange(-3.0, 3.0, 0.1)\r# 这里的Y其实是X2. 须知y=xw，x0=0，而根据sigmod函数图像，当xw=0时可以分割两类曲线.因此0=xw，可以推导出x1和x2（这里的Y）的关系\ry = (-weights[0]-weights[1]*x)/weights[2]\rax.plot(x, y)\rplt.xlabel('X1'); plt.ylabel('X2');\rplt.show()\r 4. 随机的梯度上升  gradAscent中dataMatrix * weights是矩阵相乘，实际进行了300次，即需要遍历整个数据集，在数据集很大时效率不好.需要进行优化.\n使用随机梯度上升，每次只计算一个数据的梯度而非整个数据集，可以达到更快的速度.\n\r# 随机梯度上升迭代参数\rdef stocGradAscent0(dataMatrix, classLabels):\rm,n = shape(dataMatrix)\ralpha = 0.01\rweights = ones(n) #initialize to all ones\rfor i in range(m):\rh = sigmoid(sum(dataMatrix[i]*weights))\rerror = classLabels[i] - h\rweights = weights + alpha * error * dataMatrix[i]\rreturn weights\r 当存在非线性可分的点时，会导致出现较大的波动。另外，需要加快收敛速度.故而引入改进的随机梯度上升.\n\rdef stocGradAscent1(dataMatrix, classLabels, numIter=150):\rm,n = shape(dataMatrix)\rweights = ones(n) #initialize to all ones\rfor j in range(numIter):\rdataIndex = range(m)\rfor i in range(m):\ralpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant\rh = sigmoid(sum(dataMatrix[randIndex]*weights))\rerror = classLabels[randIndex] - h\rweights = weights + alpha * error * dataMatrix[randIndex]\rdel(dataIndex[randIndex])\rreturn weights\r alpha = 4/(1.0+j+i)+0.0001 是为了让α随迭代逐步减小到逼近0.（但是并不是严格单调递减）\n另外，区别于上个算法，这个算法使用随机的样本来更新权重系数，这样做的好处是能能够减少周期波动.\n5.分类评估方法  混淆矩阵：\n其中TP和TN是正确预测的.\n准确率(accuracy)：含义为所有样本中预测正确的占比 (TP+TN) / (TP+TN+FP+FN)\n精确率(precision)：含义为预测为正例的样本中真实为正的占比，代表查的准不准 (TP) / (TP+FP)\n召回率(recall)：含义为真实为正例中预测为正的占比，表示对正样本的区分能力，查的全不全 (TP) / (TP+FN)\nF1-score：含义为模型的稳健性 (2TP) / (2TP+FN+FP) = 2precisionrecall / recall+precision\n这些评估方法在sklearn中的实现如下图所示：\n真阳率TPR，即召回率：含义为检测出来的真阳性样本数除以所有真实阳性样本数 (TP) / (TP+FN)\n假阳率FPR：含义为检测出来的假阳性样本数除以所有真实阴性样本数 (FP) / (TN+FP)\nROC曲线：x轴为FPR，y轴为TPR.当预测所有样本为1时，曲线的结果为中间的虚线.\nAUC指标，即随机取一样本，正样本大于负样本的概率.范围在[0.5,1]之间.当为1时为完美分类器.这个指标一般用于不平衡二分类问题\n需要注意的是，在sklearn中使用AUC计算API时，必须0为反例1为正例（一般取样本多的为正例，比如本实验中，4代表恶性，样本多，取为1）.\nROC曲线的绘制过程如下：\n1.先将各个点的概率从高到低排序.\n2.挑选最高的，假设其为正例.如果其概率大于阈值，那么就是真正例，否则就是假正例.每次挑选，都要就是那个FPR和TPR\n3.迭代直到所有点都求出TPR和FPR.在轴上描点，结果就是ROC曲线.如下图所示，取阈值为0.75\nAUC其实就是ROC曲线的积分，也即面积.其表示分对的概率\n6. sklearn实现  sklearn-linearRegression\n下面是该api的一些常见参数\n其中penalty 有L1和L2，详见线性回归的博客；C代表正则化力度，C越大，惩罚越大；solver代表使用的梯度下降算法.\nimport pandas as pd\rimport numpy as np\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import roc_auc_score\r# 1.获取数据\r# 2.基本数据处理\r# 2.1 缺失值处理\r# 2.2 确定特征值,目标值\r# 2.3 分割数据\r# 3.特征工程(标准化)\r# 4.机器学习(逻辑回归)\r# 5.模型评估\r# 1.获取数据\rnames = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\r'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\r'Normal Nucleoli', 'Mitoses', 'Class']\rdata = pd.read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\u0026quot;,names=names)\r# 2.基本数据处理\r# 2.1 缺失值处理\rdata = data.replace(to_replace=\u0026quot;?\u0026quot;, value=np.nan)\rdata = data.dropna()\rx = data.iloc[:, 1:-1]\ry = data[\u0026quot;Class\u0026quot;]\r# 2.3 分割数据\rx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2, test_size=0.2)\r# 3.特征工程(标准化)\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(逻辑回归)\restimator = LogisticRegression()\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 基本评估\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是：\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是：\\n\u0026quot;, score)\r# 5.2 其他评估\rret = classification_report(y_test, y_pre, labels=(2,4), target_names=(\u0026quot;良性\u0026quot;, \u0026quot;恶性\u0026quot;))\rprint(ret)\r# 不平衡二分类问题评估方法\ry_test = np.where(y_test\u0026gt;3, 1, 0)\rroc_auc_score(y_true=y_test, y_score=y_pre)\r 7. 总结 本文主要介绍了逻辑回归的原理和源码，并使用梯度上升\u0026ndash;随机梯度上升来更新参数，求最优解.另外，本文介绍了一些分类评估的方法.最后，对于上述内容，本文介绍了sklearn API的详细实现过程.\n逻辑回归的优点：\n(1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;\n(2)简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响;\n(3)适合二分类问题，不需要缩放输入特征;\n(4)内存资源占用小，因为只需要存储各个维度的特征值;\n(5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题\n(6)以概率的形式输出，而非知识0.1判定，对许多利用概率辅助决策的任务很有用\n逻辑回归的缺点：\n(1)不能用逻辑回归去解决非线性问题，因为Logistic的决策面试线性的;\n(2)对多重共线性数据较为敏感;\n(3)很难处理数据不平衡的问题;\n(4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;\n(5)逻辑回归本身无法筛选特征，有时会用gbdt来筛选特征，然后再上逻辑回归。\n逻辑回归的适用场景：\n如是否患病，金融诈骗，虚假账号等二分类问题\n8. ps.  numpy中mat.getA 将矩阵转换为ndarray\nones((n, 1))，n代表几行，1代表每行有一个数.\n关于loc和iloc\n前者是根据标签索引进行查找，后者是根据整数索引进行查找.\n使用案例：\ndf.loc([['a','j'], ['name','score']]) #定位行索引值为a，j和列索引值为name和score的dataframe\rdf.iloc([[0,9],[0,2]]) #定位第0,9行，第0,2列的dataframe\r# 切片用法，loc前后都闭合，而iloc前闭后开\rdf.loc(['a':'j', 'name':'score']) #定位行索引值为a到j和列索引值为name到score的dataframe\rdf.iloc([0:9,0:2]) #定位第0到8行，第0到1列的dataframe\r drop的用法\n像本案例的情况 还是使用replace更好\n","date":"2022-06-03","permalink":"https://wudaore.github.io/post/logistic-regression/","tags":["机器学习","algo"],"title":"逻辑回归学习"},{"content":"目录 基本原理 线性回归的损失优化 局部加权线性回归 正则化线性模型 逐步向前回归 sklearn用法 总结 ps. 1. 基本原理 1.1 分类和回归 区别在于分类问题是定性的，输出离散值（如+1，-1）.而回归问题是定量的，输出连续值（如预测明天的气温为36.5度）\n1.2 基本原理 只有在线性可分的情况下才能使用.大概就是求一组参数w使得y能拟合wx\n2. 线性回归的损失优化 2.1 损失\u0026mdash;最小二乘法 最小，即最小化.二乘，即真实值-预测值的平法.普通最小二乘法就是要最小化这个平方值.\n由此，可以得到模型的损失函数\n2.2 优化\u0026mdash;正规方程 将上式写作矩阵形式.因为要求差值（j(θ)）的极值，所以两边对θ求导=0，解得θ的值\n最后解得w(即θ)的最优解\n特点：一蹴而就，一下子就能算出来，但是只适合样本和特征比较少的情况.\n2.3 优化\u0026mdash;梯度下降（更多用到） 梯度下降的公式如下：\n原理就是逐步降低梯度，更新数据，如此循环，知道梯度无限趋近于0，找到一定区间内的极小值.梯度下降无法保证找到最小值.\n其中参数α为步长，步长太大容易导致跳过极值点，太小导致计算缓慢.\n2.4 各种梯度下降算法 fg，全梯度下降 计算所有样本的误差平均值作为目标函数. 时间长，内存消耗大（一般不用）\nsag，随机平均梯度下降给每个样本都维持一个平均值.后期计算的时候参考这个平均值. 初期不佳，优化慢，因为该算法将初始梯度设为1，每轮梯度更新都结合上一轮.（首选）\nsg，随机梯度下降每次只选择一个样本 . 能快速将平均损失函数降到很低，但是必须注意步长，且无法代表整体样本（一般不用）\nmini-batch，小批量梯度下降选择一部分样本. 介于SG和FG之间（次选）\n2.5 正规方程和梯度下降两者对比 3. 局部加权线性回归LWLR  线性回归容易出现欠拟合.所以可以引入一些误差来降低均方误差.\n对待求点附近每个点加权（即赋予核），对于更近的点，将赋予更高的权重.\n对于权重W，《实战》中使用的是高斯核.显然，越近的点权重越大.\n公式中还有一个人为规定的参数k.k越大，越多的数据被用于训练.\n\rdef lwlr(testPoint,xArr,yArr,k=1.0):\rxMat = mat(xArr); yMat = mat(yArr).T\rm = shape(xMat)[0]\rweights = mat(eye((m)))\rfor j in range(m):\rdiffMat = testPoint - xMat[j,:]\rweights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))\rxTx = xMat.T * (weights * xMat)\rif linalg.det(xTx) == 0.0: #需要检测矩阵是否可逆.\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = xTx.I * (xMat.T * (weights * yMat))\rreturn testPoint * ws\rdef lwlrTest(testArr,xArr,yArr,k=1.0): #对于每个点，都要遍历xarr计算权重\rm = shape(testArr)[0]\ryHat = zeros(m)\rfor i in range(m):\ryHat[i] = lwlr(testArr[i],xArr,yArr,k)\rreturn yHat\r 4.正则化线性模型（岭回归和lasso回归）  4.0 欠拟合和过拟合. 欠：在训练集和测试集上表现都不包\n解决方法：\n1.添加其他特征\n2.添加多项式特征.\n过：在训练集上好 测试集上不好.\n解决方法：\n1.重新清洗数据\n2.增大训练量\n3.正则化 即尽量减少高次项的影响.分为L1正则化（直接使w的一些值为0，如LASSO回归）和L2正则化(使得一些高次项的w值接近0，削弱某个特征的影响，如ridge岭回归)\n4.减少特征维度，防止维度灾难\n4.1 岭回归（推荐） 线性回归和局部加权的线性回归都存在求逆的问题.而当矩阵不可逆时（或者输入线性相关，约等于不可逆），可以引入岭回归.岭回归不仅解决矩阵不可逆的情况，还引入惩罚项，防止过拟合.\n相较于线性回归，岭回归引入了惩罚项如下：\n经过与线性回归相似的推导（求导，使极值为0）,得到岭回归的w最优解：\n岭回归的具体推导过程\n\rdef rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays\rreturn ((yArr-yHatArr)**2).sum()\rdef ridgeRegres(xMat,yMat,lam=0.2):\rxTx = xMat.T*xMat\rdenom = xTx + eye(shape(xMat)[1])*lam\rif linalg.det(denom) == 0.0:\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = denom.I * (xMat.T*yMat)\rreturn ws\rdef ridgeTest(xArr,yArr):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #to eliminate X0 take mean off of Y\r#regularize X's\rxMeans = mean(xMat,0) #calc mean then subtract it off\rxVar = var(xMat,0) #calc variance of Xi then divide by it\rxMat = (xMat - xMeans)/xVar\rnumTestPts = 30\rwMat = zeros((numTestPts,shape(xMat)[1]))\rfor i in range(numTestPts):\rws = ridgeRegres(xMat,yMat,exp(i-10))\rwMat[i,:]=ws.T\rreturn wMat\r 4.2 lasso 相较于岭回归，lasso只是更改了惩罚项.\n岭回归通过增加平方项，并结合约束条件来约束高次项w的值（L2正则），顾其图线会是圆润的.而lasso回归通过增加绝对值项，并结合约束条件使得一些w项为0，表现在图线上就是不可导点，是尖锐的（在添加约束条件\u0026ndash;wk 的绝对值求和小于λ\u0026ndash;后，当λ足够小，会使一些系数被迫降到0.）.\n通过这种方法，能够自动进行特征选择.\nlasso可以更好的帮助分析数据（庞大特征数量下的特征选择），但是却大大增加了计算量且不太稳定.所以提出既能分析数据又能减少计算量的方法\u0026ndash;逐步向前回归\n4.3 Elastic Net 弹性网络 弹性网络通过混合比参数r控制岭回归和lasso回归.\n一般情况下优先使用岭回归，岭回归不适合的话才考虑弹性网络，最后才是lasso\n4.4 Early Stopping 当错误率达到阈值时，停止迭代.\n5.逐步向前回归  逐步向前回归的原理大概是:标准化数据后，经过若干次迭代，每次迭代改变一个系数得到新的w（分为增大和减小）.如新的w误差更小，则取新的w.\n算法共有两个可以调节的参数，即迭代次数和迭代步长.\ndef stageWise(xArr,yArr,eps=0.01,numIt=100):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #can also regularize ys but will get smaller coef\rxMat = regularize(xMat)\rm,n=shape(xMat)\rreturnMat = zeros((numIt,n)) #testing code remove\rws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()\rfor i in range(numIt):\rprint(ws.T)\rlowestError = inf; for j in range(n):\rfor sign in [-1,1]:\rwsTest = ws.copy()\rwsTest[j] += eps*sign\ryTest = xMat*wsTest\rrssE = rssError(yMat.A,yTest.A)\rif rssE \u0026lt; lowestError:\rlowestError = rssE\rwsMax = wsTest\rws = wsMax.copy()\rreturnMat[i,:]=ws.T\rreturn returnMat\r 6.sklearn用法  6.1 线性回归 可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n正规方程：\n梯度下降：\n具体案例使用如下（使用正规方程和梯度下降）：\n预测波士顿房价\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据\r2.数据基本处理\r2.1 数据集划分\r3.特征工程 --标准化\r4.机器学习(线性回归)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_boston\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, RidgeCV\rfrom sklearn.metrics import mean_squared_error\rdef linear_model1():\r\u0026quot;\u0026quot;\u0026quot;\r正规方程\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(线性回归)\restimator = LinearRegression()\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rdef linear_model2():\r\u0026quot;\u0026quot;\u0026quot;\r梯度下降法\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\restimator = SGDRegressor(max_iter=1000, learning_rate=\u0026quot;constant\u0026quot;, eta0=0.001)\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rif __name__ == '__main__':\rlinear_model1()\rlinear_model2()\rlinear_model3()\r 6.2 岭回归 岭回归是线性回归的正则化版本.在sklearn中，使用SGDRegressor，设置参数penalty=l2，也可以实现岭回归，但是只能实现普通的梯度下降.而使用 Ridge可以实现随机平均梯度下降SAG\n对于参数alpha，根据上面岭回归的公式，alpha越大，正则化力度越大，权重系数就越小.\n对于参数normalize，如果为true，就可以不必进行上面的标准化.\n可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n另外，该博客对参数和返回值做了一些中文解释，方便阅读和理解.点击跳转\n\rdef linear_model3():\r\u0026quot;\u0026quot;\u0026quot;\r岭回归\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\r# estimator = Ridge()\restimator = RidgeCV(alphas=(0.001, 0.1, 1, 10, 100))\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\r 6.3 lasso 官网\n注释参数博客\n7. 总结  从本文总结的算法来看，主要经过以下流程：线性回归(容易欠拟合)\u0026ndash;\u0026gt;加权线性回归(容易过拟合且无法处理不可逆矩阵)\u0026ndash;\u0026gt;岭回归\u0026ndash;(不好分析数据)\u0026ndash;\u0026gt;lasso(计算量大)\u0026ndash;\u0026gt;逐步向前回归.\n通过加权线性回归我们知道，越小的核越能拟合训练数据.当然，也越容易过拟合.而lasso和逐步向前回归允许我们参考输入向量中每一维数据的作用大小，从而可以对数据进行缩减.\n然而，上述算法终究只适合线性的数据.对于非线性的数据，需要使用其他算法.\n8.ps.  1.numpy.matrix numpy.matrix表示矩阵，使用numpy.matrix.A方法可以像数组一样根据下标取出值，不能直接用下标.\n官方文档\n2.numpy.eye() 生成对角阵.\n具体链接\n3.矩阵求导 在推导正规方程时使用了矩阵的求导公式，点击跳转\n","date":"2022-06-02","permalink":"https://wudaore.github.io/post/linear-regression/","tags":["机器学习","algo"],"title":"线性回归学习"},{"content":"Introduction 基本原理 各种距离 距离调参 KNN优化-KD树 数据处理（归一化和标准化） 实战\u0026ndash;鸢尾花数据集 总结 1. 基本原理 非常朴素的原理。根据距离最近的K个点来判断目标点的类别.\n2. 各种距离 标准距离\n汉明距离\n杰卡德距离\n马氏距离\n余弦距离\n3. 距离调参 p=1 曼哈顿距离 2 欧式距离 无穷 切比雪夫距离 三者都是闵式距离.\n4.KNN优化-KD树 4.1 KD树基本原理 根据KNN的基本原理可知，当需要预测一个点时，需要计算训练集中每个店到它的距离.当数据集很大时，对于N个样本，D个特征的数据集，算法的时间复杂度到达O(D*N^2)\nKD树的基本原理是通过树来分割数据，假设A和B的距离很远，B和C的距离很近，那么A和C的距离一定很远.通过这种方法，可以跳过很多距离远的点，减少计算成本.\n优化后算法的复杂度为O(DNlog(N))\n4.2 KD树的实现 假设有数据点{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}\n要根据X和Y的方差计算第一次分割的平面.X的方差更大所以选择首先分割X轴，选择其中间的(7,2),切割\n第二次根据Y进行分割平面.\n左：(2,3),(4,7),(5,4) \u0026ndash;\u0026gt;3,4,7\n右：(8,1),(9,6) \u0026ndash;\u0026gt;1,6\n对点(5,4)和(9,6)进行切割\n第三次数据集中的点已经很少了，直接进行切割.得到结果如下：\n4.3 KD树的最近邻查找 以上述数据集为例，现在要查找点(2.1,3.1)则根据x-y-x的顺序遍历构造好的KD树，发现经过以下路径：\n\u0026lt;(7,2),(5,4),(2,3)\u0026gt; 反向遍历该路径.\n先假设(2,3)为最近点，计算与其欧式距离为0.141.以(2.1,3.1)为圆心，以0.141这个距离为半径画圆，发现不能与Y=4相交.因为(5,4)点是根据Y分割的，所以没必要到该点的右平面去查找.\n同样，圆不能与x=7相，所以也没必要到(7,2)的右平面查找.可知最近点为(2,3)\n而以点(2,4.5)为例，得到查找路径 \u0026lt;(7,2),(5,4),(4,7)\u0026gt; .先假设(4,7)为最优解，并按照上述步骤画圆.(4,7)和(5,4)出队列后发现，该圆能将(5,4)包括进去.意味着此时的最优点已经发生了变化，半径也变成了3.04.所以需要到(5,4)的左平面查找.此时的查找队列为\u0026lt;(7,2),(2,3)\u0026gt;\n继续回溯队列到(2,3)，此时发现该点更近，更新为最优解.距离更新为1.5\n最后回溯至(7,2)发现圆无法包括，不需要找(7,2)的右平面.查找完毕.\n以上只是举了例子，更详细的原理：https://www.bilibili.com/video/BV19f4y1R7vh?spm_id_from=333.337.search-card.all.click\n5.数据处理 5.1 归一化 将数据映射到[0,1]区间内.公式如下图：\nsklearn提供了对应的API接口.具体实现代码如下：\n# # 1.归一化\r# # 1.1 实例化一个转换器\rtransfer = MinMaxScaler(feature_range=(0, 1))\r# # 1.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过归一化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 5.1 标准化 异常点对归一化的影响特别大，只适合传统精确小数，鲁棒性较差.\n标准化：通过转换将数据转换为均值为0，标准差为1的范围内，公式如下：\nsklearn提供了对应的API接口.具体实现代码如下：\n# 2.标准化\r# 2.1 实例化一个转换器\rtransfer = StandardScaler()\r# 2.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过标准化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 6.实战\u0026ndash;鸢尾花数据集(交叉验证，网格搜索) KNeighborsClassifier 可以使用algorithm参数指定使用的算法，包括ball树，暴力和kd树.具体见sklearn官网https://scikit-learn.org.cn/view/85.html\n交叉验证：将训练集分为训练集和验证集.交叉验证并不能提高准确率 但是能使模型更加可信 网格搜索：即将超参数(需要手动指定的参数)通过字典的形式传入，进行最优选择\nGridSearchCV的参数如下：https://scikit-learn.org.cn/view/655.html\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据集\r2.数据基本处理\r3.特征工程\r4.机器学习(模型训练)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_iris\rfrom sklearn.model_selection import train_test_split, GridSearchCV\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.neighbors import KNeighborsClassifier\r# 1.获取数据集\riris = load_iris()\r# 2.数据基本处理\r# 2.1 数据分割\rx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\r# 3.特征工程\r# 3.1 实例化一个转换器\rtransfer = StandardScaler()\r# 3.2 调用fit_transform方法\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(模型训练)\r# 4.1 实例化一个估计器\restimator = KNeighborsClassifier()\r# 4.2 调用交叉验证网格搜索模型\rparam_grid = {\u0026quot;n_neighbors\u0026quot;: [1, 3, 5, 7, 9]}\restimator = GridSearchCV(estimator, param_grid=param_grid, cv=10, n_jobs=-1)\r# 4.3 模型训练\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 输出预测值\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rprint(\u0026quot;预测值和真实值对比:\\n\u0026quot;, y_pre == y_test)\r# 5.2 输出准确率\rret = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, ret)\r# 5.3 其他评价指标\rprint(\u0026quot;最好的模型：\\n\u0026quot;, estimator.best_estimator_)\rprint(\u0026quot;最好的结果:\\n\u0026quot;, estimator.best_score_)\rprint(\u0026quot;整体模型结果:\\n\u0026quot;, estimator.cv_results_)\r 7.总结  k近邻的优点如下：\n1.简单有效\n2.重新训练代价低\n3.适合大样本自动分类\n该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n4.适合类域交叉样本\nKNN方法主要靠周围有限的邻近的样本,而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合\n缺点如下：\n1.惰性学习\nKNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多\n2.类别评分非规格化\n不像一些通过概率评分的分类\n3.输出的可解释性不强\n例如决策树的输出可解释性就较强\n4.不擅长不均衡样本\n当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。\n5.计算量太大\n","date":"2022-05-29","permalink":"https://wudaore.github.io/post/knn/","tags":["机器学习","algo"],"title":"KNN学习"},{"content":"Introduction 对matplotlib的简易学习\nimport matplotlib.pyplot as plt import random  matplotlib图像的绘制 # 1. 创建画布 # figsize 画布大小 dpi 像素 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 3.图像展示 plt.show()  # help(plt.figure)  图像保存 # 1. 创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 2.1 图像保存 plt.savefig(\u0026quot;./data/test.png\u0026quot;) # 3.图像展示 plt.show() # 图像保存一定要放到show前面 # # 2.1 图像保存 # plt.savefig(\u0026quot;./data/test.png\u0026quot;)  案例：显示温度变化状况 图像基本绘制功能演示 # 解决中文显示不正常 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False # 0.生成数据 x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图形绘制 plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] plt.yticks(y_ticks[::5]) plt.xticks(x[::5], x_ticks_labels[::5]) # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 # 第一个参数数字的功能大概是横坐标的实际个数 # 2.2 添加网格 plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # 2.3 添加描述 plt.xlabel(\u0026quot;时间\u0026quot;) plt.ylabel(\u0026quot;温度\u0026quot;) plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # 图例对应plt.label plt.legend(loc='best') # 3.图像展示 plt.show()  多个坐标系显示图像 # 0.生成数据 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] y_tianjin = [random.uniform(10, 19) for i in x] y_zhejiang = [random.uniform(12, 25) for i in x] # 1.创建画布 # plt.figure(figsize=(20, 8), dpi=100) # 多个坐标系 使用subplots 参数：几行几列 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 16), dpi=100) # 2.图形绘制 # plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) # plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[0][0].plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[0][1].plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[1][0].plot(x, y_tianjin, label=\u0026quot;天津\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[1][1].plot(x, y_zhejiang, label=\u0026quot;浙江\u0026quot;) # # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] # plt.yticks(y_ticks[::5]) # plt.xticks(x[::5], x_ticks_labels[::5]) # # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 axes[0][0].set_xticks(x[::5]) axes[0][0].set_yticks(y_ticks[::5]) axes[0][0].set_xticklabels(x_ticks_labels[::5]) axes[0][1].set_xticks(x[::5]) axes[0][1].set_yticks(y_ticks[::5]) axes[0][1].set_xticklabels(x_ticks_labels[::5]) # # 2.2 添加网格 # plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][0].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][1].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # # 2.3 添加描述 # plt.xlabel(\u0026quot;时间\u0026quot;) # plt.ylabel(\u0026quot;温度\u0026quot;) # plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) axes[0][0].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][0].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][0].set_title(\u0026quot;北京一小时温度变化图\u0026quot;, fontsize=20) axes[0][1].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][1].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][1].set_title(\u0026quot;上海一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # plt.legend(loc=0) axes[0][0].legend(loc=0) axes[0][1].legend(loc=0) # 3.图像展示 plt.show()  plot绘制数学图像 import numpy as np  # 0.生成数据 x = np.linspace(-10, 10, 1000) y = x*x*x # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.绘制 plt.plot(x, y) plt.grid() # 3.显示 plt.show()    ","date":"2022-05-24","permalink":"https://wudaore.github.io/post/hello_matplotlib/","tags":["机器学习"],"title":"matplotlib基础"},{"content":"Introduction \u0026laquo;实战\u0026raquo;中的svm数学推导实在是有些晦涩难懂.\nbilibili视频教程：https://www.bilibili.com/video/BV1Hs411w7ci?spm_id_from=333.337.search-card.all.click\n本文主要总结一部分svm的数学推导.后续再补全以及学习SVM的使用\n在开始之前，需要了解一些基本的概念\n1. 基础概念 1.1 wTx=b 以二维空间为例子，wTx 是指向量x在向量W上的投影长度乘以向量W的长度.wTx=b几何上来说所有乘积为b的向量x构成的一个平面.且该平面显然垂直于向量x.\n当出现空间中的另一个点a，想要计算该点到上述超平面的距离，可先将该点与x向量与w向量的起始点连接，形成x,向量.根据公式\n进行计算.详细推导见博客https://blog.csdn.net/Zelf0914/article/details/95811715\n理解之后不难发现，参数b的作用是控制超平面前后移动\nsvm \u0026mdash;间隔 对偶 核技巧\n2.硬间隔SVM 即最大间隔分类器.适用于数据线性可分.\n核心就是要找出一个超平面，使其对于“最近的点”的距离最大.（数据集中有很多点，对当前点的距离大了会导致和其他点的距离变小，这个时候其他点就变成“最近的点”了.)硬间隔SVM就是要求出对于“最近的点”的距离的最大值，即\n其中yi对应书中的label，当wTx\u0026gt;0时yi=1，反之为-1\n而|wTx+b|等价于yi（wTx+b） （因为yi只有可能是正负1且wTx+b时yi为+1，反之为-1.这样的转换是等价的.）\n替换后提出1/||w||（因为min内计算的是点到直线的距离，和向量w无关.||w||表示n维向量的模，称为范数），令min[yi（wTx+b）]=1（wTx+b 本质是一个向量，对其进行缩放不影响，就比如二维空间中的直线，所以设为1其实是设了个最小值）\n此时等式就变成了求max（1/||w||），等价于求min（1/2 wTw）(加1/2是为了方便求最小值？)\n推导过程见下图\n式子的目标函数是二次函数，限制条件是N个线性不等式条件，所以这是凸二次优化问题，可以用拉格朗日乘子法和KKT条件进行求解。拉格朗日乘子法可以寻找多元函数在一组约束条件下的极值，将d个变量与k个约束的最优化问题转变成d+k个变量的无约束问题。\n2.1 拉格朗日乘子法 经过上述推导，问题就变成了带有不等式约束条件的最优化问题（约束条件为yi（wTx+b）\u0026gt;=1）.\n借助拉格朗日乘子 将带约束问题转换成无约束问题（考研数学做过类似的题型，也可参考博客https://blog.csdn.net/lijil168/article/details/69395023），\n同时，1/2wTw 又等价于maxλL(w，b，λ)，故而可以如下进行转换，如下图：\n该问题的目标函数是二次的，约束又是线性的，故而满足强对偶关系.故而进行以下转换：\nL先对b求偏导，令结果=0，并带回L原式：\n继续对w求偏导.因为wTw=wwT，有以下转换：\n此时的w已经是最优解（？）\n最终得到如下转换\n2.2 KKT条件 满足强对偶关系（如上述，目标函数是二次且约束是线性）的充要条件是满足KKt条件.\nKKT条件：\n可知w和b都是关于x和y的组合.同时，λi只在支持向量上有意义，其他时候都是0\n支持向量即分类结果中平行于分类平面的过最近点的两个向量（不知道这个理解对不对），只有在支持向量上，wwT+b的绝对值才等于1，λi才有意义.\n软间隔SVM soft 指允许一点点误差.只需要在原目标函数后加一个误差函数loss即可：min（1/2 wTw）+loss\nloss如果使用yi(wwT+b)的点的个数的话会导致结果离散，不好求导.所以一般使用距离。即loss = max{0, 1-yi(wwT+b)},这就是hinge loss\n引入ξ表示1-yi(wwT+b)，即误差的距离.调整约束条件和目标函数，的软间隔svm模型：\n写在最后 非数学系，数学功底不好，做这个东西真的困难.还是了解一下，不做深入证明（如强弱对偶性的证明，太头疼），更偏向代码和运用一些.\n","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm1/","tags":["机器学习","algo"],"title":"svm学习(1)--基础概念和公式"},{"content":"Introduction SMO算法 即序列最小优化算法 对\u0026laquo;实战\u0026raquo;中的smoSimple函数尝试理解\n \rdef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\r\u0026quot;\u0026quot;\u0026quot;smoSimple\rArgs:\rdataMatIn 特征集合\rclassLabels 类别标签\rC 松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。\r控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。\r可以通过调节该参数达到不同的结果。\r其实就是上一篇博客中提到的loss值\rtoler 容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）\rmaxIter 退出前最大的循环次数\rReturns:\rb 模型的常量值\ralphas 拉格朗日乘子\r\u0026quot;\u0026quot;\u0026quot;\rdataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\rb = 0; m,n = shape(dataMatrix)\r# alpha 初始全为0\ralphas = mat(zeros((m,1)))\rprint(len(alphas))\rprint(len(dataMatrix))\riter = 0\rwhile (iter \u0026lt; maxIter):\r# 记录alpha是否已经优化\ralphaPairsChanged = 0\rfor i in range(m):\r# 预测的类别\r# 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*label[n]*x[n]\r# 理解了半天 被这个括号害死了\rfXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\r# 预测结果与真是结果比对 得到误差\rEi = fXi - float(labelMat[i])#if checks if an example violates KKT conditions\r# 如果误差大的话，就要进行优化.选择另一个向量j\rif ((labelMat[i]*Ei \u0026lt; -toler) and (alphas[i] \u0026lt; C)) or ((labelMat[i]*Ei \u0026gt; toler) and (alphas[i] \u0026gt; 0)):\rj = selectJrand(i,m)\rfXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\rEj = fXj - float(labelMat[j])\ralphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\r# L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句\r# labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。\rif (labelMat[i] != labelMat[j]):\rL = max(0, alphas[j] - alphas[i])\rH = min(C, C + alphas[j] - alphas[i])\relse:\rL = max(0, alphas[j] + alphas[i] - C)\rH = min(C, alphas[j] + alphas[i])\rif L==H: print (\u0026quot;L==H\u0026quot;); continue\r# eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程\r# 参考《统计学习方法》李航-P125~P128\u0026lt;序列最小最优化算法\u0026gt;\reta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\rif eta \u0026gt;= 0: print (\u0026quot;eta\u0026gt;=0\u0026quot;); continue\r# 计算出一个新的alphas[j]值\ralphas[j] -= labelMat[j]*(Ei - Ej)/eta\r# 并使用辅助函数，以及L和H对其进行调整\ralphas[j] = clipAlpha(alphas[j],H,L)\rif (abs(alphas[j] - alphaJold) \u0026lt; 0.00001): print(\u0026quot;j not moving enough\u0026quot;); continue\r# 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反\ralphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\r# 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。\r# w= Σ[1~n] ai*yi*xi =\u0026gt; b = yj- Σ[1~n] ai*yi(xi*xj)\r# 所以： b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)\r# 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍\rb1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\rb2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\rif (0 \u0026lt; alphas[i]) and (C \u0026gt; alphas[i]): b = b1\relif (0 \u0026lt; alphas[j]) and (C \u0026gt; alphas[j]): b = b2\relse:\rb = (b1 + b2)/2.0\ralphaPairsChanged += 1\rprint(\u0026quot;iter: %d i:%d, pairs changed %d\u0026quot; % (iter, i, alphaPairsChanged))\r# 在for循环外，检查alpha值是否做了更新，如果更新则将iter设为0后继续运行程序\r# 直到更新完毕后，iter次循环无变化，才退出循环。\rif (alphaPairsChanged == 0):\riter += 1\relse:\riter = 0\rprint(\u0026quot;iteration number: %d\u0026quot; % iter)\rreturn b, alphas\r ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm2/","tags":["机器学习","algo"],"title":"svm学习(2)--简易SMO算法代码理解"},{"content":"Introduction sklearn的SVM算法可以调整c值和核函数.本篇并未介绍核函数\n 导入数据和数据-标签的处理\nfrom sklearn.svm import SVC\rfrom sklearn import datasets\riris = datasets.load_iris()\rX = iris['data']\rX = iris['data'][:,(2,3)]\ry = iris['target']\r SVM训练.c值设置为无穷大代表几乎不容错 使用线性核函数\nsetosa_or_versicolor = (y==0)|(y==1)\rX = X[setosa_or_versicolor]\ry = y[setosa_or_versicolor]\rsvm_clf = SVC(kernel='linear', C=float('inf'))\rsvm_clf.fit(X,y)\rw = svm_clf.coef_\rb = svm_clf.intercept_\rw\r 使用matplotlib绘图\nimport numpy as np\rimport matplotlib\rimport os\rimport matplotlib.pyplot as plt\rimport math\r%matplotlib inline\rplt.rcParams['axes.labelsize']=14\rplt.rcParams['xtick.labelsize']=12\rplt.rcParams['ytick.labelsize']=12\rx0=np.linspace(0, 5.5, 200) pred_1=5*x0-20 pred_2=x0-1.8 pred_3=0.1*x0+0.5 def plot_svc_decision_boundary(svm_clf,xmin,xmax,sv=True):\rw= svm_clf.coef_[0]\rb=svm_clf.intercept_[0]\rprint(w)\rx0=np.linspace(xmin,xmax,200)\rdecision_boundary=-w[0]/w[1] *x0-b/w[1]\r#这里的margin算的并不是1/||w||,而是1/w[1]，是为了方便绘制支持向量\r#margin = 1 / np.sqrt(np.sum(svm_clf.coef_**2))\rmargin=1/w[1]\rprint(margin)\rgutter_up=decision_boundary+margin\rgutter_down=decision_boundary-margin\rif sv:\rsvs = svm_clf.support_vectors_\rplt.scatter(svs[:,0], svs[:,1], s=180, facecolors='#FFAAAA')\rplt.plot(x0, decision_boundary, 'k-', linewidth=2)\rplt.plot(x0, gutter_up, 'k-', linewidth=2)\rplt.plot(x0, gutter_down, 'k-', linewidth=2)\rplt.figure(figsize=(14,4))\rplt.subplot(121)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.plot(x0,pred_1,'g--',linewidth=2)\rplt.plot(x0,pred_2,'m-',linewidth=2)\rplt.plot(x0,pred_3,'r-',linewidth=2)\rplt.axis([0,5.5,0,2])\rplt.subplot(122)\rplot_svc_decision_boundary(svm_clf, 0, 5.5)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.axis([0,5.5,0,2])\r 结果如下： ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm3/","tags":["机器学习","algo"],"title":"svm学习(3)--SVM实现鸢尾花数据分类"},{"content":"Introduction 系统的学习Linux对日常学习和之后的工作都有帮助\n本篇博客记录学习ubuntu文件系统时遇到的问题和需要记录的点\n1.ubuntu 常见目录 ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/ubuntu-filesys/","tags":["linux"],"title":"ubuntu学习(1)--文件系统"},{"content":"Introduction 本文主要介绍如何在windows下使用hugo搭建自己的博客并通过github挂载，同时绑定自己的域名\nbilibili视频教程：https://www.bilibili.com/video/BV13c411h7k7?spm_id_from=333.999.0.0\n本文主要总结一些踩的坑以及功能拓展\nstep1 安装配置hugo hugo中文文档 ：https://www.gohugo.org/\nhugo编译文件下载地址：https://github.com/gohugoio/hugo/releases\n解压后放入bin文件夹内，将bin添加到系统变量，如下图所示\n打开cmd输入hugo version，若能显示版本，则配置成功\nstep2 创建博客 2.1创建项目文件 在cmd中输入以下指令创建项目文件.\nhugo new site yourname.com  2.2选择博客样式 在https://themes.gohugo.io/中选择下载博客的样式.本博客使用fuji模板，https://themes.gohugo.io/themes/hugo-theme-fuji/\n样式下载方法：直接git clone到项目文件夹下的themes中（ssh），或下载压缩包解压到themes中（http），如下图所示\n下载后需要在项目文件夹下的config.toml中修改样式名.同时可以修改博客名等字段\n注意项目名为应该与模板中的文件夹名相同，如下图所示，项目名就应该是hermit.\n进入hermit（以hermit模板为例）下的ExampleSite文件夹中，复制content和config.toml粘贴到项目文件夹xxx.com下.其中content为博客模板，是markdown文件，config.toml是配置项.\n导入完毕后在项目文件夹xxx.com下的cmd中执行命令.\nhugo  即可编译项目.\nstep3 通过github挂载 3.1建立个人仓库 注册github账号后点击头像-Your Repositories即可新建/管理仓库\n个人仓库名必须是github用户名+github.io，否则会报错.readme文档暂时不添加，防止冲突\n作者使用的是master分支而非main，这需要进入setting-branch中进行配置.还需要使用git push origin -d main 删除main分支\n建立仓库后通过git-scm.com下载最新版git，同样需要将git的bin文件加入系统变量\n3.2本地代码同步到git 首先在项目文件夹.com下的cmd中输入hugo编译项目.编译完成后出现public文件夹，即编译后的文件.\n进入public，右键Git Bash Here进入Git命令行.相继输入\ngit init git add -A git commit -m\u0026quot;init\u0026quot; git remote add origin https://github.com/你的用户名/你的用户名.github.io.git git push -f origin master  如果是第一次使用git，还需要进行验证（如果是http方式则会跳出窗口，如果是ssh方式则需要设置密钥公钥，详见博客https://www.xuanfengge.com/using-ssh-key-link-github-photo-tour.html）\npush完毕后打开github仓库，就能发现public中的内容已经被存到仓库中了\n此时浏览器访问用户名.github.io即可进入博客.\nstep4 关联到域名 4.1域名解析 本文使用的是腾讯云.进入控制台选择域名解析，添加以下配置：\n4.2github中的配置 进入项目仓库-settings-page，在Custom domain中配置自己的域名，等待解析完成即可.此页面也可配置默认分支为master，就不需要每次都修改.\n设置完毕后就可以通过自己的域名访问博客.\nstep5 新增，删除或者修改博客 所有的博客都保存在content/posts中（当然也可改名为blogs等，无伤大雅）.修改完毕后需要回退到项目文件夹.com下，cmd执行hugo命令重新编译，并进入public文件夹输入以下代码上传\ngit add -A git commit -m\u0026quot;edit\u0026quot; git push  step6 补充 有些博客模板在本地运行没有问题，但是绑定域名后出错，需要仔细筛选.\nHugo官网：https://gohugo.io/content-management 要学会查找官方文档，解决开发中遇到的问题.\n","date":"2022-05-12","permalink":"https://wudaore.github.io/post/creating-a-boke/","tags":["test","others"],"title":"windows下使用hugo搭建博客"},{"content":"\r\rThis post is for in-post APlayer test, above is previous post-player.\nMusic files are all downloaded from Free Music Archive.\nSingle file \r\rMultiple files You can open the playlist to check other musics.\n\r\rSpaces between multiple items can be omited.\n","date":"2021-01-10","permalink":"https://wudaore.github.io/post/aplayer-test/","tags":["test","aplayer"],"title":"In-post APlayer Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot; /\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item   First Sub-item Second Sub-item  Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-05-11","permalink":"https://wudaore.github.io/post/markdown-syntax/","tags":["markdown","css","html","themes"],"title":"Markdown Syntax"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }}\r{{ partial \u0026quot;math.html\u0026quot; . }}\r{{ end }}\r  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files.  Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $ \\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887… $\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2020-03-08","permalink":"https://wudaore.github.io/post/math-typesetting/","tags":null,"title":"Math Typesetting"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09","permalink":"https://wudaore.github.io/post/placeholder-text/","tags":["markdown","text"],"title":"Placeholder Text"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n:s ee_no_evil:[Remove the space] 🙈 :h ear_no_evil:[Remove the space] 🙉 :s peak_no_evil:[Remove the space] 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji {\rfont-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;\r}\r","date":"2019-03-05","permalink":"https://wudaore.github.io/post/emoji-support/","tags":["emoji"],"title":"Emoji Support"},{"content":"The following is part of the CJK text, this page is for test use only.\nCJK Radicals Supplement ⺀ ⺁ ⺂ ⺃ ⺄ ⺅ ⺆ ⺇ ⺈ ⺉ ⺊ ⺋ ⺌ ⺍ ⺎ ⺏ ⺐ ⺑ ⺒ ⺓ ⺔ ⺕ ⺖ ⺗ ⺘ ⺙ ⺛ ⺜ ⺝ ⺞ ⺟ ⺠ ⺡ ⺢ ⺣ ⺤ ⺥ ⺦ ⺧ ⺨ ⺩ ⺪ ⺫ ⺬ ⺭ ⺮ ⺯ ⺰ ⺱ ⺲ ⺳ ⺴ ⺵ ⺶ ⺷ ⺸ ⺹ ⺺ ⺻ ⺼ ⺽ ⺾ ⺿ ⻀ ⻁ ⻂ ⻃ ⻄ ⻅ ⻆ ⻇ ⻈ ⻉ ⻊ ⻋ ⻌ ⻍ ⻎ ⻏ ⻐ ⻑ ⻒ ⻓ ⻔ ⻕ ⻖ ⻗ ⻘ ⻙ ⻚ ⻛ ⻜ ⻝ ⻞ ⻟ ⻠ ⻡ ⻢ ⻣ ⻤ ⻥ ⻦ ⻧ ⻨ ⻩ ⻪ ⻫ ⻬ ⻭ ⻮ ⻯ ⻰ ⻱ ⻲ ⻳\nKangxi Radicals ⼀ ⼁ ⼂ ⼃ ⼄ ⼅ ⼆ ⼇ ⼈ ⼉ ⼊ ⼋ ⼌ ⼍ ⼎ ⼏ ⼐ ⼑ ⼒ ⼓ ⼔ ⼕ ⼖ ⼗ ⼘ ⼙ ⼚ ⼛ ⼜ ⼝ ⼞ ⼟ ⼠ ⼡ ⼢ ⼣ ⼤ ⼥ ⼦ ⼧ ⼨ ⼩ ⼪ ⼫ ⼬ ⼭ ⼮ ⼯ ⼰ ⼱ ⼲ ⼳ ⼴ ⼵ ⼶ ⼷ ⼸ ⼹ ⼺ ⼻ ⼼ ⼽ ⼾ ⼿ ⽀ ⽁ ⽂ ⽃ ⽄ ⽅ ⽆ ⽇ ⽈ ⽉ ⽊ ⽋ ⽌ ⽍ ⽎ ⽏ ⽐ ⽑ ⽒ ⽓ ⽔ ⽕ ⽖ ⽗ ⽘ ⽙ ⽚ ⽛ ⽜ ⽝ ⽞ ⽟ ⽠ ⽡ ⽢ ⽣ ⽤ ⽥ ⽦ ⽧ ⽨ ⽩ ⽪ ⽫ ⽬ ⽭ ⽮ ⽯ ⽰ ⽱ ⽲ ⽳ ⽴ ⽵ ⽶ ⽷ ⽸ ⽹ ⽺ ⽻ ⽼ ⽽ ⽾ ⽿ \u0026hellip;\nCJK Symbols and Punctuation 、 。 〃 〄 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〓 〔 〕 〖 〗 〘 〙 〚 〛 〜 〝 〞 〟 〠 〡 〢 〣 〤 〥 〦 〧 〨 〩 〪 〫 〬 〭 〮 〯 〰 〱 〲 〳 〴 〵 〶 〷 〸 〹 〺 〻 〼 〽 〾 〿\nHiragana ぁ あ ぃ い ぅ う ぇ え ぉ お か が き ぎ く ぐ け げ こ ご さ ざ し じ す ず せ ぜ そ ぞ た だ ち ぢ っ つ づ て で と ど な に ぬ ね の は ば ぱ ひ び ぴ ふ ぶ ぷ へ べ ぺ ほ ぼ ぽ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ ゎ わ ゐ ゑ を ん ゔ ゕ ゖ ゙ ゚ ゛ ゜ ゝ ゞ ゟ\nKatakana ゠ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ ガ キ ギ ク グ ケ ゲ コ ゴ サ ザ シ ジ ス ズ セ ゼ ソ ゾ タ ダ チ ヂ ッ ツ ヅ テ デ ト ド ナ ニ ヌ ネ ノ ハ バ パ ヒ ビ ピ フ ブ プ ヘ ベ ペ ホ ボ ポ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ヮ ワ ヰ ヱ ヲ ン ヴ ヵ ヶ ヷ ヸ ヹ ヺ ・ ー ヽ ヾ ヿ\nBopomofo ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ ㄚ ㄛ ㄜ ㄝ ㄞ ㄟ ㄠ ㄡ ㄢ ㄣ ㄤ ㄥ ㄦ ㄧ ㄨ ㄩ ㄪ ㄫ ㄬ\nHangul Compatibility Jamo ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄸ ㄹ ㄺ ㄻ ㄼ ㄽ ㄾ ㄿ ㅀ ㅁ ㅂ ㅃ ㅄ ㅅ ㅆ ㅇ ㅈ ㅉ ㅊ ㅋ ㅌ ㅍ ㅎ ㅏ ㅐ ㅑ ㅒ ㅓ ㅔ ㅕ ㅖ ㅗ ㅘ ㅙ ㅚ ㅛ ㅜ ㅝ ㅞ ㅟ ㅠ ㅡ ㅢ ㅣ ㅤ ㅥ ㅦ ㅧ ㅨ ㅩ ㅪ ㅫ ㅬ ㅭ ㅮ ㅯ ㅰ ㅱ ㅲ ㅳ ㅴ ㅵ ㅶ ㅷ ㅸ ㅹ ㅺ ㅻ ㅼ ㅽ ㅾ ㅿ ㆀ ㆁ ㆂ ㆃ ㆄ ㆅ ㆆ ㆇ ㆈ ㆉ ㆊ ㆋ ㆌ ㆍ ㆎ\nKanbun ㆐ ㆑ ㆒ ㆓ ㆔ ㆕ ㆖ ㆗ ㆘ ㆙ ㆚ ㆛ ㆜ ㆝ ㆞ ㆟\nBopomofo Extended ㆠ ㆡ ㆢ ㆣ ㆤ ㆥ ㆦ ㆧ ㆨ ㆩ ㆪ ㆫ ㆬ ㆭ ㆮ ㆯ ㆰ ㆱ ㆲ ㆳ ㆴ ㆵ ㆶ ㆷ\nKatakana Phonetic Extensions ㇰ ㇱ ㇲ ㇳ ㇴ ㇵ ㇶ ㇷ ㇸ ㇹ ㇺ ㇻ ㇼ ㇽ ㇾ ㇿ\nEnclosed CJK Letters and Months ㈀ ㈁ ㈂ ㈃ ㈄ ㈅ ㈆ ㈇ ㈈ ㈉ ㈊ ㈋ ㈌ ㈍ ㈎ ㈏ ㈐ ㈑ ㈒ ㈓ ㈔ ㈕ ㈖ ㈗ ㈘ ㈙ ㈚ ㈛ ㈜ ㈠ ㈡ ㈢ ㈣ ㈤ ㈥ ㈦ ㈧ ㈨ ㈩ ㈪ ㈫ ㈬ ㈭ ㈮ ㈯ ㈰ ㈱ ㈲ ㈳ ㈴ ㈵ ㈶ ㈷ ㈸ ㈹ ㈺ ㈻ ㈼ ㈽ ㈾ ㈿ ㉀ ㉁ ㉂ ㉃ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㉠ ㉡ ㉢ ㉣ ㉤ ㉥ ㉦ ㉧ ㉨ ㉩ ㉪ ㉫ ㉬ ㉭ ㉮ ㉯ ㉰ ㉱ ㉲ ㉳ ㉴ ㉵ ㉶ ㉷ ㉸ ㉹ ㉺ ㉻ ㉿ ㊀ ㊁ ㊂ ㊃ ㊄ ㊅ ㊆ ㊇ ㊈ ㊉ ㊊ ㊋ ㊌ ㊍ ㊎ ㊏ ㊐ ㊑ ㊒ \u0026hellip;\nCJK Compatibility ㌀ ㌁ ㌂ ㌃ ㌄ ㌅ ㌆ ㌇ ㌈ ㌉ ㌊ ㌋ ㌌ ㌍ ㌎ ㌏ ㌐ ㌑ ㌒ ㌓ ㌔ ㌕ ㌖ ㌗ ㌘ ㌙ ㌚ ㌛ ㌜ ㌝ ㌞ ㌟ ㌠ ㌡ ㌢ ㌣ ㌤ ㌥ ㌦ ㌧ ㌨ ㌩ ㌪ ㌫ ㌬ ㌭ ㌮ ㌯ ㌰ ㌱ ㌲ ㌳ ㌴ ㌵ ㌶ ㌷ ㌸ ㌹ ㌺ ㌻ ㌼ ㌽ ㌾ ㌿ ㍀ ㍁ ㍂ ㍃ ㍄ ㍅ ㍆ ㍇ ㍈ ㍉ ㍊ ㍋ ㍌ ㍍ ㍎ ㍏ ㍐ ㍑ ㍒ ㍓ ㍔ ㍕ ㍖ ㍗ ㍘ ㍙ ㍚ ㍛ ㍜ ㍝ ㍞ ㍟ ㍠ ㍡ ㍢ ㍣ ㍤ ㍥ ㍦ ㍧ ㍨ ㍩ ㍪ ㍫ ㍬ ㍭ ㍮ ㍯ ㍰ ㍱ ㍲ ㍳ ㍴ ㍵ ㍶ ㍻ ㍼ ㍽ ㍾ ㍿ ㎀ ㎁ ㎂ ㎃ \u0026hellip;\nCJK Unified Ideographs Extension A 㐀 㐁 㐂 㐃 㐄 㐅 㐆 㐇 㐈 㐉 㐊 㐋 㐌 㐍 㐎 㐏 㐐 㐑 㐒 㐓 㐔 㐕 㐖 㐗 㐘 㐙 㐚 㐛 㐜 㐝 㐞 㐟 㐠 㐡 㐢 㐣 㐤 㐥 㐦 㐧 㐨 㐩 㐪 㐫 㐬 㐭 㐮 㐯 㐰 㐱 㐲 㐳 㐴 㐵 㐶 㐷 㐸 㐹 㐺 㐻 㐼 㐽 㐾 㐿 㑀 㑁 㑂 㑃 㑄 㑅 㑆 㑇 㑈 㑉 㑊 㑋 㑌 㑍 㑎 㑏 㑐 㑑 㑒 㑓 㑔 㑕 㑖 㑗 㑘 㑙 㑚 㑛 㑜 㑝 㑞 㑟 㑠 㑡 㑢 㑣 㑤 㑥 㑦 㑧 㑨 㑩 㑪 㑫 㑬 㑭 㑮 㑯 㑰 㑱 㑲 㑳 㑴 㑵 㑶 㑷 㑸 㑹 㑺 㑻 㑼 㑽 㑾 㑿 \u0026hellip;\nCJK Unified Ideographs 一 丁 丂 七 丄 丅 丆 万 丈 三 上 下 丌 不 与 丏 丐 丑 丒 专 且 丕 世 丗 丘 丙 业 丛 东 丝 丞 丟 丠 両 丢 丣 两 严 並 丧 丨 丩 个 丫 丬 中 丮 丯 丰 丱 串 丳 临 丵 丶 丷 丸 丹 为 主 丼 丽 举 丿 乀 乁 乂 乃 乄 久 乆 乇 么 义 乊 之 乌 乍 乎 乏 乐 乑 乒 乓 乔 乕 乖 乗 乘 乙 乚 乛 乜 九 乞 也 习 乡 乢 乣 乤 乥 书 乧 乨 乩 乪 乫 乬 乭 乮 乯 买 乱 乲 乳 乴 乵 乶 乷 乸 乹 乺 乻 乼 乽 乾 乿 \u0026hellip;\nHangul Syllables 가 각 갂 갃 간 갅 갆 갇 갈 갉 갊 갋 갌 갍 갎 갏 감 갑 값 갓 갔 강 갖 갗 갘 같 갚 갛 개 객 갞 갟 갠 갡 갢 갣 갤 갥 갦 갧 갨 갩 갪 갫 갬 갭 갮 갯 갰 갱 갲 갳 갴 갵 갶 갷 갸 갹 갺 갻 갼 갽 갾 갿 걀 걁 걂 걃 걄 걅 걆 걇 걈 걉 걊 걋 걌 걍 걎 걏 걐 걑 걒 걓 걔 걕 걖 걗 걘 걙 걚 걛 걜 걝 걞 걟 걠 걡 걢 걣 걤 걥 걦 걧 걨 걩 걪 걫 걬 걭 걮 걯 거 걱 걲 걳 건 걵 걶 걷 걸 걹 걺 걻 걼 걽 걾 걿 \u0026hellip;\nCJK Compatibility Ideographs 豈 更 車 賈 滑 串 句 龜 龜 契 金 喇 奈 懶 癩 羅 蘿 螺 裸 邏 樂 洛 烙 珞 落 酪 駱 亂 卵 欄 爛 蘭 鸞 嵐 濫 藍 襤 拉 臘 蠟 廊 朗 浪 狼 郎 來 冷 勞 擄 櫓 爐 盧 老 蘆 虜 路 露 魯 鷺 碌 祿 綠 菉 錄 鹿 論 壟 弄 籠 聾 牢 磊 賂 雷 壘 屢 樓 淚 漏 累 縷 陋 勒 肋 凜 凌 稜 綾 菱 陵 讀 拏 樂 諾 丹 寧 怒 率 異 北 磻 便 復 不 泌 數 索 參 塞 省 葉 說 殺 辰 沈 拾 若 掠 略 亮 兩 凉 梁 糧 良 諒 量 勵 \u0026hellip;\nCJK Compatibility Forms ︰ ︱ ︲ ︳ ︴ ︵ ︶ ︷ ︸ ︹ ︺ ︻ ︼ ︽ ︾ ︿ ﹀ ﹁ ﹂ ﹃ ﹄ ﹅ ﹆ ﹉ ﹊ ﹋ ﹌ ﹍ ﹎ ﹏\n","date":"2018-03-09","permalink":"https://wudaore.github.io/post/cjk-unicode-test/","tags":["test","cjk"],"title":"CJK Unicode Test"},{"content":"本文内容无实际意义，由狗屁不通文章生成器自动生成，不代表作者本人观点。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，到底应该如何实现。马克思在不经意间这样说过，一切节省，归根到底都归结为时间的节省。这不禁令我深思。马克思曾经说过，一切节省，归根到底都归结为时间的节省。\n带着这句话，我们还要更加慎重的审视这个问题：对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如何，我们不得不面对一个非常尴尬的事实，那就是，亚伯拉罕·林肯曾经提到过，你活了多少岁不算什么，重要的是你是如何度过这些岁月的。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我认为，在这种困难的抉择下，本人思来想去，寝食难安。所谓一段代码写一天，关键是一段代码写一天需要如何写。裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。总结的来说，带着这些问题，我们来审视一下一段代码写一天。西班牙曾经说过，自知之明是最难得的知识。这不禁令我深思。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。贝多芬在不经意间这样说过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。\n每个人都不得不面对这些问题。在面对这种问题时，歌德曾经提到过，读一本好书，就如同和一个高尚的人在交谈。这似乎解答了我的疑惑。歌德在不经意间这样说过，读一本好书，就如同和一个高尚的人在交谈。我希望诸位也能好好地体会这句话。从这个角度来看，一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。现在，解决一段代码写一天的问题，是非常非常重要的。所以，马克思曾经提到过，一切节省，归根到底都归结为时间的节省。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。我希望诸位也能好好地体会这句话。既然如此，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。白哲特曾经说过，坚强的信念能赢得强者的心，并使他们变得更坚强。带着这句话，我们还要更加慎重的审视这个问题：富勒在不经意间这样说过，苦难磨炼一些人，也毁灭另一些人。带着这句话，我们还要更加慎重的审视这个问题：这样看来，一般来讲，我们都必须务必慎重的考虑考虑。从这个角度来看，从这个角度来看，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。而这些并不是完全重要，更加重要的问题是。\n带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。经过上述讨论，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。佚名曾经提到过，感激每一个新的挑战，因为它会锻造你的意志和品格。这句话语虽然很短，但令我浮想联翩。现在，解决一段代码写一天的问题，是非常非常重要的。所以，每个人都不得不面对这些问题。在面对这种问题时，我们都知道，只要有意义，那么就必须慎重考虑。经过上述讨论。\n了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一般来说，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。笛卡儿在不经意间这样说过，阅读一切好书如同和过去最杰出的人谈话。我希望诸位也能好好地体会这句话。在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。莎士比亚曾经说过，抛弃时间的人，时间也抛弃他。我希望诸位也能好好地体会这句话。\n笛卡儿说过一句富有哲理的话，阅读一切好书如同和过去最杰出的人谈话。这句话语虽然很短，但令我浮想联翩。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。一段代码写一天因何而发生？一段代码写一天因何而发生？我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。邓拓曾经说过，越是没有本领的就越加自命不凡。这启发了我，从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说。\n对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。总结的来说，从这个角度来看，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。德谟克利特说过一句富有哲理的话，节制使快乐增加并使享受加强。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们都知道，只要有意义，那么就必须慎重考虑。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。史美尔斯曾经提到过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这句话语虽然很短，但令我浮想联翩。一般来讲，我们都必须务必慎重的考虑考虑。既然如此，我们都知道，只要有意义，那么就必须慎重考虑。这样看来，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。德国曾经提到过，只有在人群中间，才能认识自己。我希望诸位也能好好地体会这句话。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，从这个角度来看，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。莎士比亚在不经意间这样说过，意志命运往往背道而驰，决心到最后会全部推倒。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。那么，一段代码写一天，到底应该如何实现。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，发生了会如何，不发生又会如何。在这种困难的抉择下，本人思来想去，寝食难安。塞涅卡在不经意间这样说过，生命如同寓言，其价值不在与长短，而在与内容。这不禁令我深思。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。所谓一段代码写一天，关键是一段代码写一天需要如何写。我认为，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。在这种困难的抉择下，本人思来想去，寝食难安。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，到底应该如何实现。一段代码写一天，发生了会如何，不发生又会如何。既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。总结的来说，那么，带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天，发生了会如何，不发生又会如何。西班牙曾经提到过，自己的鞋子，自己知道紧在哪里。带着这句话，我们还要更加慎重的审视这个问题：既然如何，每个人都不得不面对这些问题。在面对这种问题时，问题的关键究竟为何？从这个角度来看，既然如此，在这种困难的抉择下，本人思来想去，寝食难安。我认为。\n一段代码写一天因何而发生？我们不得不面对一个非常尴尬的事实，那就是，洛克在不经意间这样说过，学到很多东西的诀窍，就是一下子不要学很多。这不禁令我深思。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说，而这些并不是完全重要，更加重要的问题是，问题的关键究竟为何？而这些并不是完全重要，更加重要的问题是，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天因何而发生？所谓一段代码写一天，关键是一段代码写一天需要如何写。一段代码写一天因何而发生？这样看来，一段代码写一天，到底应该如何实现。卡耐基说过一句富有哲理的话，一个不注意小事情的人，永远不会成就大事业。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，到底应该如何实现。既然如此，而这些并不是完全重要，更加重要的问题是。\n冯学峰说过一句富有哲理的话，当一个人用工作去迎接光明，光明很快就会来照耀着他。我希望诸位也能好好地体会这句话。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。而这些并不是完全重要，更加重要的问题是，那么，要想清楚，一段代码写一天，到底是一种怎么样的存在。从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。在这种困难的抉择下，本人思来想去，寝食难安。我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。问题的关键究竟为何？本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这不禁令我深思。莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这启发了我。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。海贝尔曾经说过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。这似乎解答了我的疑惑。德国曾经说过，只有在人群中间，才能认识自己。这不禁令我深思。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。这样看来，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。克劳斯·莫瑟爵士在不经意间这样说过，教育需要花费钱，而无知也是一样。这似乎解答了我的疑惑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一般来讲，我们都必须务必慎重的考虑考虑。一般来讲，我们都必须务必慎重的考虑考虑。一段代码写一天因何而发生？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。而这些并不是完全重要，更加重要的问题是，一般来讲，我们都必须务必慎重的考虑考虑。我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。博说过一句富有哲理的话，一次失败，只是证明我们成功的决心还够坚强。维这启发了我，从这个角度来看，问题的关键究竟为何？这样看来，既然如此，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。总结的来说，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。经过上述讨论，史美尔斯说过一句富有哲理的话，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。冯学峰曾经说过，当一个人用工作去迎接光明，光明很快就会来照耀着他。这句话语虽然很短，但令我浮想联翩。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。米歇潘在不经意间这样说过，生命是一条艰险的峡谷，只有勇敢的人才能通过。我希望诸位也能好好地体会这句话。问题的关键究竟为何？培根在不经意间这样说过，合理安排时间，就等于节约时间。这句话语虽然很短，但令我浮想联翩。吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这启发了我，这样看来，既然如何，吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这句话语虽然很短，但令我浮想联翩。所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，在这种困难的抉择下，本人思来想去，寝食难安。要想清楚，一段代码写一天，到底是一种怎么样的存在。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我们不得不面对一个非常尴尬的事实，那就是，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。所谓一段代码写一天，关键是一段代码写一天需要如何写。而这些并不是完全重要，更加重要的问题是，左拉在不经意间这样说过，生活的道路一旦选定，就要勇敢地走到底，决不回头。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。一般来说，经过上述讨论，总结的来说，一般来讲，我们都必须务必慎重的考虑考虑。这样看来，既然如此，一般来说，一段代码写一天，发生了会如何，不发生又会如何。那么，既然如此，每个人都不得不面对这些问题。在面对这种问题时，莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。带着这句话，我们还要更加慎重的审视这个问题：韩非在不经意间这样说过，内外相应，言行相称。这似乎解答了我的疑惑。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，我认为，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。我们都知道，只要有意义，那么就必须慎重考虑。培根在不经意间这样说过，合理安排时间，就等于节约时间。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。那么，所谓一段代码写一天，关键是一段代码写一天需要如何写。歌德曾经说过，意志坚强的人能把世界放在手中像泥块一样任意揉捏。带着这句话，我们还要更加慎重的审视这个问题：这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。每个人都不得不面对这些问题。在面对这种问题时，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。塞涅卡曾经提到过，生命如同寓言，其价值不在与长短，而在与内容。这句话语虽然很短，但令我浮想联翩。\n我们都知道，只要有意义，那么就必须慎重考虑。带着这些问题，我们来审视一下一段代码写一天。笛卡儿曾经说过，我的努力求学没有得到别的好处，只不过是愈来愈发觉自己的无知。带着这句话，我们还要更加慎重的审视这个问题：现在，解决一段代码写一天的问题，是非常非常重要的。所以，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这启发了我，现在，解决一段代码写一天的问题，是非常非常重要的。所以，既然如何，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。易卜生说过一句富有哲理的话，伟大的事业，需要决心，能力，组织和责任感。带着这句话，我们还要更加慎重的审视这个问题：我们都知道，只要有意义，那么就必须慎重考虑。一段代码写一天，发生了会如何，不发生又会如何。带着这些问题，我们来审视一下一段代码写一天。我们不得不面对一个非常尴尬的事实，那就是，我们不得不面对一个非常尴尬的事实，那就是。\n我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。我们不得不面对一个非常尴尬的事实，那就是，现在，解决一段代码写一天的问题，是非常非常重要的。所以，俾斯麦说过一句富有哲理的话，失败是坚忍的最后考验。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。达·芬奇在不经意间这样说过，大胆和坚定的决心能够抵得上武器的精良。这似乎解答了我的疑惑。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。而这些并不是完全重要，更加重要的问题是，我认为，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n问题的关键究竟为何？我们不得不面对一个非常尴尬的事实，那就是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我认为，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。问题的关键究竟为何？从这个角度来看，我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。那么，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，从这个角度来看，带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，发生了会如何，不发生又会如何。\n培根在不经意间这样说过，要知道对好事的称颂过于夸大，也会招来人们的反感轻蔑和嫉妒。这句话语虽然很短，但令我浮想联翩。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，而这些并不是完全重要，更加重要的问题是，这样看来，贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这不禁令我深思。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一段代码写一天，发生了会如何，不发生又会如何。问题的关键究竟为何？卡耐基在不经意间这样说过，我们若已接受最坏的，就再没有什么损失。我希望诸位也能好好地体会这句话。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。现在，解决一段代码写一天的问题，是非常非常重要的。所以，要想清楚，一段代码写一天，到底是一种怎么样的存在。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。我希望诸位也能好好地体会这句话。既然如何，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。海贝尔说过一句富有哲理的话，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。我希望诸位也能好好地体会这句话。\n这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。经过上述讨论，我认为，莫扎特曾经提到过，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。经过上述讨论，我们不得不面对一个非常尴尬的事实，那就是，达尔文说过一句富有哲理的话，敢于浪费哪怕一个钟头时间的人，说明他还不懂得珍惜生命的全部价值。这句话语虽然很短，但令我浮想联翩。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。一般来讲，我们都必须务必慎重的考虑考虑。罗曼·罗兰曾经说过，只有把抱怨环境的心情，化为上进的力量，才是成功的保证。这启发了我，而这些并不是完全重要，更加重要的问题是，总结的来说，这样看来，每个人都不得不面对这些问题。在面对这种问题时，一般来讲，我们都必须务必慎重的考虑考虑。既然如此，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n德国曾经提到过，只有在人群中间，才能认识自己。带着这句话，我们还要更加慎重的审视这个问题：既然如此，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。杰纳勒尔·乔治·S·巴顿曾经提到过，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。一段代码写一天因何而发生？问题的关键究竟为何？海贝尔曾经提到过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。带着这句话，我们还要更加慎重的审视这个问题。\n经过上述讨论，我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。我认为，一般来说，经过上述讨论，我认为，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。问题的关键究竟为何？一段代码写一天因何而发生？莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。这似乎解答了我的疑惑。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天，到底应该如何实现。我们都知道，只要有意义，那么就必须慎重考虑。总结的来说，那么，叔本华在不经意间这样说过，普通人只想到如何度过时间，有才能的人设法利用时间。这句话语虽然很短，但令我浮想联翩。总结的来说，经过上述讨论，从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。总结的来说，所谓一段代码写一天，关键是一段代码写一天需要如何写。既然如何，一段代码写一天，到底应该如何实现。\n问题的关键究竟为何？一般来讲，我们都必须务必慎重的考虑考虑。歌德说过一句富有哲理的话，读一本好书，就如同和一个高尚的人在交谈。这不禁令我深思。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这不禁令我深思。在这种困难的抉择下，本人思来想去，寝食难安。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。要想清楚，一段代码写一天，到底是一种怎么样的存在。莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。\n这样看来，每个人都不得不面对这些问题。在面对这种问题时，从这个角度来看，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，富兰克林曾经提到过，读书是易事，思索是难事，但两者缺一，便全无用处。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。康德曾经说过，既然我已经踏上这条道路，那么，任何东西都不应妨碍我沿着这条路走下去。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。带着这些问题，我们来审视一下一段代码写一天。既然如此，我们不得不面对一个非常尴尬的事实，那就是，一般来说，美华纳曾经提到过，勿问成功的秘诀为何，且尽全力做你应该做的事吧。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。叔本华曾经说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。我希望诸位也能好好地体会这句话。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，郭沫若曾经说过，形成天才的决定因素应该是勤奋。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。在这种困难的抉择下，本人思来想去，寝食难安。经过上述讨论，一段代码写一天，发生了会如何，不发生又会如何。从这个角度来看，一段代码写一天因何而发生？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。既然如何。\n而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，那么，我认为，经过上述讨论，既然如此，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。卡耐基曾经提到过，一个不注意小事情的人，永远不会成就大事业。这句话语虽然很短，但令我浮想联翩。黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这似乎解答了我的疑惑。一般来说，雷锋在不经意间这样说过，自己活着，就是为了使别人过得更美好。这启发了我，一般来说，而这些并不是完全重要，更加重要的问题是。\n裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。带着这句话，我们还要更加慎重的审视这个问题：带着这些问题，我们来审视一下一段代码写一天。既然如此，富勒曾经提到过，苦难磨炼一些人，也毁灭另一些人。我希望诸位也能好好地体会这句话。培根说过一句富有哲理的话，深窥自己的心，而后发觉一切的奇迹在你自己。这似乎解答了我的疑惑。问题的关键究竟为何。\n既然如何，一般来讲，我们都必须务必慎重的考虑考虑。经过上述讨论，普列姆昌德曾经说过，希望的灯一旦熄灭，生活刹那间变成了一片黑暗。这不禁令我深思。总结的来说，在这种困难的抉择下，本人思来想去，寝食难安。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这启发了我，那么，在这种困难的抉择下，本人思来想去，寝食难安。那么，带着这些问题，我们来审视一下一段代码写一天。\n我认为，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。既然如何，黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这不禁令我深思。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这似乎解答了我的疑惑。在这种困难的抉择下，本人思来想去，寝食难安。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。拉罗什福科在不经意间这样说过，我们唯一不会改正的缺点是软弱。这不禁令我深思。既然如此，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一般来讲，我们都必须务必慎重的考虑考虑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，史美尔斯曾经说过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这不禁令我深思。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一般来讲，我们都必须务必慎重的考虑考虑。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一段代码写一天，到底应该如何实现。贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这似乎解答了我的疑惑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。这似乎解答了我的疑惑。我们不得不面对一个非常尴尬的事实，那就是。\n池田大作在不经意间这样说过，不要回避苦恼和困难，挺起身来向它挑战，进而克服它。这句话语虽然很短，但令我浮想联翩。叔本华在不经意间这样说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，发生了会如何，不发生又会如何。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n","date":"2017-04-01","permalink":"https://wudaore.github.io/post/wtf-article/","tags":["test","wtf"],"title":"纯简体中文测试文章"}]