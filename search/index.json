[{"content":"0. 序 开始李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现.\n1. pytorch中dataset的使用 要使用自己的数据集，首先要导入pytorch对应的库\nfrom torch.utils.data import DataLoader,Dataset\r DataLoader有两个比较重要的参数.其一batch_size代表每个batch（即一次读入的单位）中数据的数量；shuffle 在训练时一般设为True.\n然后要定义数据类 重写方法：\n一. init 方法 用于初始化导入数据\n二. getitem 方法 返回键对应的值.类实例化之后，通过使用下标的方法对类实例化对象进行取值\n三. init 方法 返回元素的长度\n比如现在要导入一个积极/消极言论的数据集aclImdb，可以如此构造数据类：\nclass ImdbDataset(Dataset):\rdef __init__(self, train=True):\r# super(ImdbDataset,self).__init__()\rdata_path = r\u0026quot;D:\\BaiduNetdiskDownload\\阶段9-人工智能NLP项目\\第四天\\代码\\data\\aclImdb\u0026quot;\rdata_path += r\u0026quot;\\train\u0026quot; if train else r\u0026quot;\\test\u0026quot;\rself.total_path = [] #保存所有的文件路径\rfor temp_path in [r\u0026quot;\\pos\u0026quot;,r\u0026quot;\\neg\u0026quot;]:\rcur_path = data_path + temp_path\rself.total_path += [os.path.join(cur_path,i) for i in os.listdir(cur_path) if i.endswith(\u0026quot;.txt\u0026quot;)]\rdef __getitem__(self, idx):\rfile = self.total_path[idx]\rreview = utils.tokenlize(open(file, encoding=\u0026quot;utf-8\u0026quot;).read(),) #评论\rlabel = int(file.split(\u0026quot;_\u0026quot;)[-1].split(\u0026quot;.\u0026quot;)[0])\rlabel = 0 if label \u0026lt;5 else 1\rreturn review, label\rdef __len__(self):\rreturn len(self.total_path)\r 2. tensor的基本使用 导入后的数据类型被称为tensor.可以使用torch.tensor将数组，numpy转换成tensor.\n也可以使用tensor.mean(),tensor.sum()等方法进行计算，与numpy类似\n使用torch.squzzes/unsqueeze(维度下标)进行维度的减少/增加\n3. pytorch怎么构建一个神经网络 以Linear Layer为例.本质其实是做一个矩阵的乘法和向量的加法.\n使用方法是要先导入对应的库：\nimport torch.nn as nn\r nn.Linear有两个参数，第一个是数据原本的维度，第二个是数据转换后的维度.\nnn.Linear(64*2, 64)\r 其作用就是将原本64*2维的数据转换成64维的数据.其实本质就是将4096*x的矩阵左乘一个64*4096的矩阵w，变成一个64*x的矩阵；再加上64*1的权重参数矩阵b.\n激活函数的作用是去线性化.如若不用激活函数，无论使用多少隐藏层， 最终得到的结果都是线性的.常见的激活函数如softmax，ReLu，tanh\n下面依旧以数据集aclImdb的训练为例，构建一个神经网络模型\nclass ImdbModel(nn.Module):\rdef __init__(self):\rsuper(ImdbModel,self).__init__()\rself.embedding = nn.Embedding(num_embeddings=len(config.ws),embedding_dim=200,padding_idx=config.ws.PAD)\rself.lstm = nn.LSTM(input_size=200, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True, dropout=0.5)\rself.fc1 = nn.Linear(64*2, 64)\rself.fc2 = nn.Linear(64, 2)\rdef forward(self, input):\r\u0026quot;\u0026quot;\u0026quot;\r:param input:[batch_size,max_len]\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rinput_embeded = self.embedding(input) # input embeded :[batch_size,max_len,200]\routput, (h_n, c_n) = self.lstm(input_embeded) # output：[batch_size, seq_len, 64*2];h_n :[batch_size,4,hidden_size]\rout = torch.cat([h_n[-1, :, :], h_n[-2, :, :]], dim=-1) # 拼接正向最后一个输出和反向最后一个输出#out :[batch_size,hidden_size*2]\r# 进行全连接\rout_fc1 = self.fc1(out)\r# 进行relu\rout_fc1_relu = F.relu(out_fc1)\r# 再经过一个全连接层全连接\rout_fc2 = self.fc2(out_fc1_relu) # out :[batch_size,2]\rreturn F.log_softmax(out_fc2, dim=-1)\r 由上述代码可见，在__init__方法中我们定义了神经网络中的几个层：包括LSTM，Embedding和两个线性层.在forward方法中，我们依次使用了这些层.\n当然，也可以nn.Sequential将这些层整合，并在forword中直接使用\n4. 模型的优化 4.1 lossFunction 模型的最终目的就是最小化损失.pyTorch提供了一系列的损失如nll_loss，MSELoss\n4.2 优化器optimizer optimizer优化器，作用是根据反向传播算法更新神经网络中的参数，以达到降低损失值loss的目的.\n使用optimizer训练和测试数据时的步骤其实大同小异.测试时不需要计算梯度，不需要反向传播，只需要计算损失即可\n训练步骤：\n1.准备数据\n2.实例化模型\n3.实例化优化器\n4.实例化损失函数\n然后进入循环.每次循环需要：\n1.优化器的梯度置0\n2.使用模型得到预测值\n3.将预测值与真实值代入损失函数，计算损失\n4.反向传播，计算梯度\n5.更新优化器参数.\n以Mnist数据集（手写数字识别）为例，训练代码如下：\n#1. 实例化模型，优化器，损失函数\rmodel = MnistModel().to(conf.device)\roptimizer = optim.Adam(model.parameters(),lr=1e-3)\r# if os.path.exists(\u0026quot;./models/model.pkl\u0026quot;):\r# model.load_state_dict(torch.load(\u0026quot;./models/model.pkl\u0026quot;))\r# optimizer.load_state_dict(torch.load(\u0026quot;./models/optimizer.pkl\u0026quot;))\r#2. 进行循环，进行训练\rdef train(epoch):\rtrain_dataloader = get_dataloader(train=True)\rbar = tqdm(enumerate(train_dataloader),total=len(train_dataloader))\rtotal_loss = []\rfor idx,(input,target) in bar:\rinput = input.to(conf.device)\rtarget = target.to(conf.device)\r#梯度置为0\roptimizer.zero_grad()\r#计算得到预测值\routput = model(input)\r#得到损失\rloss = F.nll_loss(output,target)\r#反向传播，计算损失\rloss.backward()\rtotal_loss.append(loss.item())\r#参数的更新\roptimizer.step()\r#打印数据\rif idx%10 ==0 :\rbar.set_description(\u0026quot;epcoh:{} idx:{},loss:{:.6f}\u0026quot;.format(epoch,idx,np.mean(total_loss)))\r# 保存优化器和模型\rtorch.save(model.state_dict(),\u0026quot;./models/model.pkl\u0026quot;)\rtorch.save(optimizer.state_dict(),\u0026quot;./models/optimizer.pkl\u0026quot;)\r 测试步骤：\n1.实例化模型，优化器，损失函数\n然后对于每个数据集中的data和label，进行循环：\n1.通过模型得到预测值.\n2.得到，计算损失.\n3.计算准确率.\n4.可以通过准确率得出预测值.\n具体步骤代码如下：\ndef eval():\r# 1. 实例化模型，优化器，损失函数\rmodel = MnistModel().to(conf.device)\rif os.path.exists(\u0026quot;./models/model.pkl\u0026quot;):\r# 模型的加载\rmodel.load_state_dict(torch.load(\u0026quot;./models/model.pkl\u0026quot;))\rtest_dataloader = get_dataloader(train=False)\rtotal_loss = []\rtotal_acc = []\rwith torch.no_grad():\r# 其实与训练的时候大同小异，只是不需要计算梯度，不需要反向传播，只需要计算损失即可\rfor input,target in test_dataloader: #2. 进行循环，进行训练\rinput = input.to(conf.device)\rtarget = target.to(conf.device)\r#计算得到预测值\routput = model(input)\r#得到损失\rloss = F.nll_loss(output,target)\r#计算损失\rtotal_loss.append(loss.item())\r# 计算准确率\r# 计算预测值\r# output的形状是[batch_size,10], 其中每一行代表一条数据属于每个类别的概率，取其中的最大值作为预测结果;target的形状为[batch_size]代表每一行数据的真实结果\rpred = output.max(dim=-1)[-1]\rtotal_acc.append(pred.eq(target).float().mean().item())\rprint(\u0026quot;test loss:{},test acc:{}\u0026quot;.format(np.mean(total_loss),np.mean(total_acc)))\r ","date":"2022-09-06","permalink":"https://wudaore.github.io/post/deeplearning-lhy1/","tags":["深度学习","algo","pytorch"],"title":"李宏毅-深度学习课程学习笔记(1)"},{"content":"1. 序 对页面定位 position进行学习\n使用position属性来设置定位\n可选值：\nstatic 默认值，元素是静止的没有开启定位\rrelative 开启元素的相对定位\rabsolute 开启元素的绝对定位\rfixed 开启元素的固定定位\rsticky 开启元素的粘滞定位\r 2. 相对定位 自己概括就是 相对定位的参照物是它本身；且移动以后不影响文档流：类似于灵魂出窍，肉体还占在原地. 和margin的区别在于它只改变自己的位置.且只适用于定位\n相对定位的特性：\n1.元素开启相对定位以后，如果不设置偏移量元素不会发生任何的变化\r2.相对定位是参照于元素在文档流中的位置进行定位的\r3.相对定位会提升元素的层级（会盖住层级低的元素）\r4.相对定位不会使元素脱离文档流（下面的元素位置不会变）\r5.相对定位不会改变元素的性质块还是块，行内还是行内\r 3. 绝对定位 绝对定位的特性：\n1.开启绝对定位后，如果不设置偏移量元素的位置不会发生变化但是其他元素位置会变（相对定位的话其他元素不会）\r2.开启绝对定位后，元素会从文档流中脱离\r3.绝对定位会改变元素的性质，行内变成块，块的宽高被内容撑开（2,3两条相当于浮动.）\r4.绝对定位会使元素提升一个层级\r5.绝对定位元素是相对于其包含块进行定位的\r 包含块( containing block )\n- 正常情况下：\r包含块就是离当前元素最近的祖先块元素\r\u0026lt;div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\r\u0026lt;div\u0026gt;\u0026lt;span\u0026gt;\u0026lt;em\u0026gt;hello\u0026lt;/em\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\r- 绝对定位的包含块:\r包含块就是离它最近的开启了定位的祖先元素，\r如果所有的祖先元素都没有开启定位则根元素就是它的包含块\r- html（根元素、初始包含块）\r 4. 固定定位 固定定位：\n- 将元素的position属性设置为fixed则开启了元素的固定定位\r- 固定定位也是一种绝对定位，所以固定定位的大部分特点都和绝对定位一样\r唯一不同的是固定定位永远参照于浏览器的视口进行定位\r固定定位的元素不会随网页的滚动条滚动（也即视口和根元素的区别.如果参照根元素，那么上下拉的时候定位元素不会动；如果参照视口，则会一直定位在视口，会动）\r 5. 沾滞定位 粘滞定位:\n- 当元素的position属性设置为sticky时则开启了元素的粘滞定位\r- 粘滞定位和相对定位的特点基本一致，\r不同的是粘滞定位可以在元素到达某个位置时将其固定\r 5. 沾滞定位 ","date":"2022-07-22","permalink":"https://wudaore.github.io/post/css4/","tags":["web","css"],"title":"CSS(4)"},{"content":"1. 序 对页面浮动 float进行学习\n2. 浮动的简介与特点 通过浮动可以使一个元素向其父元素的左侧或右侧移动.\n注意，元素设置浮动以后，水平布局的等式便不需要强制成立元素设置浮动以后，会完全从文档流中脱离，不再占用文档流的位置，所以元素下边的还在文档流中的元素会自动向上移动 .\n浮动的特点：\n1、浮动元素会完全脱离文档流，不再占据文档流中的位置\r2、设置浮动以后元素会向父元素的左侧或右侧移动，\r3、浮动元素默认不会从父元素中移出\r4、浮动元素向左或向右移动时，不会超过它前边的其他浮动元素\r5、如果浮动元素的上边是一个没有浮动的块元素，则浮动元素无法上移\r6、浮动元素不会超过它上边的浮动的兄弟元素，最多最多就是和它一样高\r7、浮动不会盖过文字.用浮动可以实现文字环绕.\r8、块元素浮动后有以下特点：\r1、块元素不在独占页面的一行\r2、脱离文档流以后，块元素的宽度和高度默认都被内容撑开\r9、行内元素脱离文档流以后会变成块元素，特点和块元素一样.脱离文档流以后，不需要再区分块和行内了.\r 3. 使用clear清除浮动 clear\n- 作用：清除浮动元素对当前元素所产生的影响\r- 可选值：\rleft 清除左侧浮动元素对当前元素的影响\rright 清除右侧浮动元素对当前元素的影响\rboth 清除两侧中最大影响的那侧\r原理：\r设置清除浮动以后，浏览器会自动为元素添加一个上外边距，\r以使其位置不受其他元素的影响\r 4. 高度塌陷以及解决方法 当进行嵌套时，如果将外面元素的高度写死，内部元素的位置就不好调整.所以不写外部元素的高度，让内部元素将外部撑开.\n但是问题来了.内部元素浮动后，从文档流脱离.就会导致外部元素失去高度.发生塌陷.\n解决高度塌陷的方法是开启BFC(Block Formatting Context) 块级格式化环境.\n元素开启BFC后的特点：\n1.开启BFC的元素不会被浮动元素所覆盖\r2.开启BFC的元素子元素和父元素外边距不会重叠\r3.开启BFC的元素可以包含浮动的子元素\r 开启方法：\n1、设置元素的浮动（不推荐）会导致从文档流中脱离，丢失宽度\r2、将元素设置为行内块元素（不推荐）行内块不适合做外部布局容器；且丢失宽度\r3、将元素的overflow设置为一个非visible的值\r- 常用的方式 为元素设置 overflow:hidden 开启其BFC 以使其可以包含浮动元素\r ::after解决高度塌陷\n如果嵌套内部再加一个块元素，使其清除浮动，就可以达到将父元素撑起来的作用.\n可以直接加一个div，但这样就是拿结构(HTML)来解决布局(CSS)的事情了.想纯通过CSS解决，可以使用伪元素.\n.box1::after{\rcontent: '';\rclear: both;\rdisplay: block;\r}\r 在外部元素的最后添加抗浮动.注意::after默认给的是行内元素，需要display: block;来转换为块元素才能达到效果.\n::before解决上边距重叠\n只需要在外部元素的开头设置一个块，这样外部元素就不会和原来的首部元素共享外边距了.\ndisplay使用table可以解决问题.选display选block，和content赋空值都无法隔绝；display选inline-block则会导致多处一行.\n.box1::before{\rcontent: '';\rdisplay: table;\r}\r clearfix同时解决高度塌陷和上边距\n/* clearfix 这个样式可以同时解决高度塌陷和外边距重叠的问题，当你在遇到这些问题时，直接使用clearfix这个类即可 */\r.clearfix::before,\r.clearfix::after{\rcontent: '';\rdisplay: table;\rclear: both;\r}\r ","date":"2022-07-22","permalink":"https://wudaore.github.io/post/css3/","tags":["web","css"],"title":"CSS(3)"},{"content":"目录 1. 序 NLP\u0026ndash;RNN的学习\n在普通的神经网络中，信息是单向传递的.这么做虽然让网络更容易学习，但是在很多的现实任务中，网络的输出不仅依赖于当前的输入，也依赖于过去的输出.此外，普通的神经网络要求输入和输出的维度是固定的，这样就难以处理诸如视频，语音，文本这样的数据长度不固定的时序数据.\n循环神经网络(RNN)是一类具有短期记忆能力的神经网络.神经元不仅可以接受其他神经元的信息，也可以接受自己的信息.换句话说，神经元的输出可以作为下一个时间的输入\n普通神经网络结构图\n循环神经网络结构图\n通过图我们可以看到，相较于传统神经网络，RNN多了一个循环圈.将RNN在时间点上展开，可以得到：\n2. 反向传播 前面的学习中已经初步了解了反向传播.接下来通过吴恩达的专项课程来进一步理解其中的数学原理.\n2.1 正向传播 就简单的神经网络而言，其实h = self.activate(input_data · self.W + self.b) 就是一个正向传播的过程.\n也即，输入x经过权重w和偏置b后通过激活函数得到结果h\n2.2 非输出层的反向传播 我们的最终目的是算出误差loss并更新权重w.这需要我们计算得出loss(后面统称l)对传播过程中各个参数的偏导.\n参考博客\n有如下神经网络：\n反向传播从l开始.先求l对a的偏导：\n再通过a，求l到h的偏导\n最后求出l对w和b的偏导\n由此，反向传播的表达如下：\n注意，\u0026quot;*\u0026ldquo;代表element-wise乘积，即元素对应相乘;\u0026rdquo;.\u0026ldquo;代表矩阵相乘\n代码如下：\nclass Layer:\r'''中间层类'''\rself.W # (input_dim, output_dim)\rself.b # (1, output_dim)\rself.activate(a) = sigmoid(a)/tanh(a)/ReLU(a)/Softmax(a)\rdef forward(self, input_data): # input_data: (1, input_dim)\r'''单个样本的前向传播'''\rinput_data · self.W + self.b = a # a: (1, output_dim)\rh = self.activate(a) # h: (1, output_dim)\rreturn h\rdef backward(input_grad):\r'''单个样本的反向传播'''\ra_grad = input_grad * activate’(a) # (1, output_dim)\rb_grad = a_grad # (1, output_dim)\rW_grad = (input_data.T) · a_grad # (input_dim, output_dim)\rself.b -= learning_rate * b_grad self.W -= learning_rate * W_grad\r 2.3 输出层的反向传播 输出层的反向传播略有不同，因为若使用softmax等激活函数，a和h的数量不嫩一一对应，element-wise乘积无法生效.\n这时就需要l对h的偏导乘以h到a的向量梯度(雅克比矩阵)\n但实际上，经过看上去复杂的计算后输出层l对a的偏导就等于h-y(具体过程看博客，其实我也不是很懂)\n那么，输出层的反向传播表达如下图：\n代码如下：\n# * 表示element-wise乘积，· 表示矩阵乘积\rclass Output_layer(Layer):\r'''属性和forward方法继承Layer类'''\rdef backward(input_grad):\r'''输出层backward方法'''\r'''单个样本的反向传播'''\ra_grad = input_grad # (1, output_dim)\rb_grad = a_grad # (1, output_dim)\rW_grad = (input_data.T) · a_grad # (input_dim, output_dim)\rself.b -= learning_rate * b_grad self.W -= learning_rate * W_grad\rreturn a_grad · (self.W).T # (1, input_dim)\r 2.4 改进：batch批量计算 除非用随机梯度下降，否则每次用以训练的样本都是整个batch计算的，损失函数l则是整个batch中样本得到损失的均值。\n在计算中会以向量化的方式增加运算效率，用batch_size表示批的规模，代码可更改为：\n# * 表示element-wise乘积，· 表示矩阵乘积\rclass Layer:\r'''中间层类'''\rdef forward(self, input_data): # input_data: (batch_size, input_dim)\r'''batch_size个样本的前向传播'''\rinput_data · self.W + self.b = a # a: (1, output_dim)\rh = self.activate(a) # h: (1, output_dim)\rreturn h\rdef backward(input_grad): # input_grad: (batch_size, output_dim)\r'''batch_size个样本的反向传播'''\ra_grad = input_grad * activate’(a) # (batch_size, output_dim)\rb_grad = a_grad.mean(axis=0) # (1, output_dim)\rW_grad = (a_grad.reshape(batch_size,1,output_dim) * input_data.reshape(batch_size,input_dim,1)).mean(axis=0)\r# (input_dim, output_dim) self.b -= lr * b_grad\rself.W -= lr * W_grad\rreturn a_grad · (self.W).T # output_grad: (batch_size, input_dim)\rclass Output_layer(Layer):\r'''输出层类：属性和forward方法继承Layer类'''\rdef backward(input_grad): # input_grad: (batch_size, output_dim)\r'''输出层backward方法'''\r'''batch_size个样本的反向传播'''\ra_grad = input_grad # (batch_size, output_dim)\rb_grad = a_grad.mean(axis=0) # (1, output_dim)\rW_grad = (a_grad.reshape(batch_size,1,output_dim) * input_data.reshape(batch_size,input_dim,1)).mean(axis=0)\r# (input_dim, output_dim) self.b -= learning_rate * b_grad self.W -= learning_rate * W_grad\rreturn a_grad · (self.W).T # output_grad: (batch_size, input_dim)\r 梯度对中间结果a和h上均不用求均值，在w和b上需要\n3. RNN的不同表示和功能  one to one  固定长度的输入输出(图像分类)\none to many  序列输出(图像转文字)\nmany to one  数列输入(文字分类)\nmany to many  序列输入和输出\n4.1 异步\r文本翻译\r4.2 同步\r根据视频的每一帧给视频分类\r 4. LSTM 在进行一些预测时，可能决定性的因素在时间序列上排在很前面，这时RNN就不会有非常好的效果.因为它是短期记忆的.\n为了解决这个问题就需要LSTM(Lone Short-Term Memory)\u0026ndash;一种可以学习长期依赖的特殊类型RNN\n4.1 基本原理 其中∅代表sigmod函数.\nLSTM的核心是单元(细胞)中的状态，也即上面那条线.\n但是光靠那条线不能实现信息的增删.此时需要通过sigmod和点乘来实现一个门，控制信息通过与否.\n上图中，C保存的是记忆\n4.2 遗忘门 即，将上一层的输出ht-1和输入xt做矩阵concat操作([]代表concat操作，也即矩阵的拼接)后，乘以权值加上偏置，通过sigmod函数之后乘以上一次的记忆Ct-1\nsigmod取值在0到1之间.1代表全部通过，0代表不给通过.与Ct-1相乘后可以决定保留记忆的程度.\n4.3 输入门 it代表就当前更新而言，有多少比例的信息会更新，Ct代表要更新的信息.\n通过遗忘门和输入门，就可以更新记忆(细胞状态)了.具体来说就是旧的细胞状态和遗忘门相乘，加上数入门和tanh相乘的结果.\n举个例子：\u0026ldquo;我昨天吃了苹果，我今天吃了菠萝\u0026quot;在这个句子中，通过遗忘门遗忘苹果，通过输入门逐步更新主语为菠萝.\n4.4 输出门 作用是输出当前时刻的结果和ht，这两者其实是一样的，也即hidden_state，隐藏状态.\n4.5 API 实例化LSTM对象后，不仅要传入输入，还要传入上一次的隐藏状态h和前一次的memory C\nLSTM的默认输出格式如下：\n这是默认的输出.也即batch_first为False的情况.要想输出的batch在seq_len前面，需要使batch_first为True.\nnumdirection：当为双向LSTM时为2，否则为1.\n对于输入值，格式为[batch, seq_len].经过word-emdbedding后，格式变成[batch, seq_len, n],n为word-emdbedding-dim\n[seq_len, batch, n]，也即默认输入input的格式.\n输入经过LSTM后，因为LSTM有hidden_size个单元.输入经过处理后由每个单元输出.所以输出的格式为[seq_len, batch, hidden_size*num_direction]\nh和C的格式都是[num_layer*num_direction, batch, hidden_size].因为LSTM有hidden_size个单元，所以h(或者C)的长度也是hidden_size.每次有batch条数据，又有num_layer*num_direction个LSTM单元.所以得出这样的h和C的格式.\noutput其实是将每一时间步上的结果在seq_len这一维度进行了拼接；h则是将不同层的隐藏状态在第个维度上进行了拼接\n代码：\n\rimport torch\rimport torch.nn as nn\rbatch_size = 10\rseq_len = 20\rnum_embeddings = 100\rembedding_dim = 30\rinput = torch.ones([batch_size,seq_len],dtype=torch.long)\r#embedding\rembedding = nn.Embedding(100,30)\rinput_embeded = embedding(input) #[batch_size,seq_len,embedding_dim] #[10,20,30]\r#LSTM\rlstm = nn.LSTM(input_size=embedding_dim,hidden_size=8,num_layers=1,batch_first=True,bidirectional=True)\r# gru = nn.GRU(input_size=embedding_dim,hidden_size=8,num_layers=2,batch_first=True,bidirectional=False)\routput,(h_n,c_n) = lstm(input_embeded)\rprint('*****')\rprint(output.size()) #[batch_Size,seq_len,8]\rprint(h_n.size()) #[2,batch_size,hidden_size]\rprint(c_n.size())\r#获取最后一个时间步骤的输出：\r#最上一层的输出：\r# last_output = output[:,-1,:]\r#\r# #最上层的h_n:\r# h_n_last = h_n[-1,:,:]\r# print(last_output.eq(h_n_last))\r#双向的LSTM中，最上层的正向的最后一个输出\ro1 = output[:,-1,:8] #正向的最后一个输出\ro2 = output[:,0,8:] #反向最后一个输出\r#双向LSTM中，最上层正向的h_n\rh1 = h_n[-2,:,:] #最上层正向\rh2 = h_n[-1,:,:] #最上层反向\rprint(o1.eq(h1))\rprint(o2.eq(h2))\r 5. 双向LSTM 在进行预测时，有时会需要从后面预测全面.这时就需要双向LSTM.\n双向LSTM既有正向的LSTM又有反向的LSTM.两者的输出concat以后再代入一个LSTM（也可以是全连接层.接口中只提供前面的两个方向相反的LSTM）中进行计算.这样，输出就既有正向也有反向的记忆了.\n6. GRU GRU是LSTM的变形版本.将输入门和遗忘门组合成了更新门.同时合并了单元状态(C)和隐藏状态(h)并进行了一系列其他改进.模型比LSTM简化.\n7. 使用LSTM/GRU改进文本情感分类模型 7.1 注意事项   第一次调用前要初始化隐藏状态，不然就会默认生成全为0的隐藏状态.\n  使用单向LSTM时一般使用output的最后一维结果(也就是hn)作为文本处理的结果.因为它也包含了前面所有的隐藏状态h.默认情况下，表现为output[-1::]\n  使用双向LSTM时一般使用每个方向output的最后一维结果.torch.cat([h_n[-2,:,:], h_n[-1,:,:]], dim=-1).当然也可以用output但是比较麻烦.最后的形状为[batch_size, hidden_size*2]\n  如果要交换output中batch_size和seq_len的位置，不能用view等变形函数，而需要用permute交换0，1轴.两者区别\n  7.2 梯度消失和梯度爆炸 梯度消失\n有如下简单的神经网络：\n展开来就是这样的\n令yi=σ(zi) = σ(wi*xi+bi)\n求输出C对参数b1的偏导.等于：\nσ是激活函数sigmod，其导数图像为：\n由图像可见，这个值最大为0.25\n反向传播时，当层数足够多时，梯度一直乘以0.25，呈指数倍无限下降，直至出现消失的情况.\n梯度爆炸\n同上图所示求输出C对参数b1的偏导.当权重w很大时，结果会呈指数增加，导致爆炸.\n解决方法\n  选择relu等梯度大部分落在常数上的激活函数\n  改进梯度优化算法.如使用adam算法\n  使用batch normalization\n  7.3 一些API  nn.BatchNormld  批规范化.在每个batch训练的过程中归一化数据，以达到加速训练的目的.\n以sigmod为例，当梯度接近0时更新幅度小，更新会很慢.使用归一化后会尽量将梯度拉到[0,1]的范围内，加速更新.\nbatchNormld一般在激活函数之后使用\nnn.Dropout  对参数的随机失活.可以增加模型的稳健性，解决过拟合问题(增加泛化能力)，最后的模型可以理解为多个模型的组合结果，类似随机森林.\n关于with torch.no_grad(): with语句保证了不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。\n7.5 使用LSTM/优化模型 区别于上次的只使用了word-embedding的模型，这次的模型只需要修改model.py并添加测试文件即可.\n而使用了with torch.no_grad后，所有计算得出的tensor的requires_grad都自动设置为False，即使前面已经设置了requires_grad = True.\n为什么要这样做？因为当测试的时候，直接导入模型进行训练就行，不需要再求导数反向传播，大大节省时间和内存\nmodel.py\n\u0026quot;\u0026quot;\u0026quot;构建模型\u0026quot;\u0026quot;\u0026quot;\rimport torch.nn as nn\rimport config\rimport torch\rimport torch.nn.functional as F\rclass ImdbModel(nn.Module):\rdef __init__(self):\rsuper(ImdbModel,self).__init__()\rself.embedding = nn.Embedding(num_embeddings=len(config.ws),embedding_dim=200,padding_idx=config.ws.PAD)\rself.lstm = nn.LSTM(input_size=200,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True,dropout=0.5)\rself.fc1 = nn.Linear(64*2,64)\rself.fc2 = nn.Linear(64,2)\rdef forward(self, input):\r\u0026quot;\u0026quot;\u0026quot;\r:param input:[batch_size,max_len]\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rinput_embeded = self.embedding(input) #input embeded :[batch_size,max_len,200]\routput,(h_n,c_n) = self.lstm(input_embeded) # output：[batch_size, seq_len, 64*2];h_n :[batch_size,4,hidden_size]\rout = torch.cat([h_n[-1,:,:],h_n[-2,:,:]],dim=-1) #拼接正向最后一个输出和反向最后一个输出#out :[batch_size,hidden_size*2]\r#进行全连接\rout_fc1 = self.fc1(out)\r#进行relu\rout_fc1_relu = F.relu(out_fc1)\r#再经过一个全连接层全连接\rout_fc2= self.fc2(out_fc1_relu) #out :[batch_size,2]\rreturn F.log_softmax(out_fc2,dim=-1)\r eval.py\n\u0026quot;\u0026quot;\u0026quot;进行模型的评估\u0026quot;\u0026quot;\u0026quot;\rimport config\rfrom model import ImdbModel\rfrom dataset import get_dataloader\rimport torch\rimport torch.nn.functional as F\rimport numpy as np\rdef eval():\rmodel = ImdbModel().to(config.device)\rmodel.load_state_dict(torch.load(\u0026quot;./models/model.pkl\u0026quot;))\rloss_list = [] # 保存每一次的损失\racc_list = []\rtest_dataloader = get_dataloader(train=False)\rwith torch.no_grad():\rfor idx,(input,target) in enumerate(test_dataloader):\rinput = input.to(config.device)\rtarget = target.to(config.device)\routput = model(input)\rloss = F.nll_loss(output,target)\rloss_list.append(loss.item())\r#准确率\rpred = output.max(dim=-1)[-1]\racc_list.append(pred.eq(target).cpu().float().mean())\rprint(\u0026quot;loss mean:{},acc mean:{}\u0026quot;.format(np.mean(loss_list),np.mean(acc_list)))\rif __name__ == '__main__':\reval()\r 7.6 模型优化方法.  调参.  包括增加embedding_dim的维度；增加LSTM的hidden_size=64,num_layers=2等等.\n添加一个新的全连接层作为输出层，激活函数处理.  #再经过一个全连接层全连接\rout_fc2= self.fc2(out_fc1_relu) #out :[batch_size,2]\rreturn F.log_softmax(out_fc2,dim=-1)\r 将双向LSTM的输出带入到一个新的LSTM中  8. 总结 更深的理解了正向传播和反向传播，初步理解了RNN以及其变体LSTM。\n对于LSTM，学习了其API的调用还有一些其他可以优化模型的API调用.\n对梯度消失和梯度爆炸以及他们的解决方法进行了学习\n最后，通过上述知识，改机了前面的文本情感分类模型\n","date":"2022-07-20","permalink":"https://wudaore.github.io/post/nlp-rnn/","tags":["深度学习","NLP"],"title":"NLP--RNN"},{"content":"1. 序 对页面布局 layout进行学习\n2. 文档流 文档流（normal flow）\n- 网页是一个多层的结构，一层摞着一层\r- 通过CSS可以分别为每一层来设置样式\r- 作为用户来讲只能看到最顶上一层\r- 这些层中，最底下的一层称为文档流，文档流是网页的基础\r我们所创建的元素默认都是在文档流中进行排列\r- 对于我们来元素主要有两个状态\r在文档流中\r不在文档流中（脱离文档流）\r- 元素在文档流中有什么特点：\r- 块元素\r- 块元素会在页面中独占一行(自上向下垂直排列)\r- 默认宽度是父元素的全部（会把父元素撑满）\r- 默认高度是被内容撑开（子元素）\r- 行内元素\r- 行内元素不会独占页面的一行，只占自身的大小\r- 行内元素在页面中左向右水平排列，如果一行之中不能容纳下所有的行内元素\r则元素会换到第二行继续自左向右排列（书写习惯一致）\r- 行内元素的默认宽度和高度都是被内容撑开\r 常见的块元素：address、article、aside、audio、blockquote、canvas、dd、div、dl、fieldset、figcaption、figure、footer、form、h1、header、hgroup、hr、noscript、ol output p pre section table tfootul video\n常见的行内元素：b,big,i,small,ttabbr,acronym,cite,code,dfn,em,kbd,strong,samp,var,a,bdo,br,img,map,object,q,script,span,sub,sup,button,input,label,select,textarea\n3. 盒子模型 每一个盒子都由以下几个部分组成：\n  内容区（content）\n  内边距（padding）\n  边框（border）\n  外边距（margin）\n  3.1 边框 这么写就行了:\nborder-bottom: 10px solid red;\r border-style：solid 表示实线;dotted 点状虚线;dashed 虚线;double 双线\n3.2 内边距 padding 内边距的简写属性，可以同时指定四个方向的内边距,规则和border-width 一样\npadding-bottom: 100px;\r 3.3 外边距 外边距不会影响盒子可见框的大小，但是外边距会影响盒子的位置.一共有四个方向的外边距：规则和border-width 一样\nmargin-bottom: 100px;\r 3.4 盒子的水平布局 margin-left+border-left+padding-left+width+padding-right+border-right+margin-right = 其父元素内容区的宽度 （必须满足）\n当不满足时，会有以下调整：\n  如果这七个值中没有为 auto 的情况，则浏览器会自动调整margin-right值以使等式满足\n  对于width，margin-left，maring-right三个值：\n  如果某个值为auto，则会自动调整为auto的那个值以使等式成立\n  如果将一个宽度和一个外边距设置为auto，则宽度会调整到最大，设置为auto的外边距会自动为0\n  如果将三个值都设置为auto，则外边距都是0，宽度最大\n  如果将两个外边距设置为auto，宽度固定值，则会将外边距设置为相同的值.所以我们经常利用这个特点来使一个元素在其父元素中水平居中\n    3.5 盒子的垂直布局 子元素是在父元素的内容区中排列的，如果子元素的大小超过了父元素，则子元素会从父元素中溢出.\n使用 overflow 属性来设置父元素如何处理溢出的子元素\n可选值：\n overflow:visible，默认值 子元素会从父元素中溢出，在父元素外部的位置显示\roverflow:hidden 溢出内容将会被裁剪不会显示\roverflow:scroll 生成两个滚动条(x和y轴)，通过滚动条来查看完整的内容\roverflow:auto 根据需要生成滚动条\r 3.6 盒子的外边距折叠 垂直外边距的重叠（折叠）\n1. 相邻的垂直方向外边距会发生重叠现象\r2. 兄弟元素\r-2.1 当两外边距都是正值时：兄弟元素间的相邻垂直外边距会取两者之间的较大值.\r-2.2 特殊情况：\r2.2.1 如果相邻的外边距一正一负，则取两者的和\r2.2.2如果相邻的外边距都是负值，则取两者中绝对值较大的\r-2.3 兄弟元素之间的外边距的重叠，对于开发是有利的，所以我们不需要进行处理\r- 3 父子元素\r- 3.1 父子元素间相邻外边距，子元素的会传递给父元素（上外边距）\r- 3.2 父子外边距的折叠会影响到页面的布局，必须要进行处理\r 对于情况3(父子元素)：现有以下代码\n\u0026lt;div class=\u0026quot;box3\u0026quot;\u0026gt;\r\u0026lt;div class=\u0026quot;box4\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r 样式如下：\n.box3{\rwidth: 200px;\rheight: 200px;\rbackground-color: #bfa;\r/* padding-top: 100px; */\r/* border-top: 1px #bfa solid; */\r}\r.box4{\rwidth: 100px;\rheight: 99px;\rbackground-color: orange;\r/* margin-top: 100px; */\r}\r 效果如下图：\n如果想将内部的box移动到下方，是否可以直接咋在内部盒子中添加代码margin-top: 100px;？答案是不行，因为子元素的会传递给父元素（上外边距）.结果会导致内外一起向下移动如下图：\n这个问题怎么解决呢？有两种办法但是都比较麻烦.\n办法1：设置外box的padding-top: 100px;，同时将height属性调到100\n办法2：设置外box的border-top: 1px #bfa solid;，为了保证对齐，需要调整，再设置内boxmargin-top: 100px;.\n3.7 行内元素的盒模型 行内元素，如span，有以下特性\n- 行内元素不支持设置宽度和高度\r- 行内元素可以设置padding，但是垂直方向padding不会影响页面的布局\r- 行内元素可以设置border，垂直方向的border不会影响页面的布局\r- 行内元素可以设置margin，垂直方向的margin不会影响布局\r 不会影响页面的布局是指仍然会生效，但不会影响到其他块的位置\n练习：购物栏目 效果如下：\n设计思路\n大块竖直嵌套三个小块，可以使用ui-li来实现，但需要先去除其样式.\n要实现大块在页面中的居中，可以将左右margin设为auto\n要实现前两块底部有空最后一块没有，可以使用伪元素选择器.img-list li:not(:last-child)来实现\n要实现图片可点击，将图片放在a标签内；希望点击以后无跳转，设置\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;给予一个空的js文件.\n最后，测试图片宽度 可以直接使用截图工具.京东的图片已经调整过了，只需要确定宽度是父元的100%，高度就不会出错.\n代码如下：\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;ie=edge\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;图片列表\u0026lt;/title\u0026gt;\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;./css/reset.css\u0026quot;\u0026gt;\r\u0026lt;style\u0026gt;\r/* 设置body的背景颜色 */\rbody{\rbackground-color: antiquewhite\r}\r/* 设置外部ul的样式 */\r.img-list{\r/* 设置ul的宽度 */\rwidth: 190px;\r/* 设置ul的高度 */\rheight: 470px;\r/* 裁剪溢出的内容 */\roverflow: hidden;\r/* 使ul在页面中居中（实际示例中不需要这么写） */\r/* margin 50代表上下 auto代表左右.左右auto可以表示居中 */\rmargin: 50px auto;\rbackground-color: #F4F4F4;\r}\r/* 设置li的位置 */\r.img-list li:not(:last-child){\rmargin-bottom: 9px;\r}\r/* 设置图片的大小 */\r.img-list img{\rwidth: 100%;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;!-- \u0026lt;div\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt; --\u0026gt;\r\u0026lt;ul class=\u0026quot;img-list\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\r\u0026lt;img src=\u0026quot;./img/01/1.jpg\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\r\u0026lt;img src=\u0026quot;./img/01/2.jpg\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;javascript:;\u0026quot;\u0026gt;\r\u0026lt;img src=\u0026quot;./img/01/3.jpg\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 练习：导航栏 效果如下：\n设计思路\n可以用ul-li也可以直接div嵌套.\n选项需要点击所以用a标签；斜杠不能点击用span标签.\na标签移动经过时需要变色.使用链接伪类来实现.item a:hover{};同时需要去除下划线text-decoration: none\n最后 要让字体在父元素中垂直居中，只需将父元素的line-height设置为一个和父元素height一样的值\n代码如下：\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;ie=edge\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;京东的左侧导航\u0026lt;/title\u0026gt;\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;./css/reset.css\u0026quot;\u0026gt;\r\u0026lt;style\u0026gt;\r/* 设置body */\rbody{\r/* 设置一个网页的背景，以使我们方便查看 */\rbackground-color: #bfa;\r}\r/* 设置菜单外部容器 */\r.left-nav{\r/* 设置宽度 */\rwidth: 190px;\r/* 设置高度 */\rheight: 450px;\r/* 设置padding */\rpadding: 10px 0;\r/* 设置一个背景颜色 */\rbackground-color: #fff;\rmargin: 50px auto;\r}\r/* 设置菜单内部的item */\r.left-nav .item{\rheight: 25px;\r/* 要让一个文字在父元素中垂直居中，只需将父元素的line-height设置为一个和父元素height一样的值 */\rline-height: 25px;\r/* 设置item的右内边距，将文字向内移动 */\rpadding-left: 18px;\r/* 设置字体大小 */\rfont-size: 12px;\r}\r/* 设置/的距离 */\r.item .line{\rpadding: 0 2px;\r}\r/* 为item设置一个鼠标移入的状态 */\r.item:hover{\rbackground-color: yellow;\r}\r/* 设置超链接的样式 */\r.item a{\r/* 设置字体大小 */\rfont-size: 14px;\r/* 设置字体的颜色 */\rcolor: #333;\r/* 去除下划线 */\rtext-decoration: none;\r}\r/* 设置超链接的hover的样式 */\r.item a:hover{\rcolor: #c81623;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;!-- 创建一个外部的容器 nav(div) div(div) ul(li) --\u0026gt;\r\u0026lt;nav class=\u0026quot;left-nav\u0026quot;\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;家用电器\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;手机\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;运营商\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;数码\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;电脑\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;办公\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;家居\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;家具\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;家装\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;厨具\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;男装\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;女装\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;童装\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;内衣\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;美妆\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;个护清洁\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;宠物\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;女鞋\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;箱包\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;钟表\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;珠宝\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;男鞋\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;运动\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;户外\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;房产\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;汽车\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;汽车用品\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;母婴\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;玩具乐器\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;食品\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;酒类\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;生鲜\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;特产\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;艺术\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;礼品鲜花\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;农资绿植\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;医药保健\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;计生情趣\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;图书\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;文娱\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;电子书\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;机票\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;酒店\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;旅游\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;生活\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;理财\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;众筹\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;白条\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;保险\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;安装\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;维修\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;清洗\u0026lt;/a\u0026gt;\u0026lt;span class='line'\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;二手\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;item\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;工业品\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/nav\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 练习：新闻列表 效果如下：\n设计思路\ndiv里面嵌套ul-li.消除样式后要使li前面有框框，需要使用before伪元素.\n.news-list li::before{\rcontent: '■';\rcolor: rgb(218, 218, 218);\rfont-size: 12px;\rmargin-right: 4px;\r}\r 要使文字覆盖在图片上，需要使用margin-top:-300px;\n特别注意在使用ul-li时想要在第一个li上方空一格，对li使用margin-top是没有用的，子类的margin-top会传递给父类.正确的做法是父类设置padding-bottom\n代码如下：\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt;\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;../css/reset.css\u0026quot;\u0026gt;\r\u0026lt;style\u0026gt;\r.big {\rwidth: 300px;\rheight: 450px;\rbackground-color: yellow;\rmargin: 50px auto;\rpadding-top: 30px;\r}\r.inside li{\rlist-style-type:armenian;\r}\r.big li{\rmargin-bottom: 20px;\r}\r.big li span{\rfont-size: 20px;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;ul class=\u0026quot;big\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;\r\u0026lt;span\u0026gt;\r新闻\r\u0026lt;/span\u0026gt;\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;\r\u0026lt;img class=\u0026quot;imga\u0026quot; src=\u0026quot;../img/03/1.jpeg\u0026quot; alt=\u0026quot;费德勒\u0026quot;\u0026gt;\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;ul class=\u0026quot;inside\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;www.baidu.com\u0026quot;\u0026gt;\r震惊,我居然单抽出了苍古\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;www.baidu.com\u0026quot;\u0026gt;\r震惊,我居然单抽出了胡桃\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;www.baidu.com\u0026quot;\u0026gt;\r震惊,我居然单抽出了护摩\r\u0026lt;/a\u0026gt;\r\u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\r\u0026lt;/ul\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 3.8 盒子的尺寸 默认情况下，盒子可见框的大小由内容区、内边距和边框共同决定\nbox-sizing 用来设置盒子尺寸的计算方式（设置width和height的作用） 可选值：\ncontent-box 默认值，宽度和高度用来设置内容区的大小\rborder-box 宽度和高度用来设置整个盒子可见框的大小.此时width 和 height 指的是内容区 和 内边距 和 边框的总大小\r 使用样例:\n.box1{\rwidth: 100px;\rheight: 100px;\rbackground-color: #bfa;\rpadding: 10px;\rborder: 10px red solid;\r/* 默认情况下，盒子可见框的大小由内容区、内边距和边框共同决定\rbox-sizing 用来设置盒子尺寸的计算方式（设置width和height的作用）\r可选值：\rcontent-box 默认值，宽度和高度用来设置内容区的大小\rborder-box 宽度和高度用来设置整个盒子可见框的大小\rwidth 和 height 指的是内容区 和 内边距 和 边框的总大小\r*/\rbox-sizing: content-box;\r}\r 3.9 轮廓和圆角 阴影\nbox-shadow 用来设置元素的阴影效果，阴影不会影响页面布局\n第一个值 水平偏移量 设置阴影的水平位置 正值向右移动 负值向左移动\r第二个值 垂直偏移量 设置阴影的水平位置 正值向下移动 负值向上移动\r第三个值 阴影的模糊半径\r第四个值 阴影的颜色\r 用法：\nbox-shadow: 0px 0px 50px rgba(0, 0, 0, .3) ;  轮廓\noutline 用来设置元素的轮廓线，用法和border一模一样\n与边框的区别在于：1.轮廓不占用空间。 2.轮廓线可能不是矩形的。\n用法：\n.box1:hover{\routline: 10px red solid;\r}\r 圆角\nborder-radius 可以分别指定四个角的圆角\n四个值 左上 右上 右下 左下\r三个值 左上 右上/左下 右下 两个个值 左上/右下 右上/左下  用法：\nborder-radius: 20px / 40px; /*设置椭圆*/\r border-radius: 50%;/* 将元素设置为一个圆形 */\r ","date":"2022-07-19","permalink":"https://wudaore.github.io/post/css2/","tags":["web","css"],"title":"CSS(2)"},{"content":"目录 序 NLP基础 文本情感分类 总结 1. 序 NLP\u0026ndash;word embedding的学习\n2. RNN-NLP基础 2.1 N-gram N表示能够用在一起的词语的数量.使用N-gram模型时，往往将N个词语当成一个单位使用\n区别于传统分词， N-gram考虑了词语间的顺序，比传统分词效果更好\n2.1 文本向量化 one-hot encoding：\n在独热编码中，每一个token（词语）都被一个长度为N的向量表示，其中N代表词典长度.\n不可避免的，这么做会导致矩阵稀疏，占用空间大\nword embedding：\n区别于独热编码，word embedding的维度由人为规定而不取决于词典长度.矩阵中每个向量的值都是一个超参数，是随机生成的，之后会在学习过程中训练而获得.\n在这过程中，模型会将token转换为数字，再将数字转换为向量.\n距离:维度为4的token的word embedding过程：首先训练集的形状为[batch_size,N],然后经过转化，变为[batch_size,N,4]的形状.\n因此，word embedding使用一个稠密矩阵来表示token.\npytorch的API如下：\n3. 实战-文本情感分类 3.1 数据的预处理\u0026ndash;数据类的定义 训练和测试的文本，都放在文件夹aclImdb中.其中都包括pos和neg两个文件夹，分别表示消极文本和积极文本.\n对于每个文本，下划线之前的是编号，下划线之后的是评分.大于5分被视为积极，小于5则被视为消极.\n根据之前的学习我们知道，要导入数据集需要先定义一个数据类，继承于from torch.utils.data.Dataset.\n这个类要复写三个方法.\n首先是__init__，初始化方法.可以在该方法中加入每个文件的路径.\n def __init__(self,train=True):\r# super(ImdbDataset,self).__init__()\rdata_path = r\u0026quot;D:\\BaiduNetdiskDownload\\阶段9-人工智能NLP项目\\第四天\\代码\\data\\aclImdb\u0026quot;\rdata_path += r\u0026quot;\\train\u0026quot; if train else r\u0026quot;\\test\u0026quot;\rself.total_path = [] #保存所有的文件路径\rfor temp_path in [r\u0026quot;\\pos\u0026quot;,r\u0026quot;\\neg\u0026quot;]:\rcur_path = data_path + temp_path\rself.total_path += [os.path.join(cur_path,i) for i in os.listdir(cur_path) if i.endswith(\u0026quot;.txt\u0026quot;)]\r 其次是__getitem__方法.用于通过下标取出对应的数据.本样例中，需要返回分词后的文本和标签.分词需要经过去除无必要字符等操作.分词和复写后如下：\n# 分词\r\u0026quot;\u0026quot;\u0026quot;\r实现额外的方法\r\u0026quot;\u0026quot;\u0026quot;\rimport re\rdef tokenlize(sentence):\r\u0026quot;\u0026quot;\u0026quot;\r进行文本分词\r:param sentence: str\r:return: [str,str,str]\r\u0026quot;\u0026quot;\u0026quot;\rfileters = ['!', '\u0026quot;', '#', '$', '%', '\u0026amp;', '\\(', '\\)', '\\*', '\\+', ',', '-', '\\.', '/', ':', ';', '\u0026lt;', '=', '\u0026gt;',\r'\\?', '@', '\\[', '\\\\', '\\]', '^', '_', '`', '\\{', '\\|', '\\}', '~', '\\t', '\\n', '\\x97', '\\x96', '”', '“', ]\rsentence = sentence.lower() #把大写转化为小写\rsentence = re.sub(\u0026quot;\u0026lt;br /\u0026gt;\u0026quot;,\u0026quot; \u0026quot;,sentence)\r# sentence = re.sub(\u0026quot;I'm\u0026quot;,\u0026quot;I am\u0026quot;,sentence)\r# sentence = re.sub(\u0026quot;isn't\u0026quot;,\u0026quot;is not\u0026quot;,sentence)\r# re.sub用竖线隔开代表同时替换\rsentence = re.sub(\u0026quot;|\u0026quot;.join(fileters),\u0026quot; \u0026quot;,sentence)\rresult = [i for i in sentence.split(\u0026quot; \u0026quot;) if len(i)\u0026gt;0]\rreturn result\r  # 复写\rdef __getitem__(self, idx):\rfile = self.total_path[idx]\rreview = utils.tokenlize(open(file, encoding='utf-8').read()) #评论\rlabel = int(file.split(\u0026quot;_\u0026quot;)[-1].split(\u0026quot;.\u0026quot;)[0])\rlabel = 0 if label \u0026lt;5 else 1\rreturn review,label\r 最后是__len__方法，用于统计数据集的长度.复写后如下：\n def __len__(self):\rreturn len(self.total_path)\r 3.2 数据的预处理\u0026ndash;dataloader 定义数据类后，需要再定义Dataloader对象来存储数据.\n运行以下代码\ndataset = ImdbDataset(train)\rbatch_size = config.train_batch_size if train else config.test_batch_size\rreturn DataLoader(dataset,batch_size=batch_size,shuffle=True)\r 会发现如下报错：\nRuntimeError: each element in list of batch should be of equal size\r 这是因为dataset中的每个文本分词后长度不一致，而DataLoader需要长度一致的文本.\n修改代码，定义collate_fn方法.这个方法用于对每个batch进行处理，最终输出一个batch的返回值：\ndef collate_fn(batch):\r\u0026quot;\u0026quot;\u0026quot;\r对batch数据进行处理\r:param batch: [一个getitem的结果，getitem的结果,getitem的结果]\r:return: 元组\r\u0026quot;\u0026quot;\u0026quot;\rreviews,labels = zip(*batch)\rreviews = torch.LongTensor([config.ws.transform(i,max_len=config.max_len) for i in reviews])\rlabels = torch.LongTensor(labels)\rreturn reviews,labels\rdef get_dataloader(train=True):\rdataset = ImdbDataset(train)\rbatch_size = config.train_batch_size if train else config.test_batch_size\rreturn DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)\r 经过collate_fn处理前的batch:([token, label], [token, label], [token, label]\u0026hellip;)\n经过collate_fn处理后的batch:([token, token, token\u0026hellip;], [label, label, label])\n3.3 文本序列化 思路分析：\n  对词语进行分词\n  分词后存入词典，并根据次数进行过滤，并统计次数\n  实现文本转数字序列\n  实现数字序列转文本\n  需要解决的问题：\n  每个batch的长度不同，需要对短句子进行填充.代码中用\u0026rsquo;UKN\u0026rsquo;表示\n  测试集冲出现新的词语，需要用特殊字符代理.代码中用\u0026rsquo;PAD\u0026rsquo;表示\n  具体代码如下：\n\u0026quot;\u0026quot;\u0026quot;\r文本序列化\r\u0026quot;\u0026quot;\u0026quot;\rclass WordSequence:\rUNK_TAG = \u0026quot;\u0026lt;UNK\u0026gt;\u0026quot; #表示未知字符\rPAD_TAG = \u0026quot;\u0026lt;PAD\u0026gt;\u0026quot; #填充符\rPAD = 0\rUNK = 1\rdef __init__(self):\rself.dict = { #保存词语和对应的数字\rself.UNK_TAG:self.UNK,\rself.PAD_TAG:self.PAD\r}\rself.count = {} #统计词频的\rdef fit(self,sentence):\r\u0026quot;\u0026quot;\u0026quot;\r接受句子，统计词频\r:param sentence:[str,str,str]\r:return:None\r\u0026quot;\u0026quot;\u0026quot;\rfor word in sentence:\rself.count[word] = self.count.get(word,0) + 1 #所有的句子fit之后，self.count就有了所有词语的词频\rdef build_vocab(self,min_count=5,max_count=None,max_features=None):\r\u0026quot;\u0026quot;\u0026quot;\r根据条件构造 词典\r:param min_count:最小词频\r:param max_count: 最大词频\r:param max_features: 最大保留的词语数\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rif min_count is not None:\rself.count = {word:count for word,count in self.count.items() if count \u0026gt;= min_count}\rif max_count is not None:\rself.count = {word:count for word,count in self.count.items() if count \u0026lt;= max_count}\rif max_features is not None:\r# 先根据词频进行排序 保留前max_features个\rself.count = dict(sorted(self.count.items(),lambda x:x[-1],reverse=True)[:max_features])\rfor word in self.count:\rself.dict[word] = len(self.dict) #每次word对应一个数字\r#把dict进行翻转\rself.inverse_dict = dict(zip(self.dict.values(),self.dict.keys()))\rdef transform(self,sentence,max_len=None):\r\u0026quot;\u0026quot;\u0026quot;\r把句子转化为数字序列\r:param sentence:[str,str,str]\r:return: [int,int,int]\r\u0026quot;\u0026quot;\u0026quot;\rif len(sentence) \u0026gt; max_len:\rsentence = sentence[:max_len]\relse:\rsentence = sentence + [self.PAD_TAG] *(max_len- len(sentence)) #填充PAD\rreturn [self.dict.get(i,1) for i in sentence]\rdef inverse_transform(self,incides):\r\u0026quot;\u0026quot;\u0026quot;\r把数字序列转化为字符\r:param incides: [int,int,int]\r:return: [str,str,str]\r\u0026quot;\u0026quot;\u0026quot;\r# dict.get(a,b):a是键值key，如果存在dict存在键值a，则函数返回dict[a]；否则返回b，如果没有定义b参数，则返回None。\rreturn [self.inverse_dict.get(i,\u0026quot;\u0026lt;UNK\u0026gt;\u0026quot;) for i in incides]\rdef __len__(self):\rreturn len(self.dict)\rif __name__ == '__main__':\r# 保存模型\rimport pickle\rimport os\rfrom tqdm import tqdm\rfrom utils import tokenlize\rws = WordSequence()\rpath = r'D:\\BaiduNetdiskDownload\\阶段9-人工智能NLP项目\\第四天\\代码\\data\\aclImdb\\train'\rtemp_data_path = [os.path.join(path, \u0026quot;pos\u0026quot;), os.path.join(path, \u0026quot;neg\u0026quot;)]\rfor data_path in temp_data_path:\rfile_paths = [os.path.join(data_path, file_name) for file_name in os.listdir(data_path) if file_name.endswith('txt')]\rfor file_path in tqdm(file_paths):\rsentence = tokenlize(open(file_path, encoding='utf-8').read())\rws.fit(sentence)\rws.build_vocab(min_count=10)\rpickle.dump(ws, open(\u0026quot;./models/myws.pkl\u0026quot;, \u0026quot;wb\u0026quot;))\r# torch.save(ws, \u0026quot;./models/ws2.pkl\u0026quot;)\r# ws = torch.load(\u0026quot;./models/ws2.pkl\u0026quot;)\rprint(len(ws))\rpass\r 3.4 构建模型 只练习word_embedding，所以模型只有一层.数据通过word_embedding后通弄个全连接层，计算log_softmax\n对于线性模型，在__init__方法中定义一些函数，在forword中定义详细的处理过程.\n\u0026quot;\u0026quot;\u0026quot;构建模型\u0026quot;\u0026quot;\u0026quot;\rimport torch.nn as nn\rimport config\rimport torch.nn.functional as F\rimport pickle\rimport dataset\rfrom word_sequece import WordSequence\rclass ImdbModel(nn.Module):\rdef __init__(self):\rsuper(ImdbModel,self).__init__()\r# padding_idx:短句填充\rself.embedding = nn.Embedding(num_embeddings=len(config.ws),embedding_dim=200,padding_idx=config.ws.PAD)\r# 全连接层，将config.max_len*200转化为2维，即二分类\rself.fc = nn.Linear(config.max_len*200,2)\rdef forward(self, input):\r\u0026quot;\u0026quot;\u0026quot;\r:param input:[batch_size,max_len]\r:return:\r\u0026quot;\u0026quot;\u0026quot;\r# word_embedding\rinput_embeded = self.embedding(input) #input embeded :[batch_size,max_len,200]\r#变形.batch_size不能变，而全连接层不改变形状.因为上面定义了self.fc = nn.Linear(config.max_len*200,2)，所以也要将输入变形为2维\rinput_embeded_viewed = input_embeded.view(input_embeded.size(0),-1)\r#全连接\rout = self.fc(input_embeded_viewed)\rreturn F.log_softmax(out,dim=-1)\r 3.5 构建训练 具体过程依然是：\n1.实例化模型，将模型设置为训练模式\n2.实例化优化器类，实例化损失函数\n3.获取，并遍历dataloader\n4.梯度置为0\n5.进行向前计算\n6.计算损失\n7.反向传播\n8.更新参数.\n\u0026quot;\u0026quot;\u0026quot;进行模型的训练\u0026quot;\u0026quot;\u0026quot;\rimport config\rfrom model import ImdbModel\rfrom dataset import get_dataloader\rfrom torch.optim import Adam\rfrom tqdm import tqdm\rimport torch.nn.functional as F\rmodel = ImdbModel()\roptimizer = Adam(model.parameters())\rdef train(epoch):\rtrain_dataloader = get_dataloader(train=True)\rbar = tqdm(train_dataloader,total=len(train_dataloader))\rfor idx,(input,target) in enumerate(bar):\roptimizer.zero_grad()\routput = model(input)\rloss = F.nll_loss(output,target)\rloss.backward()\roptimizer.step()\rbar.set_description(\u0026quot;epcoh:{} idx:{} loss:{:.6f}\u0026quot;.format(epoch,idx,loss.item()))\rif __name__ == '__main__':\rfor i in range(10):\rtrain(i)\r 4. 总结 本章讲解了word_embedding的基本原理，API的实现并进行了关于情感分析的实战\n实战过程中，在文本序列化时，需要使用pickle保存训练好的模型，以方便下次直接调用.\n必须注意的是，使用pickle保存模型时，需要在原文件的主函数中导入自己的模型.因为不这么做的话，后面的文件在使用该模型时会不知道按照什么方式反序列化\n如：在word_sequence.py的主函数中，我对该文件内的类进行了导入后再pickle.\n这个问题把我从下午困惑到了晚上.找了很多方法都不尽如人意.现在终于解决了.\nif __name__ == '__main__':\rimport pickle\rimport os\rfrom tqdm import tqdm\rfrom utils import tokenlize\r# 在主函数中导入类再pickle\rfrom word_sequece import WordSequence\rws = WordSequence()\rpath = r'D:\\BaiduNetdiskDownload\\阶段9-人工智能NLP项目\\第四天\\代码\\data\\aclImdb\\train'\rtemp_data_path = [os.path.join(path, \u0026quot;pos\u0026quot;), os.path.join(path, \u0026quot;neg\u0026quot;)]\rfor data_path in temp_data_path:\rfile_paths = [os.path.join(data_path, file_name) for file_name in os.listdir(data_path) if file_name.endswith('txt')]\rfor file_path in tqdm(file_paths):\rsentence = tokenlize(open(file_path, encoding='utf-8').read())\rws.fit(sentence)\rws.build_vocab(min_count=10)\rpickle.dump(ws, open(\u0026quot;./models/finalws.pkl\u0026quot;, \u0026quot;wb\u0026quot;))\rprint(len(ws))\r ","date":"2022-07-14","permalink":"https://wudaore.github.io/post/nlp-word-embedding/","tags":["深度学习","NLP"],"title":"NLP--word embedding"},{"content":"目录 序 pytorch数据加载器 pytorch自带数据集 数据处理方法 案例：手写数字识别 总结 1. 序 进一步学习Pytorch\n2. pytorch数据加载器 定义数据类并读取数据：\n\rclass SmsDataset(Dataset):\rdef __init__(self):\r\u0026quot;\u0026quot;\u0026quot;\r初始化\r:param None:\r:return None:\r\u0026quot;\u0026quot;\u0026quot;\rself.file_path = \u0026quot;./data/SMSSpamCollection\u0026quot;\rself.lines = open(self.file_path, encoding='utf-8').readlines()\rdef __getitem__(self, index):\r\u0026quot;\u0026quot;\u0026quot;\r获取索引对应位置的一条数据\r:param index: 索引\r:return: label: 标签\r:return: content: 文本\r\u0026quot;\u0026quot;\u0026quot;\rline = self.lines[index].strip()\rlabel = line.split(\u0026quot;\\t\u0026quot;)[0]\rcontent = line.split(\u0026quot;\\t\u0026quot;)[1]\rreturn label, content\rdef __len__(self):\r# 获取数据的总数量.\rreturn len(self.lines)\r 直接在类内用open方法读取数据，不能实现数据的批处理，打乱和多线程处理.\n故使用数据加载器，torch.utils.data.Dataloader加载，迭代数据集.\nDataLoader的参数详解如下：\nsms_dataset = SmsDataset()\rdataloader = DataLoader(sms_dataset,batch_size=2,shuffle=True,drop_last=True) # droplast:防止读取最后数据的时候因为个数不匹配batch_size报错.\rif __name__ == '__main__':\r# Mydata = SmsDataset()\r# print(Mydata[0])\r# print(Mydata.__len__())\rfor x in dataloader:\rprint(x)\rfor idx,(labels,contents) in enumerate(dataloader):\rprint(idx)\rprint(labels)\rprint(contents)\rbreak\rprint(len(sms_dataset))\rprint(len(dataloader))\r 3. pytorch自带数据集 图像数据：torchvision.datasets\n文本数据：torchtext.datasets\n以免费的图像识别数据集MNIST为例，只要对torchvision.datasets.MNIST进行实例化，就能读取数据：\nmnist = MNIST(root=\u0026quot;./data\u0026quot;,train=True,download=True,transform=func)\r 其中参数如下图：\n4. 数据处理方法 4.1 torchvision.transform.ToTensor 将一个PIL.Img对象或者一个(H，W，C)转换为取值范围为0到1的，形状为[C,H,W]的torch.FloatTensor.\n对于[H,W,C]的转换，并非使用tensor.transpose(0,2)而是使用tensor.permute(2,0,1)\n4.2 torchvision.transform.Normalize torchvision.transform.Normalize(mean,std)\r 给定均值mean和方差std，（注意，均值和方差的个数都必须和图片的通道数相同.因为这里指的是每个通道的均值和方差）即可进行标准化处理.\n即：Normalized_img = (img-mean)/std.\n4.3 torchvision.transform.Compose 将多个数据处理方法集合起来，传入数据时依次通过这些方法：\ndef mnist_dataset(train):\rfunc = torchvision.transforms.Compose([\rtorchvision.transforms.ToTensor(),\rtorchvision.transforms.Normalize(\rmean=(0.1307,),\rstd=(0.3081,)\r)]\r)\r 5. 案例：手写数字识别 5.1 数据的导入和处理. 先在config.py中添加配置选项：\n\u0026quot;\u0026quot;\u0026quot;\r项目配置\r\u0026quot;\u0026quot;\u0026quot;\rimport torch\rtrain_batch_size = 128\rtest_batch_size = 1000\rdevice = torch.device(\u0026quot;cuda\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;)\r 然后在dataset.py中导入数据.\n\u0026quot;\u0026quot;\u0026quot;\r准备数据集\r\u0026quot;\u0026quot;\u0026quot;\rimport torch\rfrom torch.utils.data import DataLoader\rfrom torchvision.datasets import MNIST\rimport torchvision\rimport conf\r# 准备minist的dataset MINSET只有一个通道，是黑白照片，所以均值和标准差是一维元组.\r# 处理后的图片形状为[batch_size,通道数(1),宽,高]\rdef mnist_dataset(train):\rfunc = torchvision.transforms.Compose([\rtorchvision.transforms.ToTensor(),\rtorchvision.transforms.Normalize(\rmean=(0.1307,),\rstd=(0.3081,)\r)]\r)\r# 1. 准备Mnist数据集\rreturn MNIST(root=\u0026quot;../data/MNIST\u0026quot;, train=train, download=False, transform=func)\rdef get_dataloader(train=True):\rmnist = mnist_dataset(train)\rbatch_size = conf.train_batch_size if train else conf.test_batch_size\rreturn DataLoader(mnist,batch_size=batch_size,shuffle=True)\rif __name__ == '__main__':\rfor (images,labels) in get_dataloader():\rprint(images.size())\rprint(labels.size())\rbreak\r 5.2 模型的建立. 数据集读入，处理完毕后带入模型中\n在进入全连接层之前需要处理数据形状.\n在models.py中输入以下代码：batch_size肯定是不能变的，照样传入；而后面的1,28,28则需要相乘之后才能传入.\n前面的博客已经解释了，全连接层其实就是一次线性回归或者一次矩阵乘法，可以实现维度的转换.\n传入第一层全连接层后，将1*28*28维的数据转换为100维的数据；传入第二层全连接层后，转换为10维的数据.\n\u0026quot;\u0026quot;\u0026quot;定义模型\u0026quot;\u0026quot;\u0026quot;\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass MnistModel(nn.Module):\rdef __init__(self):\rsuper(MnistModel,self).__init__()\rself.fc1 = nn.Linear(1*28*28,100)\rself.fc2 = nn.Linear(100,10)\rdef forward(self, image):\r# 修改形状.batch_size不能变\rimage_viwed = image.view(-1,1*28*28) #[batch_size,1*28*28]\r# 全连接层\rfc1_out = self.fc1(image_viwed) #[batch_size,100]\r# 激活函数处理，形状不变\rfc1_out_relu = F.relu(fc1_out) #[batch_siz3,100]\rout = self.fc2(fc1_out_relu) #[batch_size,10]\r# dim代表在哪个维度上进行操作.一般都在最后一个维度，代表对每一个数据\rreturn F.log_softmax(out,dim=-1)\r 5.3 多分类的损失函数. 在学习逻辑回归时，我们通过sigmod函数计算对数似然损失，从而定义二分类的损失\n而在这个多分类问题中，我们使用softmax函数来实现.区别于sigmod函数只需要计算一次，softmax需要计算样本属于每个类别的概率.公式如下图：\n整个过程可以抽象为：\n与二分类一样，多分类的损失函数依然是对数似然损失\n最后，计算每个样本的损失，也即上式的平均值.\n把softmax传入对数似然损失得到的损失函数叫交叉熵损失\npytorch中有两种实现交叉熵损失的方法\n1.\ncriterion = nn.CrossEntropyLoss()\rloss = criterion(input, target)\r 2.\n# 1.对输出值计算softmax和取对数\routput = F.log_softmax(out,dim=-1)\r# 2.使用torch中带权损失.F.nll_loss:带权损失.其中设target为权值和output相乘累加，契合对数似然损失的公式\rloss = F.nll_loss(output, target)\r 5.4 模型的训练 模型的训练有以下步骤：\n1.实例化模型，将模型设置为训练模式\n2.实例化优化器类，实例化损失函数\n3.获取，并遍历dataloader\n4.梯度置为0\n5.进行向前计算\n6.计算损失\n7.反向传播\n8.更新参数.\n训练完毕后保存模型，需要测试时直接调用\n\u0026quot;\u0026quot;\u0026quot;\r进行模型的训练\r\u0026quot;\u0026quot;\u0026quot;\rfrom dataset import get_dataloader\rfrom models import MnistModel\rfrom torch import optim\rimport torch.nn.functional as F\rimport conf\rfrom tqdm import tqdm\rimport numpy as np\rimport torch\rimport os\rfrom test import eval\r#1. 实例化模型，优化器，损失函数\rmodel = MnistModel().to(conf.device)\roptimizer = optim.Adam(model.parameters(),lr=1e-3)\r# if os.path.exists(\u0026quot;./models/model.pkl\u0026quot;):\r# model.load_state_dict(torch.load(\u0026quot;./models/model.pkl\u0026quot;))\r# optimizer.load_state_dict(torch.load(\u0026quot;./models/optimizer.pkl\u0026quot;))\r#2. 进行循环，进行训练\rdef train(epoch):\rtrain_dataloader = get_dataloader(train=True)\rbar = tqdm(enumerate(train_dataloader),total=len(train_dataloader))\rtotal_loss = []\rfor idx,(input,target) in bar:\rinput = input.to(conf.device)\rtarget = target.to(conf.device)\r#梯度置为0\roptimizer.zero_grad()\r#计算得到预测值\routput = model(input)\r#得到损失\rloss = F.nll_loss(output,target)\r#反向传播，计算损失\rloss.backward()\rtotal_loss.append(loss.item())\r#参数的更新\roptimizer.step()\r#打印数据\rif idx%10 ==0 :\r# 输出进度条\rbar.set_description(\u0026quot;epcoh:{} idx:{},loss:{:.6f}\u0026quot;.format(epoch,idx,np.mean(total_loss)))\rtorch.save(model.state_dict(),\u0026quot;./models/model.pkl\u0026quot;)\rtorch.save(optimizer.state_dict(),\u0026quot;./models/optimizer.pkl\u0026quot;)\rif __name__ == '__main__':\rfor i in range(10):\rtrain(i)\r# eval是测试函数\reval()\r 5.5 模型的测试 训练好模型之后保存，需要测试时直接调用\n测试不需要计算梯度和反向传播，直接计算损失和正确率\n\u0026quot;\u0026quot;\u0026quot;\r进行模型的评估\r\u0026quot;\u0026quot;\u0026quot;\r\u0026quot;\u0026quot;\u0026quot;\r进行模型的训练\r\u0026quot;\u0026quot;\u0026quot;\rfrom dataset import get_dataloader\rfrom models import MnistModel\rfrom torch import optim\rimport torch.nn.functional as F\rimport conf\rfrom tqdm import tqdm\rimport numpy as np\rimport torch\rimport os\rdef eval():\r# 1. 实例化模型，优化器，损失函数\rmodel = MnistModel().to(conf.device)\rif os.path.exists(\u0026quot;./models/model.pkl\u0026quot;):\r# 模型的加载\rmodel.load_state_dict(torch.load(\u0026quot;./models/model.pkl\u0026quot;))\rtest_dataloader = get_dataloader(train=False)\rtotal_loss = []\rtotal_acc = []\rwith torch.no_grad():\r# 其实与训练的时候大同小异，只是不需要计算梯度，不需要反向传播，只需要计算损失即可\rfor input,target in test_dataloader: #2. 进行循环，进行训练\rinput = input.to(conf.device)\rtarget = target.to(conf.device)\r#计算得到预测值\routput = model(input)\r#得到损失\rloss = F.nll_loss(output,target)\r#计算损失\rtotal_loss.append(loss.item())\r# 计算准确率\r# 计算预测值\r# output的形状是[batch_size,10], 其中每一行代表一条数据属于每个类别的概率，取其中的最大值作为预测结果;target的形状为[batch_size],代表每一行数据的真实结果\rpred = output.max(dim=-1)[-1]\rtotal_acc.append(pred.eq(target).float().mean().item())\rprint(\u0026quot;test loss:{},test acc:{}\u0026quot;.format(np.mean(total_loss),np.mean(total_acc)))\rif __name__ == '__main__':\r# for i in range(10):\r# train(i)\reval()\r 6. 总结  本章学习了pytorch的数据加载，读取和一些处理方法，如：转换为tensor，标准化和综合处理.同时通过手写识别的案例，更好地了解了多层神经网络训练模型的过程.\n 下一章开始将正式进入NLP\u0026ndash;深度学习.\n","date":"2022-07-13","permalink":"https://wudaore.github.io/post/pytorchb3/","tags":["深度学习","Pytorch"],"title":"pytorch基础(3)"},{"content":"1. 序 机器学习也就图一乐，赚工资还得靠web.\nweb也就图一乐，赚米还得靠想法.\n总之先把工资赚了.\nweb从前端开始，前端从html和css开始.\n1.1 浏览器内核 chrome:webkit/blink\nie:Trident\nSafari:webkit\nFirefox:Gecko\nOpera:Presto/blink\n2. HTML基础 记一些知识点\n1.写HTML时需要注意的：\n尽可能少的使用无语义的标签div和span；\n在语义不明显时，既可以使用div或者p时，尽量用p, 因为p在默认情况下有上下间距，对兼容特殊终端有利；\n不要使用纯样式标签，如：b、font、u等，改用css设置。\n需要强调的文本，可以包含在strong或者em标签中（浏览器预设样式，能用CSS指定就不用他们），strong默认样式是加粗（不要用b），em是斜体（不用i）；\n使用表格时，标题要用caption，表头用thead，主体部分用tbody包围，尾部用tfoot包围。表头和一般单元格要区分开，表头用th，单元格用td；\n表单域要用fieldset标签包起来，并用legend标签说明表单的用途；\n每个input标签对应的说明文本都需要使用label标签，并且通过为input设置id属性，在lable标签中设置for来让说明文本和相对应的input关联起来。\n2.href和src的区别：\n点击跳转\n3.label绑定的两种方法\n 第一种\r\u0026lt;label\u0026gt; 用户名： \u0026lt;input type=\u0026quot;radio\u0026quot; name=\u0026quot;usename\u0026quot; value=\u0026quot;请输入用户名\u0026quot;\u0026gt; \u0026lt;/label\u0026gt;\r第二种\r\u0026lt;label for=\u0026quot;sex\u0026quot;\u0026gt;男\u0026lt;/label\u0026gt;\r\u0026lt;input type=\u0026quot;radio\u0026quot; name=\u0026quot;sex\u0026quot; id=\u0026quot;sex\u0026quot;\u0026gt;\r 浏览器进程\n从用户输入到页面展示，是由不同的进程相互作用的\n 用户输入url,处理输入信息，主进程开始导航，交给网络进程干活  用户输入url，处理输入信息：\n如果为非url结构的字符串，交给浏览器默认引擎去搜索该字符串；\n若为url结构的字符串，浏览器主进程会交给 网络进程 ,开始干活。\n网络进程发起网络请求，其中有可能会发生重定向  网络进程会先看看是否存在本地缓存，如果有就直接返回资源给浏览器进程，无则下一步 DNS-\u0026gt; IP -\u0026gt; TCP\n网络进程拿到url后，先会进行DNS域名解析得到IP地址。如果请求协议是HTTPS，那么还需要建立TLS连接。\n接下来就是利用IP地址和服务器建立TCP连接。连接建立之后，向服务器发送请求。\n重定向： 如果响应行状态码为301（永久重定向）和302（临时），那么说明需要重定向到其他url。这时候网络进程会从响应头中的Location字段里读取重定向的地址，并重新发起网络请求\n服务器响应URL之后，主进程就要通知渲染进程，你要开始干活了  服务器收到请求信息后，会根据请求信息生成响应行、响应头、响应体，并发给网络进程。网络进程接受了响应信息之后，就开始解析响应头的内容。\n导航会通过请求头的Content-type字段判断响应体数据的类型。浏览器通过这个来决定如何显示响应体的内容。\n比如：若为application/octet-stream，则会按照下载类型来处理这个请求，导航结束。\n若为text/html，这就告诉浏览器服务器返回的是html格式，浏览器会通知渲染进程，你要干活了。\n渲染进程准备好了，要想渲染进程提交数据，这个时间叫做提交文档  默认情况，每个页面一个渲染进程。但若处于同一站点（同根域名+协议），那么渲染进程就会复用。\n渲染进程接受到数据，完成页面渲染。  渲染进程准备好后，浏览器进程发出提交文档的消息，渲染进程接受了消息之后，会跟网络进程简历传输数据的管道。\n等数据传输完成了，渲染进程会告诉浏览器进程，确认文档提交，这时候浏览器会更新页面，安全状态，url，前进后退的历史。\n到这里导航结束，进入渲染阶段。\n注：当浏览器刚开始加载一个地址之后，标签页上的图标便进入了加载状态。但此时图中页面显示的依然是之前打开的页面内容，并没立即替换为百度首页的页面。因为需要等待提交文档阶段，页面内容才会被替换。\n多进程架构\n  浏览器主进程: 管理子进程、提供服务功能\n  渲染进程：将HTML、CSS、JS渲染成界面，js引擎v8和排版引擎Blink就在上面，他会为每一个tab页面创建一个渲染进程\n  GPU进程：本来是负责处理3Dcss的，后来慢慢的UI界面也交给GPU来绘制\n  网络进程：就是负责网络请求，网络资源加载的进程\n  插件进程：负责插件的运行的，因为插件很容易崩溃，把它放到独立的进程里不要让它影响别人\n  put和post的区别\nPUT被定义为idempotent的方法，POST则不是idempotent的方法。PUT请求：如果两个请求相同，后一个请求会把第一个请求覆盖掉。（所以PUT用来改资源）Post请求：后一个请求不会把第一个请求覆盖掉。（所以Post用来增资源）\n3. HTML面试题 1. iframe的优缺点\niframe是一种框架，也是一种很常见的网页嵌入方式。\n1.1优点：\niframe能够原封不动的把嵌入的网页展现出来。\n如果有多个网页引用iframe，那么你只需要修改iframe的内容，就可以实现调用的每一个页面内容的更改，方便快捷。\n网页如果为了统一风格，头部和版本都是一样的，就可以写成一个页面，用iframe来嵌套，可以增加代码的可重用。\n如果遇到加载缓慢的第三方内容如图标和广告，这些问题可以由iframe来解决。\n1.2缺点：\n会产生很多页面，不容易管理。\niframe框架结构有时会让人感到迷惑，如果框架个数多的话，可能会出现上下、左右滚动条，会分散访问者的注意力，用户体验度差。\n代码复杂，无法被一些搜索引擎索引到，这一点很关键，现在的搜索引擎爬虫还不能很好的处理iframe中的内容，所以使用iframe会不利于搜索引擎优化。\n很多的移动设备（PDA 手机）无法完全显示框架，设备兼容性差。\niframe框架页面会增加服务器的http请求，对于大型网站是不可取的。\n现在基本上都是用Ajax来代替iframe，所以iframe已经渐渐的退出了前端开发。\n2. HTML5的form如何关闭自动完成功能？\nHTML的输入框可以拥有自动完成的功能，当你往输入框输入内容的时候，浏览器会从你以前的同名输入框的历史记录中查找出类似的内容并列在输入框面，这样就不用全部输入进去了，直接选择列表中的项目就可以了。\n但有时候我们希望关闭输入框的自动完成功能，例如当用户输入内容的时候，我们希望使用AJAX技术从数据库搜索并列举而不是在用户的历史记录中搜索。\n关闭输入框的自动完成功能有3种方法：\n  在IE的Internet选项菜单里的内容\u0026ndash;自动完成里面设置\n  设置form的autocomplete为\u0026quot;on\u0026quot;或者\u0026quot;off\u0026quot;来开启或者关闭自动完成功能\n  设置输入框的autocomplete为\u0026quot;on\u0026quot;或者\u0026quot;off\u0026quot;来开启或者关闭该输入框的自动完成功能\n  什么是 HTML5 的基本构件（building block）？\n  语义 - 提供更准确地描述内容。\n  连接 - 提供新的方式与服务器通信。\n  离线和存储 - 允许网页在本地存储数据并有效地离线运行。\n  多媒体 - 在 Open Web 中，视频和音频被视为一等公民（first-class citizens）。\n  2D/3D 图形和特效 - 提供更多种演示选项。\n  性能和集成 - 提供更快的访问速度和性能更好的计算机硬件。\n  设备访问 - 允许使用各种输入、输出设备。\n  外观 - 可以开发丰富的主题。\n  浏览器是怎么对HTML5的离线储存资源进行管理和加载的？\n  将获取的html解析成dom树\n  处理css，构成层叠样式表模型CSSOM\n  将dom树和CSSOM合并为渲染树\n  根据CSSOM将渲染树的节点布局计算\n  将渲染树节点样式绘制到页面上\n  注意：\n在渲染的过程中是自上而下渲染，js会阻塞页面的渲染，优先等js执行完成.如果在渲染的过程中改变了样式，会造成回流需要重新渲染\nlink和@import的区别？\n1、从属关系区别：\nlink属于html标签，而@import是css提供的。\n2、加载顺序区别：\n页面被加载时，link会同时被加载，而@import引用的css会等到页面被加载完再加载。\n3、兼容性区别：\nimport只在IE5以上才能识别，而link是html标签，无兼容问题。\n4、dom可操作性区别：\n可以通过JS 操作 DOM ，插入link标签来改变样式；由于 DOM 方法是基于文档的，无法使用@import的方式插入样式\n5、权重区别：\n如果已经存在相同样式，@import引入的这个样式将被该 CSS 文件本身的样式层叠掉，表现出link方式的样式权重高于@import的权重这样的直观效果。 简而言之，link和@import，谁写在后面，谁的样式就被应用，后面的样式覆盖前面的样式。）\nsrc与href的区别？\n1、href 是指向网络资源所在位置，建立和当前元素（锚点）或当前文档（链接）之间的链接，用于超链接。\n2、src是指向外部资源的位置，指向的内容将会嵌入到文档中当前标签所在位置；在请求src资源时会将其指向的资源下载并应用到文档内，例如js脚本，img图片和frame等元素。当浏览器解析到该元素时，会暂停其他资源的下载和处理，直到将该资源加载、编译、执行完毕，图片和框架等元素也如此，类似于将所指向资源嵌入当前标签内。这也是为什么将js脚本放在底部而不是头部。\n4. CSS基础\u0026ndash;选择器 外部css的好处：方便通过link引入;方便利用浏览器的缓存机制\n4.1 选择器优先级： （1）CSS选择器都有权重值，权重值越大优先级越高。\n内联样式表的权重值最高，值为1000。\nid选择器的权重值为100。\n#red{\rcolor: red;\r}\r class选择器的权值为10。\n.blue{\rcolor: blue;\r}\r 类型（元素）选择器的优先级为1。\np{\rcolor: red;\r}\r 通配符选择器的优先级为0。\n*{\rcolor: red;\r}\r （2）当权值相等时，后定义的样式表要优于先定义的样式表。\n（3）在同一组属性设置中表有“!important\u0026quot;规则的优先级最大。\n比较优先级时，需要将所有的选择器的优先级进行相加计算，最后优先级越高，则越优先显示（分组选择器是单独计算的）,选择器的累加不会超过其最大数量级，类选择器在高也不会超过id选择器\n如果优先级计算后相同，此时则优先使用靠下的样式\n可以在某一个样式的后边添加 !important ，则此时该样式会获取到最高的优先级，甚至超过内联样式，注意：在开发中这个玩意一定要慎用！\n4.2 复合选择器： 1. 后代选择器\n样例：\nul li { 样式声明 } /* 选择 ul 里面所有的 li 标签元素 */\r 2. 子选择器\n与后代选择器的区别是只能选择亲儿子.\n样例：\ndiv\u0026gt;p { 样式声明 } /* 选择 div 里面所有最近一级 p 标签元素 */\r 3. 并集选择器\n并集选择器 可以选择多组标签，同时为他们定义相同的样式，通常用于集体声明。\n并集选择器是各选择器通过英文逗号 , 连接而成，任何形式的选择器都可以作为并集选择器的一部分\n样例：\nul, div { 样式声明 }\t/* 选择 ul 和 div标签元素 */\r 4. 伪类选择器\n伪类选择器 用于向某些选择器添加特殊的效果，比如：给链接添加特殊效果（链接伪类），或选择第 n 个元素（结构伪类）。\n伪类选择器 书写最大的特点是用冒号 : 表示，比如：:hover、:first-child。\n因为伪类选择器很多，比如：链接伪类、结构伪类 等，所以这里先讲解常用的链接伪类选择器和 :focus伪类 选择器。\n链接伪类：\n用于选择不同状态的链接.\n\u0026lt;!doctype html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;复合选择器之链接伪类选择器\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\r/* 注意：要学会触类旁通，这里不只是可以改变颜色，当链接为图片时还可以改图片 */\r/* 1、a:link 把没有点击过的（访问过的）链接选出来 */\ra:link {\rcolor: #333;\rtext-decoration: none;\r}\r/* 2、a:visited 选择点击过的（访问过的）链接选出来 */\ra:visited {\rcolor: orange;\r}\r/* 3、a:hover 选择鼠标经过（停留）的那个链接 */\ra:hover {\rcolor: skyblue;\r}\r/* 4、a:active 选择的是我们鼠标正在按下还没有弹起鼠标的那个链接 */\ra:active {\rcolor: green;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;a href=\u0026quot;#\u0026quot;\u0026gt;小猪佩奇\u0026lt;/a\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r :focus伪类选择器\n:focus 伪类选择器用于选取获得焦点的表单元素。\n焦点就是光标，一般情况 input 类表单元素才能获取，因此这个选择器也主要针对于表单元素来说。\n\u0026lt;!doctype html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;IE=edge\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;复合选择器之focus伪类选择器\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\r/* 把获得光标的 input 表单元素选区出来 */\rinput:focus {\rbackground-color: pink;\rcolor: red;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt;\r\u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt;\r\u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 复合选择器总结\n   选择器 作用 特征 使用情况 用法     后代 用来选择后代元素 可以是子孙后代 较多 符号是空格 .nav a   子代 选择最近一级元素 只选亲儿子 较少 符号是大于 .nav\u0026gt;p   并集 选择某些相同样式的元素 可以用于集体声明 较多 符号是逗号 .nav, .header   链接伪类 选择不同状态的链接 跟链接相关 较多 重点记住 a{} 和 a:hover 实际开发的写法   ：focus 选择获得光标的表单 跟表单相关 较少 input:focus 记住这个写法    复合选择器的层级写得越细越好（可读性，可维护性，安全性），同时将复合选择器的层级写得越细，可以提前避免大部分的选择器优先级混乱！\n4.3 伪元素选择器： 伪元素，表示页面中一些特殊的并不真实的存在的元素（特殊的位置）\n伪元素使用 :: 开头\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;ie=edge\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rp{\rfont-size: 20px;\r}\r/* 伪元素，表示页面中一些特殊的并不真实的存在的元素（特殊的位置）\r伪元素使用 :: 开头\r::first-letter 表示第一个字母\r::first-line 表示第一行\r::selection 表示选中的内容\r::before 元素的开始 ::after 元素的最后\r- before 和 after 必须结合content属性来使用\r*/\rp::first-letter{\rfont-size: 50px;\r}\rp::first-line{\rbackground-color: yellow; }\rp::selection{\rbackground-color: purple;\r}\rdiv::before{\rcontent: 'abc';\rcolor: red;\r}\rdiv::after{\rcontent: 'haha';\rcolor: blue;\r}\r/* div::before{\rcontent: '『';\r}\rdiv::after{\rcontent: '』';\r} */\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;!-- \u0026lt;q\u0026gt;hello\u0026lt;/q\u0026gt; --\u0026gt;\r\u0026lt;div\u0026gt;Hello Hello How are you\u0026lt;/div\u0026gt;\r\u0026lt;p\u0026gt;\rLorem ipsum dolor sit amet consectetur adipisicing elit. Atque velit modi veniam nisi veritatis tempore laborum nemo ipsa itaque optio. Id odit consequatur mollitia excepturi, minus saepe nostrum vel porro.\r\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 5. 样式的继承： 样式的继承，我们为一个元素设置的样式同时也会应用到它的后代元素上.继承是发生在祖先后代之间的\n继承的设计是为了方便我们的开发，利用继承我们可以将一些通用的样式统一设置到共同的祖先元素上，这样只需设置一次即可让所有的元素都具有该样式\n注意：并不是所有的样式都会被继承：比如 背景相关的，布局相关等的这些样式都不会被继承。\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;ie=edge\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rbody{\rfont-size: 19px;\r}\rp{\rcolor: red;\rbackground-color: orange;\r}\rdiv{\rcolor: purple;\rbackground-color: blue;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;\r我是一个p元素\r\u0026lt;span\u0026gt;我是p元素中的span\u0026lt;/span\u0026gt;\r\u0026lt;/p\u0026gt;\r\u0026lt;span\u0026gt;我是p元素外的span\u0026lt;/span\u0026gt;\r\u0026lt;div\u0026gt;\r我是div\r\u0026lt;span\u0026gt;\r我是div中的span\r\u0026lt;em\u0026gt;我是span中的em\u0026lt;/em\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 6. 单位和颜色： 单位\n长度单位：\n像素\r- 屏幕（显示器）实际上是由一个一个的小点点构成的\r- 不同屏幕的像素大小是不同的，像素越小的屏幕显示的效果越清晰\r- 所以同样的200px在不同的设备下显示效果不一样\r百分比\r- 也可以将属性值设置为相对于其父元素属性的百分比\r- 设置百分比可以使子元素跟随父元素的改变而改变\rem\r- em是相对于元素的字体大小来计算的\r- 1em = 1font-size\r- em会根据字体大小的改变而改变\rrem\r- rem是相对于根元素的字体大小来计算\r \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt;\r\u0026lt;meta http-equiv=\u0026quot;X-UA-Compatible\u0026quot; content=\u0026quot;ie=edge\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rhtml{\rfont-size: 30px;\r}\r.box1{\rwidth:300px;\rheight: 100px;\rbackground-color: orange;\r}\r.box2{\rwidth: 50%;\rheight: 50%;\rbackground-color:aqua; }\r.box3{\rfont-size: 50px;\r/* width: 10em;\rheight: 10em; */\rwidth: 10rem;\rheight: 10rem;\rbackground-color: greenyellow;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;div class=\u0026quot;box1\u0026quot;\u0026gt;\r\u0026lt;div class=\u0026quot;box2\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;box3\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r 颜色\n颜色单位：\n在CSS中可以直接使用颜色名来设置各种颜色\r比如：red、orange、yellow、blue、green ... ...\r但是在css中直接使用颜色名是非常的不方便\rRGB值：\r- RGB通过三种颜色的不同浓度来调配出不同的颜色\r- R red，G green ，B blue\r- 每一种颜色的范围在 0 - 255 (0% - 100%) 之间\r- 语法：RGB(红色,绿色,蓝色)\rRGBA:\r- 就是在rgb的基础上增加了一个a表示不透明度\r- 需要四个值，前三个和rgb一样，第四个表示不透明度\r1表示完全不透明 0表示完全透明 .5半透明\r十六进制的RGB值：\r- 语法：#红色绿色蓝色\r- 颜色浓度通过 00-ff\r- 如果颜色两位两位重复可以进行简写 #aabbcc --\u0026gt; #abc\rHSL值 HSLA值\rH 色相(0 - 360)\rS 饱和度，颜色的浓度 0% - 100%\rL 亮度，颜色的亮度 0% - 100% ","date":"2022-07-10","permalink":"https://wudaore.github.io/post/css1/","tags":["web","css"],"title":"CSS(1)"},{"content":"目录 序 pytorch模型实现线性回归 各种梯度下降算法 总结 1. 序 进一步学习Pytorch\n2. pytorch模型实现线性回归 上一章中值给出了代码，这一章对代码进行详解.\n2.1 nn.module  init:自定义的方法实现的位置 forword:完成一次向前计算的过程  class MyModel(nn.Module):\rdef __init__(self):\rsuper(MyModel,self).__init__() # 继承父类的init的参数\r# self.lr = nn.Linear(1,1) # 通过全连接层.\rself.fc1 = nn.Linear(1,1)\rdef forward(self, x): #x [500,1] ---\u0026gt; y_predict :[500,1]\rout = self.lr(x)\r# out = fc1(out)\rreturn ot\r nn.Linear是预先定义好的线性模型，两个参数分别是输入和输出的维度.\nnn.Module实际上调用forword方法.\nmodel = LR()\rpredict = model(x)\r 2.2 优化器类 nn.optimizer优化器类,是torch封装的更新参数的方法.比如随机梯度下降SGD.\ntorch.optim(参数,学习率)\n对于模型中所有requires_grad=True的参数，可以通过model.parameters()获取\n使用方法：\n1.实例化\n2.参数梯度置0\n3.反向传播计算梯度\n4.更新参数\n#1. 实例化模型\rmodel = MyModel().to(device)\r#2. 实例化优化器\roptimizer = optim.Adam(model.parameters(),lr=0.1)\r#3. 实例化损失函数\rloss_fn = nn.MSELoss()\rfor i in range(500):\r#4. 梯度置为0\roptimizer.zero_grad()\r#5. 调用模型得到预测值\ry_predict = model(x)\r#6. 通过损失函数，计算得到损失\rloss = loss_fn(y_predict,y_true)\r#7. 反向传播，计算梯度\rloss.backward()\r#8. 更新参数\roptimizer.step()\r 总体代码 import torch\rimport torch.nn as nn\rfrom torch import optim\rimport time\r#定义一个device\rdevice = torch.device(\u0026quot;cuda:0\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;)\rclass MyModel(nn.Module):\rdef __init__(self):\rsuper(MyModel,self).__init__()\rself.lr = nn.Linear(1,1)\rdef forward(self, x): #x [500,1] ---\u0026gt; y_predict :[500,1]\rreturn self.lr(x)\r#0 准备数据\rx = torch.rand([500,1]).to(device)\ry_true = 3*x + 0.8\r# print(y_true)\r#1. 实例化模型，模型定义GPU运行\rmodel = MyModel().to(device)\r#2. 实例化优化器\roptimizer = optim.Adam(model.parameters(),lr=0.1) # 使用Adam算法.当然也可以用SGD算法.\r#3. 实例化损失函数\rloss_fn = nn.MSELoss() # 使用均方误差\rt0 = time.time()\rfor i in range(500):\r#4. 梯度置为0\roptimizer.zero_grad()\r#5. 调用模型得到预测值\ry_predict = model(x)\r#6. 通过损失函数，计算得到损失\rloss = loss_fn(y_predict,y_true)\r#7. 反向传播，计算梯度\rloss.backward()\r#8. 更新参数\roptimizer.step()\r#打印部分数据\rif i%10 ==0:\rprint(i,loss.item())\rfor param in model.parameters(): # parameter中就是存的w和b\rprint(param.item())\rprint(\u0026quot;total cost time:\u0026quot;,time.time()-t0) #1.3463990688323975\r 需要注意的是，在一些模型中，训练的参数和预测的参数有所不同.比如模型中存在Dropout，BatchNorm的时候.\nmodel.eval()# 表示模型为评估（预测模式）\rmodel.train(model=True) # 表示模型为训练模式\r 3. 各种梯度下降算法 一些梯度下降算法已经在之前的博客简单的提到过了.现在进行补充和扩展.\n除了上述的四种算法，还有：\n3.1 动量法. minibatch在选择小学习率是收敛慢，大学习率时可能会跳过最优点.使用动量法（Momentum）可以解决这个问题.\n动量法基于梯度的移动指数进行加权平均，对参数进行平滑处理.\n令当前梯度=v，前一次梯度为δw，学习率为α则:\nv=0.8*v+0.2*δw\nw=w-αv\n3.2 AdaGrad. AdaGrad可以达到自适应学习率的效果.具体算法如下图：\n3.3 RMSProp RMSProp算法通过对学习率进行加权，对动量法进行了改进，进一步加快收敛和防止摆动过大.具体算法如下图：\n注意 这里的δw表示本次梯度，这个动量法不同.\n3.4 Adam Adam算法结合了动量法和RMSProp，同时对学习率和梯度进行了限制.算法的具体流程如下图：\napi为：\ntorch.optim.Adam()\r ","date":"2022-07-10","permalink":"https://wudaore.github.io/post/pytorchb2/","tags":["深度学习","Pytorch"],"title":"pytorch基础(2)"},{"content":"目录 序 神经元与神经网络 pytorch基础 梯度下降 pytorch手动实现线性回归 pytorch模型实现线性回归 总结 1. 序 NLP和CV学习之前需要的一些前置知识.\n2. 神经元与神经网络 神经元是神经网络的基本单位.相互连接，组成了神经网络.\n神经元的结构如下图所示：\n其中a1到an是输入的分量；w1到wn为各分量对应的权重.\nb为偏置；f为激活函数.常见的激活函数有sigmod，relu，tanh等；\nt表示输出,表达为t=f(WTX+b)\n2.1 单层神经网络 有有限个神经元构成.每个神经元的输入向量相同.由于每个神经元输出一个标量，单层神经网络输出一个向量.维度等于神经元个数\n线性回归其实就是一个单层神经网络.\n2.2 感知机 感知机是两层的神经网络.模型表达为f(x)=sign(WTX+b).其中WTX+b可以视为线性回归.\n这篇博客比较好的总结了线性回归，逻辑回归和感知机的区别.点击跳转\n感知机是一个简单的二分类模型.其作用就是将一个n维向量空间分为两部分.给定一个输入向量，得出输出向量是正例或者反例.\n2.3 多层神经网络 分为输入层，输出层和隐藏层.\n输入层的数量一般等同于输入向量维度.对于分类问题，输出等的数量一般等于类别个数.而对于回归问题，输出为一个值.\n隐藏层的神经元越多，神经网络的非线性越强，健壮性越显著.\n全连接层：第N层（n个神经元）和第N-1层（m个神经元）两两之间都有连接.全连接层其实就是在前一层的输出的基础上进行了一次Y=WX+b的操作.如下图所示：\n2.3 激活函数 对于一些直线不好分割的数据，若单单添加神经元层数，依旧只能绘制出直线，对分割没有改进\n引入激活函数.激活函数的一个作用就是提高模型的非线性分割能力.将神经元的输出输入到激活函数中，可以实现对问题的非线性分割.对于复杂的数据，可以使用多层的激活函数.\n常见的激活函数有：\n除了提高非线性分割能力外，激活函数还能\n1.提高模型鲁棒（稳健）性\n2.缓解梯度消失\n3.加速模型收敛\n3. pytorch基础 pytorch下载地址：，选用conda下载.\n3.1 张量 0阶张量:常数,df:1\n1阶张量:向量,df :[1,2,3]\n2阶张量:矩阵,df:[[1,2]]\n3.2 张量的一些属性和方法 tensor中只有一个元素的时候,获取数据:\ntansor.itdm()\ntansor转化为ndarray:\ntansor.numpy()\n获取tansor的形状:\ntansor.size(dim)\ntdnsor的变形和转置:\ntansor.view()\ntansor.t(0,1)\ntansor的切片和索引:\n和numpy相同\n获取tansor中的最大值:\ni. tansor.max()\nii. tansor.max(dim=-1)#dim=-1获取行方向的最大值\ntansor.view和tensor.permute区别\nview 相当于将原来的张量展开成一位的数据如 [1，2，3，4，5，6]，然后再进行变形\npermute 是更灵活的transpose，可以灵活的对原数据的维度进行调换，而数据本身不变。\n举例如下：\na = torch.tensor([[[1,2,3],[4,5,6]]])\rb = a.view(3,2)\rc = a.permute(0,2,1)\rprint(a.size(),a)\rprint(b.size(),b)\rprint(c.size(),c)\r 结果：\na: torch.Size([1, 2, 3]) tensor([[[1, 2, 3],\r[4, 5, 6]]])\rb: torch.Size([3, 2]) tensor([[1, 2],\r[3, 4],\r[5, 6]]) c: torch.Size([1, 3, 2]) tensor([[[1, 4],\r[2, 5],\r[3, 6]]])  4. 梯度下降 在前面的逻辑回归等算法中已经对梯度下降进行了一些阐述.本篇结合pytorch进行总结.\n梯度下降中，梯度的更新模式如下：\n4.1 计算图 计算图，即通过图的方式来描述函数.\n举例：对于J(a,b,c) = 3(a+bc), u=a+v,v=bc，可以绘制以下计算图，并对其每个节点求偏导：\n4.2 反向传播 反向传播就是上图从右到左的一个过程.自变量abc各自的偏导就是图上连线的乘积.\n将反向传播运用到神经网络中，可以计算输出结果对每一个权重参数的偏导数：\n结果中，括号外的内容是红蓝线共享部分，括号内加号左右边分别代表红蓝线.\n实际的计算中，反向传播是从后往前一层层进行计算的，且会保存值方便下一次计算.\n4.3 pytorch实现反向传播 x = torch.ones([2,2],requires_grad=True) # x=tensor([[1., 1.],[1., 1.]], requires_grad=True)\ry = x+2\rz = 3*y.pow(2)\rout = z.mean()\rout.backward(retain_graph=True)\rprint(x.grad) # tensor([[4.5000, 4.5000],[4.5000, 4.5000]])\r# 如果x.grad不是None则需要清空，不然会把上次计算的结果累积上去\rout.backward(retain_graph=True)\rprint(x.grad) # tensor([[9., 9.],[9., 9.]])\r 还有以下注意点\n1.对于retain_graph=False的情况，torch.tensor和torch.tenser.data并无二致.而当torch.tenser.data，两者是有区别的\nt = torch.tensor(np.arange(12).reshape(3,4))\rprint(p) #tensor([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11]], dtype=torch.int32)\rprint(p.data) #tensor([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11]], dtype=torch.int32)\rt.data[-1,-1] = 100\rprint(t) #tensor([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 100]], dtype=torch.int32)\r 2.对于torch.tenser.data，并不能直接使用tensor.numpy()，而是要用tensor.detach().numpy()或者tensor.data.numpy()\n3.对于带下划线的函数，都会改变原对象的值；不带下划线就不会.如：\na.zero_() # 会改变a\ra.zero() # 不会改变a\r 5. pytorch手动实现线性回归 线性回归的过程就是：\n1.给出初始的w和b\n2.计算wx+b，计算误差（均方误差）\n3.使用梯度下降算法优化误差，计算新的w和b\n4.不断迭代过程2,3，直到达到迭代次数或者满足误差阈值.\n使用w.grad和b.grad可以计算loss对w和b的偏导从而完成梯度下降.代码如下：\n\rimport torch\rimport matplotlib.pyplot as plt\rlearning_rate = 0.1\r# 1. 准备数据 #y = 3x + 0.8\rx = torch.randn([500,1])\ry_true = 3*x + 0.8\r# 2. 计算预测值 y_pred = x * w + b\rw = torch.rand([],requires_grad=True)\rprint(\u0026quot;w is\u0026quot;, w)\r# 只有浮点型的tensor才能使用requires_grad=True这个属性.所以需要添加dtype\rb = torch.tensor(0,dtype=torch.float,requires_grad=True)\rfor k in range(30):\r# 如果w和b已经有值了，则需要清0\rfor i in [w,b]:\rif i.grad is not None:\ri.grad.data.zero_()\ry_predict = x * w + b\r# 3. 计算损失，把参数的梯度置为0，进行反向传播\r# 此时使用均方误差\rloss = (y_predict-y_true).pow(2).mean()\rloss.backward()\r# 3.1 能够得到w和b的梯度\r# 4. 梯度下降更新参数\rw.data = w.data - learning_rate * w.grad\rb.data = b.data - learning_rate * b.grad\rif k%10 == 0:\rprint(k,loss.item(),w.item(),b.item())\r# print(w,b)\r#绘图\rplt.figure(figsize=(20,8))\rplt.scatter(x.numpy(),y_true.numpy())\ry_predict = x * w + b\rplt.plot(x.numpy(),y_predict.detach().numpy(),c=\u0026quot;red\u0026quot;)\rplt.show()\r 6. pytorch，模型实现线性回归 模型的原理将在下一篇中进行阐述.此处只给出代码.\nimport torch\rimport torch.nn as nn\rfrom torch import optim\rimport time\r#定义一个device\rdevice = torch.device(\u0026quot;cuda:0\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;)\rclass MyModel(nn.Module):\rdef __init__(self):\rsuper(MyModel,self).__init__()\rself.lr = nn.Linear(1,1)\rdef forward(self, x): #x [500,1] ---\u0026gt; y_predict :[500,1]\rreturn self.lr(x)\r#0 准备数据\rx = torch.rand([500,1]).to(device)\ry_true = 3*x + 0.8\r# print(y_true)\r#1. 实例化模型\rmodel = MyModel().to(device)\r#2. 实例化优化器\roptimizer = optim.Adam(model.parameters(),lr=0.1)\r#3. 实例化损失函数\rloss_fn = nn.MSELoss()\rt0 = time.time()\rfor i in range(500):\r#4. 梯度置为0\roptimizer.zero_grad()\r#5. 调用模型得到预测值\ry_predict = model(x)\r#6. 通过损失函数，计算得到损失\rloss = loss_fn(y_predict,y_true)\r#7. 反向传播，计算梯度\rloss.backward()\r#8. 更新参数\roptimizer.step()\r#打印部分数据\rif i%10 ==0:\rprint(i,loss.item())\rfor param in model.parameters():\rprint(param.item())\rprint(\u0026quot;total cost time:\u0026quot;,time.time()-t0) #1.3463990688323975\r 7. 总结 对神经网络，感知机，多层神经网络，激活函数等的工作原理有了初步的了解.\n对Pytorch开始了新的学习，并使用Pytorch实现反向传播.而后，利用反向传播的原理，手动实现了线性回归.\n下一篇将对Pytorch的模型进行初步的学习，方便编程.\n","date":"2022-07-09","permalink":"https://wudaore.github.io/post/pytorchb1/","tags":["深度学习","Pytorch"],"title":"pytorch基础(1)"},{"content":"目录 CART树基本原理 CART建树 CART剪枝 CART回归树 CART模型树 总结 ps. 1. CART树基本原理 和ID3的切割特征所有取值相比，CART树使用了二元切割法.即，大于给定值的走左子树，小于的则走右子树.这就让CART树可以处理连续型数据.\n因此对于每个树节点，应当有一下几个值：\n进行分割的特征；进行分割的特征值；左子树；右子树.\n伪代码为：\n找到最佳切分特征：\n\u0026mdash;-如果不能分：则存为叶子节点\n\u0026mdash;-切分，得到左右子树\n\u0026mdash;-左子树调用建树函数\n\u0026mdash;-右子树调用建树函数\n2. CART树建树 导入数据\ndef loadDataSet(fileName): # 用于解析TAB分隔的浮点数的通用函数\rdataMat = [] # 假设最后一列是目标值\rfr = open(fileName)\rfor line in fr.readlines():\rcurLine = line.strip().split('\\t')\rfltLine = list(map(float,curLine)) # 将所有元素映射为浮点数.注意外面需要嵌套list函数.因为py2和py3是不一样的\rdataMat.append(fltLine)\rreturn dataMat\r 切分数据集\n# 参数为数据集合、待切分的特征和该特征的某个值\rdef binSplitDataSet(dataSet, feature, value):\rmat0 = dataSet[nonzero(dataSet[:,feature] \u0026gt; value)[0],:]\rmat1 = dataSet[nonzero(dataSet[:,feature] \u0026lt;= value)[0],:]\rreturn mat0,mat1\r 建树.leafType有两种，一种是模型树一种是回归树.后面总结.errType代表误差计算函数；而ops是一个包含树构建所需其他参数的元组。\n切分函数chooseBestSplit.满足条件时，如果是回归树则返回None和某类模型的值.如果是模型树则返回None和一个线性方程.后面总结.\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering\rfeat, val = chooseBestSplit(dataSet, leafType, errType, ops)# 选择最优的分割特征\rif feat == None: return val # 如果构建的是回归树，该模型是一个常数。如果是模型树，其模型是一个线性方程\rretTree = {}\rretTree['spInd'] = feat\rretTree['spVal'] = val\rlSet, rSet = binSplitDataSet(dataSet, feat, val)\rretTree['left'] = createTree(lSet, leafType, errType, ops)\rretTree['right'] = createTree(rSet, leafType, errType, ops)\rreturn retTree  3. CART剪枝 3.1 预剪枝 停止条件(S - bestS) \u0026lt; tolS对数据非常敏感.如果缩放一定比例，tolS就很有可能不准.不断修改停止条件，花费时间并对上述误差容忍度取平方值，即预剪枝\u0026ndash;构建树之前进行剪枝.但是这么做很麻烦\n3.2 后剪枝 后减枝即构造树之后进行剪枝.CART树的剪枝流程如下：\n基于已有的树切分测试数据：\n\u0026ndash;如果存在任一子集是一棵树，则在该子集递归剪枝过程\n\u0026ndash;计算将当前两个叶节点合并后的误差\n\u0026ndash;计算不合并的误差\n\u0026ndash;如果合并会降低误差的话，就将叶节点合并\n后剪枝使用了训练集和测试集.使用训练集构造树将测试集代入.如果分支是树则递归剪枝；直到左右都不是树，则计算误差，尝试合并.\n# 判断是不是树\rdef isTree(obj):\rreturn (type(obj).__name__=='dict')\r# 递归函数，它从上往下遍历树直到叶节点为止。如果找到两个叶节点则计算它们的平均值。\rdef getMean(tree):\rif isTree(tree['right']): tree['right'] = getMean(tree['right'])\rif isTree(tree['left']): tree['left'] = getMean(tree['left'])\rreturn (tree['left']+tree['right'])/2.0\r# ：待剪枝的树与剪枝所需的测试数据\rdef prune(tree, testData):\rif shape(testData)[0] == 0: return getMean(tree) # 如果我们没有测试数据，就将树折叠起来\rif (isTree(tree['right']) or isTree(tree['left'])):# 如果分支是树，尝试剪枝\rlSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\rif isTree(tree['left']): tree['left'] = prune(tree['left'], lSet)\rif isTree(tree['right']): tree['right'] = prune(tree['right'], rSet)\r# 如果都是叶节点 则看看能不能合并\rif not isTree(tree['left']) and not isTree(tree['right']):\rlSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\rerrorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +sum(power(rSet[:,-1] - tree['right'],2))\rtreeMean = (tree['left']+tree['right'])/2.0\rerrorMerge = sum(power(testData[:,-1] - treeMean,2))\rif errorMerge \u0026lt; errorNoMerge: # 对合并前后的误差进行比较。如果合并后的误差比不合并的误差小就进行合并操作，反之则不合并直接返回\rprint(\u0026quot;merging\u0026quot;)\rreturn treeMean\relse: return tree\relse: return tree\r 4. CART回归树 建树函数已经在上面陈列.回归树和模型树的区别就在选择最优分割特征的方式（chooseBestSplit）准确来说是误差计算函数和叶节点返回的值.\n回归树的误差计算使用的是总方差.对于一个叶子节点，返回的是平均值.\nchooseBestSplit函数总体来说可以概括为：\n对每个特征：\n\u0026ndash;对每个特征值：\n\u0026mdash;-将数据集切分成两份\n\u0026mdash;-计算切分的误差\n\u0026mdash;-如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差\n代码如下：\ndef regLeaf(dataSet):#returns the value used for each leaf\rreturn mean(dataSet[:,-1])\r# 误差估计函数regErr()。该函数在给定数据上计算目标变量的平方误差。当\r# 然也可以先计算出均值，然后计算每个差值再平方.\r# 因为这里需要返回的是总方差，所以要用均方差乘以数据集中样本的个数。\rdef regErr(dataSet):\rreturn var(dataSet[:,-1]) * shape(dataSet)[0]\rdef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\r# tolS是容许的误差下降值，tolN是切分的最少样本数\rtolS = ops[0]; tolN = ops[1]\r# 如果所有目标变量都是同一个值: 就停止并返回值\rif len(set(dataSet[:,-1].T.tolist()[0])) == 1: # 结束条件1 只剩一个特征值，不进行切分\rreturn None, leafType(dataSet)\rm,n = shape(dataSet)\r# 最佳特性的选择是由减少平均RSS误差驱动的.S是当前数据集的大小和误差\rS = errType(dataSet)\rbestS = inf; bestIndex = 0; bestValue = 0\rfor featIndex in range(n-1):\rfor splitVal in set((dataSet[:,featIndex].T.A.tolist())[0]):\rmat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\rif (shape(mat0)[0] \u0026lt; tolN) or (shape(mat1)[0] \u0026lt; tolN): continue\rnewS = errType(mat0) + errType(mat1)\rif newS \u0026lt; bestS: bestIndex = featIndex\rbestValue = splitVal\rbestS = newS\r# 如果减小 (S-bestS) 小于阈值，则不进行分离\rif (S - bestS) \u0026lt; tolS:\rreturn None, leafType(dataSet) # 结束条件2.误差不够大 不进行切分 而是构建叶节点.\rmat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\rif (shape(mat0)[0] \u0026lt; tolN) or (shape(mat1)[0] \u0026lt; tolN): # 结束条件3 子集太小 不进行切分\rreturn None, leafType(dataSet)\rreturn bestIndex,bestValue# 返回最优的返回条件和用于该次分割的值（即特征和特征值）\r 5. CART模型树 区别于回归树，模型树的误差计算计算线性预测值和真实值的平方误差.对于一个叶子节点，返回的是一个模型ws.\n# 主要功能是将数据集格式化成目标变量Y和自变量X\rdef linearSolve(dataSet): #在两个地方使用了辅助函数\rm,n = shape(dataSet)\rX = mat(ones((m,n))); Y = mat(ones((m,1)))\rX[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1]#and strip out Y\rxTx = X.T*X\rif linalg.det(xTx) == 0.0:\rraise NameError('This matrix is singular（单数）, cannot do inverse（不可求逆）,\\n\\\rtry increasing the second value of ops')\rws = xTx.I * (X.T * Y) # .I：逆矩阵.ws即为当前估计出的w的最优值，表示最佳估计.\rreturn ws,X,Y\rdef modelLeaf(dataSet):#创建一个线性模型并返回解.\rws,X,Y = linearSolve(dataSet)\rreturn ws\rdef modelErr(dataSet):\rws,X,Y = linearSolve(dataSet)\ryHat = X * ws\rreturn sum(power(Y - yHat,2))\r 至于ws = xTx.I * (X.T * Y)，这其实是线性回归找出最拟合曲线（正规方程\u0026ndash;点击跳转）的知识点.\n6. 总结 \r最后对回归树，模型树，线性模型进行比对总结：\rtrainMat = mat(loadDataSet('bikeSpeedVsIq_train.txt'))\rtestMat = mat(loadDataSet('bikeSpeedVsIq_test.txt'))\r# 回归树\rmyTree = createTree(trainMat, ops=(1, 20))\ryhat = createForeCast(myTree, testMat[:,0])\rprint(corrcoef(yhat, testMat[:, 1], rowvar=0)[0, 1])\r# 模型树\rmyTree2 = createTree(trainMat, modelLeaf, modelErr, (1,20))\ryhat2 = createForeCast(myTree2, testMat[:,0], modelTreeEval)\r# 线性模型\rprint(corrcoef(yhat2, testMat[:, 1], rowvar=0)[0, 1])\rws, X, Y = linearSolve(trainMat)\rfor i in range(shape(testMat)[0]):\ryhat[i] = testMat[i,0]*ws[1,0]+ws[0,0]\rprint(corrcoef(yhat, testMat[:, 1], rowvar=0)[0, 1]\r 结果如下：\n0.964085231822215 0.9760412191380619 0.9434684235674767\n在该数据集上，回归树和模型树获得了比线性模型更好的结果.\n7. p.s. ","date":"2022-06-29","permalink":"https://wudaore.github.io/post/cart/","tags":["机器学习","algo"],"title":"CART决策树"},{"content":"目录 SVD基本原理 SVD数学推导 相似度 SVD《实战》代码 总结 ps. 1. SVD基本原理 SVD就是将原始的数据集矩阵Data分解成三个矩阵U、Σ 和VT.\n1.1 矩阵的拉伸和旋转 在了解SVD之前 首先要知道矩阵的拉伸和旋转.\n首先是拉伸如下图所示，数据矩阵D左乘对角阵S，可以视为将D在x和y轴上进行了一定比率的拉伸，其中S必定是一个对角阵.\n其次是旋转.若想将数据集逆时针旋转θ度，只需要让数据集左乘以下矩阵，其中R必定是一个正交阵：\n假设M是一种线性变换SVD就是将这种线性变换分解为旋转-拉伸-再旋转的过程\n2. SVD的数学推导 2.1 M = UΣVT 设V为正交单位阵，经过线性变换M，既进行了旋转又进行了拉伸.结果如下图所示：\n由此可得出MV = UΣ.将V移动到右边，得到M = UΣVT（正交矩阵的转置就是它的逆）\n那么V是原始域的标准正交基，U是经过M变换以后的标准正交基\n假设M为m*n的矩阵，那么，M(m*n) = U(m*m)Σ(m*n)V(n*n)T\n由于Σ是对角阵 所以可以去掉全为0的行.即M(m*n) = U(m*n)Σ(n*n)V(n*n)T.\n此时将奇异值（Σ对角上的元素.）从大到小排列.要达到降维的目的，可以将小的奇异值去掉，也即去掉不重要的奇异值.\n如下图所示，颜色越深的奇异值越重要.\n压缩前\n压缩后\n那么这三个矩阵有什么实际意义呢？举个例子 假设U代表时间，V代表空间，奇异值代表程度，上面的图片就能表示为三种模式之和（不同时间不同空间下不同程度）\n2.2 SVD求解. 充分利用U和V正交矩阵的特性和Σ对称阵的特性，有以下求解过程：\n根据特征向量的定义，得出以下结论：\n故而，SVD求解就是这样的一个过程：\n最后，由于协方差矩阵特征向量是PCA主成分的方向，可知SVD的V为PCA主成分的方向.\n3. 相似度 书上提到了相似度.在[KNN[(http://localhost:1313/post/knn/#2-%E5%90%84%E7%A7%8D%E8%B7%9D%E7%A6%BBa-id2a)一章中，博客做了更详细的介绍\n4. SVD《实战》代码 普通的估计评分值计算函数\ndef standEst(dataMat, user, simMeas, item):\rn = shape(dataMat)[1]\rsimTotal = 0.0;\rratSimTotal = 0.0\rfor j in range(n):\ruserRating = dataMat[user,j]\r# print('userrt is', userRating)\rif userRating == 0: continue\roverLap = nonzero(logical_and(dataMat[:,item].A\u0026gt;0, \\\rdataMat[:,j].A\u0026gt;0))[0] # 寻找两个用户都评级了的物品\rif len(overLap) == 0: similarity = 0\relse: similarity = simMeas(dataMat[overLap,item], \\\rdataMat[overLap,j])\rprint('the %d and %d similarity is: %f' % (item, j, similarity))\rsimTotal += similarity\rratSimTotal += similarity * userRating\rif simTotal == 0: return 0\relse: return ratSimTotal/simTotal\r 使用SVD处理后的估计评分值计算函数\ndef svdEst(dataMat, user, simMeas, item):\rn = shape(dataMat)[1]\rsimTotal = 0.0; ratSimTotal = 0.0\rU,Sigma,VT = la.svd(dataMat)\rSig4 = mat(eye(4)*Sigma[:4]) #排列 Sig4 into a 对角矩阵\rxformedItems = dataMat.T * U[:,:4] * Sig4.I # 利用U矩阵将物品映射到低维.xformedItems是v矩阵（大概）.MV = U*sigma\rfor j in range(n):\ruserRating = dataMat[user,j]\rif userRating == 0 or j==item: continue\rsimilarity = simMeas(xformedItems[item,:].T,\\\rxformedItems[j,:].T)\rprint('the %d and %d similarity is: %f' % (item, j, similarity))\rsimTotal += similarity\rratSimTotal += similarity * userRating\rif simTotal == 0: return 0\relse: return ratSimTotal/simTotal\r 产生最高的n个推荐结果\ndef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\runratedItems = nonzero(dataMat[user,:].A==0)[1]# 找到未分级的项目.返回两个数组，一个是非0元素的行，一个是非0元素的列\rif len(unratedItems) == 0: # 如果已经全分级了 返回.\rreturn 'you rated everything'\ritemScores = []\rfor item in unratedItems:\restimatedScore = estMethod(dataMat, user, simMeas, item)\ritemScores.append((item, estimatedScore))\rreturn sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]\r 实例：图片的压缩\ndef imgCompress(numSV=3, thresh=0.8):\rmyl = []\rfor line in open('0_5.txt').readlines():\rnewRow = []\rfor i in range(32):\rnewRow.append(int(line[i]))\rmyl.append(newRow)\rmyMat = mat(myl)\rprint (\u0026quot;****original matrix******\u0026quot;)\rprintMat(myMat, thresh)\rU,Sigma,VT = la.svd(myMat)\rSigRecon = mat(zeros((numSV, numSV)))\rfor k in range(numSV):#construct diagonal matrix from vector\rSigRecon[k,k] = Sigma[k]\rreconMat = U[:,:numSV]*SigRecon*VT[:numSV,:]\rprint (\u0026quot;****reconstructed matrix using %d singular values******\u0026quot; % numSV)\rprintMat(reconMat, thresh)\r 5. 总结 SVD能够简化数据，去除噪声，提高算法的结果。\n但是，数据的转换可能难以理解。\n6. p.s. 数学推导参考视频跳转 做的真的很好\n","date":"2022-06-25","permalink":"https://wudaore.github.io/post/svd/","tags":["机器学习","algo"],"title":"特征降维-奇异值分解SVD"},{"content":"目录 PCA基本原理 PCA数学推导 PCA代码实现 PCA接口实现 总结 ps. 1. PCA基本原理 太高维度的数据处理时会大大增加计算难度和时间.所以必要时需要对特征进行降维.\nPCA，主成分分析 是一种数据降维的方法.\n首先，拿二维数据距离.对于一批数据点，如果其在x和y轴上的分布比较均匀，那么就难以舍去一些特征，做到降维了.此时要做的就是沿着覆盖数据的最大方差位置旋转坐标轴.\n如下图所示，经过旋转后，y轴的分散程度大大提高，对数据有更好的识别能力\n旋转前\n旋转后\n接下来要开始降维.对于输入的数据集矩阵，设为Xm*n,即m条数据，有n个维度.现在想要将它降到K维.设降维后的矩阵为Zm*k\n要实现上述降维，只需要让X乘以一个n*k的矩阵即可.设该矩阵为Wn*k\nZm*k = Xm*n * Wn*k\n接下来就是一系列数学优化过程.详情跳转\n2. PCA数学推导 对于新的坐标轴 Z ，W的取值要让其方差最大化，也即数据最分散，提高识别度.\n这里需要限制W为单位向量，即||W|| = 1.这么做是为了后面的优化.不然直接取W等于无穷大就能满足最大了 .没有实际意义.\n要求最大值，可以用我们数值的拉格朗日乘子法求解.\n求解的过程涉及二次型求导，点击查看证明\n最终得到解：\n令Cov(x) = S，SW−αW=0正好是特征值的定义.\n接着推，可知，我们需要的是最大的（若干个）特征值.\n最后得出PCA的基本步骤：\n整理原始矩阵Xm*n ​ 求原始矩阵X的协防差阵S=Cov(X)\n求解协防差阵的特征值和特征向量。\n选取最大的K(人为给定)个特征值所对应的特征向量组成构成矩阵Wn*k ​ 直接进行矩阵计算 Z = XW\n3. PCA代码实现 # lowDDataMat:降维后的数据, reconMat：重构后的原始数据\rdef pca(dataMat, topNfeat=9999999):\rmeanVals = mean(dataMat, axis=0)\rmeanRemoved = dataMat - meanVals # 计算并减去原始数据集的平均值\rcovMat = cov(meanRemoved, rowvar=0)# 计算协方差矩阵\reigVals,eigVects = linalg.eig(mat(covMat))# 计算特征值和特征向量\r# print('特征值')\r# print(eigVals)\r# print('特征向量')\r# print(eigVects)\reigValInd = argsort(eigVals) # 根据特征值排序结果的逆序就可以得到topNfeat个最大的特征向量.这里得到的是下标\r# print('eigValInd is',eigValInd)\reigValInd = eigValInd[:-(topNfeat+1):-1] # 去掉不需要的数据\r# print('val after dele', eigValInd)\rredEigVects = eigVects[:,eigValInd] # 重组,得到N个最大的特征向量.reorganize eig vects largest to smallest\r# print(redEigVects)\rlowDDataMat = meanRemoved * redEigVects# 将数据转换为新的维度\rreconMat = (lowDDataMat * redEigVects.T) + meanVals\rreturn lowDDataMat, reconMat\r 书上的代码先将数据减去了均值，最后又将重构的数据加上了均值.这是因为减去均值后再除以标准差得出的数值就是标准化数据.\n4. PCA接口实现 详见之前的博客\n5. 总结 PCA其实是有很多限制的。比如，可以做PCA降维的前提必须保证数据是线性分布的，如果数据不是线性的，就不适用.\n6. p.s. 参考文章\n","date":"2022-06-24","permalink":"https://wudaore.github.io/post/pca/","tags":["机器学习","algo"],"title":"特征降维-主成分分析PCA"},{"content":"目录 FP树基本原理 FP树源码 总结 ps. 1. FP树基本原理 在寻找频繁项集时，Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁.在数据量大时运行会很慢.\n而FP-groth算法通过构造FP树，只需要扫描数据集两遍即可.FP树如下图所示.\n区别于传统的树，FP树可以出现重复内容，并且对于重复元素，使用“node link”进行连接，方便进行回溯.\n1.1 FP树节点结构 包括节点名，节点次数，linknode（指向同名节点），节点双亲和节点孩子.\n有inc和disp方法.inc方法用于修改节点次数，disp方法则用于打印该节点和该节点的孩子组成的树\n跳转至代码\n1.2 FP树的构建 首先对于所有符合频率要求（达到指定次数）的元素，构建一个头指针表，存放这些元素.头指针表同样需要使用linknode连接树中的同名元素.头指针表需要根据元素频度进行排序\n第一次遍历数据集会获得每个元素项的出现频率。接下来，去掉不满足最小支持度的元素项\n第二次遍历之前，需要对集合项进行排序，让频度高的排在前面.\n然后开始构建FP树.如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝.\n跳转至代码\n1.3 条件模式基 FP树已经构造完毕.那么要如何查看频繁项集呢？引入条件模式基的概念.条件模式基即是以所查找元素项为结尾的路径集合.FP树中加入父亲节点就是为了方便回溯，构造路径.\n跳转至代码\n1.4 条件FP树 光构造FP树并不足以帮助我们获取所有的频繁项集.\n总结一下过程，对于每个元素a的FP树H中的每个元素b都与a的组合频繁，而在a元素对应的H中，有与b频繁的元素c，那么{a，b，c}也频繁。\n同理可以处理c及其他元素。之所以可以这样做是应为元素a与其对应的H中的所有元素都频繁出现，而在该H中，b又与c频繁出现，c在该H中，所以a，b，c频繁。每一个元素对应不同的H，每一次递归时，对应的频繁项集的长度就会增加，H范围会在上一递归层H_old的范围的基础上缩小。\n代码具体过程如下：\n对于每一个频繁项，都要创建一棵条件FP树.某个元素是频繁项，但是在指定元素的条件FP树中，它就不一定是了.\n对于输入的大批量数据，先调用createTree方法构建头结点表和树节点；先将头结点表中的元素按照出现频率从小到大进行排序，然后遍历头节点表，找到它每一个元素的条件模式基.\n而后，将条件基作为数据集输入到建树函数createTree中构建条件树.最后，如果树中有元素项的话，递归调用mineTree()函数.\n跳转至代码\n参考博客\n这篇博客举了树上了例子，很好的帮助我们了解条件FP树\n2. FP树建树，更新树源码 2.1 FP树节点. # 节点值，count变量增加值，父节点\rclass treeNode:\rdef __init__(self, nameValue, numOccur, parentNode):\rself.name = nameValue\rself.count = numOccur\rself.nodeLink = None\rself.parent = parentNode #needs to be updated\rself.children = {} def inc(self, numOccur):\rself.count += numOccur\rdef disp(self, ind=1):\rprint(' '*ind, self.name, ' ', self.count)\rfor child in self.children.values():\rchild.disp(ind+1)\r 2.2 构建FP树. 函数updateTree用于递归增长树，updateHeader将同名元素使用LinkNode连接.主函数createTree的返回值一是树节点，二是头结点表.\ndef createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine\r# print('data is', dataSet)\rheaderTable = {}\r#go over dataSet twice\rfor trans in dataSet:#first pass counts frequency of occurance)\rfor item in trans:\rheaderTable[item] = headerTable.get(item, 0) + dataSet[trans]\r# print('header table is', headerTable)\rfor k in list(headerTable.keys()): #remove items not meeting minSup\rif headerTable[k] \u0026lt; minSup: del(headerTable[k])\rfreqItemSet = set(headerTable.keys())# 过滤后只剩下频繁项\r# print('fre is ', freqItemSet)\r#print 'freqItemSet: ',freqItemSet\rif len(freqItemSet) == 0: return None, None # 没有频繁项 直接结束\rfor k in headerTable:\rheaderTable[k] = [headerTable[k], None]# 将headerTable 用链表节点重新格式化\r# print('uodate table is ', headerTable)# {'z': [5, None], 'r': [3, None],\r# 'y': [3, None], 't': [3, None], 'x': [4, None], 's': [3, None]}\rretTree = treeNode('Null Set', 1, None) # 建树\rfor tranSet, count in dataSet.items(): # 第二遍遍历数据\rlocalD = {}\rfor item in tranSet: # 将事务项按顺序排列\rif item in freqItemSet:\rlocalD[item] = headerTable[item][0]\r# print('localD is', localD)\rif len(localD) \u0026gt; 0:\rorderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)] # 按频率排序.\r# 非常漂亮的列表生成式+lambda\rupdateTree(orderedItems, retTree, headerTable, count)# 用有序的频率项集填充树\rreturn retTree, headerTable #return tree and header table\r# 传入参数：按频率排序以后的列表，树，头指针列表，数据集键值对中的值（\r# 初始都为1，在init函数中设置，也即每次检测到存在就加一）\rdef updateTree(items, inTree, headerTable, count):\r# print('+++')\r# print('item ', items)\r# print('tree', inTree)\r# print('header table', headerTable)\r# print('count',count)\rif items[0] in inTree.children:# 检查 Items[0] 是否在 retTree.children中\rinTree.children[items[0]].inc(count) # 如果在，增加计数\relse: # 如果不在，就将 items[0] 加入到 inTree.children\rinTree.children[items[0]] = treeNode(items[0], count, inTree)\rif headerTable[items[0]][1] == None: # 更新 header table.更新后的格式：{'z': # [5, \u0026lt;__main__.treeNode object at 0x00000208CEE05B20\u0026gt;]}，值中第二个元素是一个树节点\rheaderTable[items[0]][1] = inTree.children[items[0]]\relse: # 如果已经有值了 要确保节点链接指向树中该元素项的每一个实例\rupdateHeader(headerTable[items[0]][1], inTree.children[items[0]])\r# print('second update is ', headerTable)\rif len(items) \u0026gt; 1:# 用剩余的有序项递归调用updateTree()\rupdateTree(items[1::], inTree.children[items[0]], headerTable, count)\r# print('+++')\rdef updateHeader(nodeToTest, targetNode): #对于结点inTree.children[items[0]]和其对应元素,\r# 此时在头指针表里该元素有指针存在（即树中已经有了该元素的存在），把这个元素所在结点和该元素上一次出现的结点“连”起来\rwhile (nodeToTest.nodeLink != None): # nodeLink,是同一元素之间的连接\rnodeToTest = nodeToTest.nodeLink\rnodeToTest.nodeLink = targetNode\r 2.3 条件模式基. 非常易懂的代码，不做过多解释.\ndef ascendTree(leafNode, prefixPath): if leafNode.parent != None:\rprefixPath.append(leafNode.name)\rascendTree(leafNode.parent, prefixPath)\r# 构建条件基\rdef findPrefixPath(basePat, treeNode): #treeNode comes from header table\rcondPats = {}\rwhile treeNode != None:\rprefixPath = []\rascendTree(treeNode, prefixPath)\rif len(prefixPath) \u0026gt; 1: condPats[frozenset(prefixPath[1:])] = treeNode.count\rtreeNode = treeNode.nodeLink\rreturn condPats\r 2.4 条件FP树. def mineTree(inTree, headerTable, minSup, preFix, freqItemList):\rbigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]#(sort header table)\r# print('bigL is ', bigL)\rfor basePat in bigL: # 从标题表的底部开始\rnewFreqSet = preFix.copy()\rnewFreqSet.add(basePat)\r# print('finalFrequent Item: ',newFreqSet)\rfreqItemList.append(newFreqSet)\r# print('freqItemList is', freqItemList)\rcondPattBases = findPrefixPath(basePat, headerTable[basePat][1])\r# print('condPattBases :',basePat, condPattBases)\r#2. construct cond FP-tree from cond. pattern base\rmyCondTree, myHead = createTree(condPattBases, minSup) #将条件基设为数据集输入到建树函数中.因为如果某元素是频繁的，那么它的条件FP树上的元素和它的组合也是频繁的.而不在该树上的元素就不一定了\r# print('head from conditional tree: ', myHead)\rif myHead != None: #3. mine cond. FP-tree\r# print('conditional tree for: ',newFreqSet)\rmyCondTree.disp(1)\r# 如若树中有元素项，则递归调用\rmineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\r 3. 总结 FP-growth算法只需要两遍遍历数据集\u0026ndash;第一遍获取元素频率，过滤不符合要求的元素；第二遍直接构建FP树，速度快于Apriori算法.\n为了和Apriori算法一样获取频繁项集，FP-growth需要对于每个频繁元素，获取它的条件模式基和它条件模式基的条件模式基\u0026hellip;..如此迭代.原理是对于某个频繁元素，其条件FP树上与该元素的组合也是频繁的.当然，条件FP树需要再一次过滤频率不符合的元素.因为对于整体频繁，对于某元素不一定频繁.\n4. p.s. dict.get(key, default=None)作用是返回字典中键的值，如若不存在则返回默认值.\nsorted的key参数可以传入一个函数.使用lambda更方便.示例\n","date":"2022-06-23","permalink":"https://wudaore.github.io/post/fp-growth/","tags":["机器学习","algo"],"title":"FP-growth算法"},{"content":"目录 Apriori关联分析基本原理 关联分析《实战》源码 关联规则基本原理 关联规则《实战》源码 总结 ps. 1. Apriori关联分析基本原理 频繁项集是指那些经常出现在一起的物品集合.使用频繁项集和关联规则，商家可以更好地理解他们的顾客.\n**支持度（support）**被定义为数据集中包含该项集的记录所占的比例\n**可信度或置信度（confidence）**针对一条关联规则来定义.\n支持度和可信度是用来量化关联分析是否成功的方法.\n当物品数量多时，针对每一种组合，统计其出现的频繁程度速度很慢.比如对于N种物品，有2n-1种项集组合.当N很大时遍历穷举显然不合适.此时引出Apriori关联分析减少计算量\n在学习Apriori算法之前，需要知道Apriori原理.也即，某个项集是频繁的，那么它的所有子集也是频繁的.逆否定理即子集不频繁，项集频繁.这就可以通过子集来对超集进行剪枝.\n算法流程如下：\n当集合中项的个数大于0时\n构建一个k个项组成的候选项集的列表\r检查数据以确认每个项集都是频繁的\r保留频繁项集并构建k+1项组成的候选项集的列表\r 2. 关联分析《实战》源码 首先需要创建一个辅助函数扫描数据集\n# 构建集合C1\rdef createC1(dataSet):\rC1 = []\rfor transaction in dataSet:\rfor item in transaction:\rif not [item] in C1:\rC1.append([item])\rC1.sort()\rreturn list(map(frozenset, C1)) # 需要使用不可修改的frozen set # 扫描\rdef scanD(D, Ck, minSupport):\r# ssCnt实际上就是统计D中的每个值在ck中出现的次数\r# retList是筛选以后的支持度达到最小支持度的Ck列表\r# supportData是ck中每一个值出现的频率，也即支持度\rssCnt = {}\rfor tid in D:\rfor can in Ck:\rif can.issubset(tid): # 如果ck中的元素都包括在D中\rif not can in ssCnt:\rssCnt[can]=1\relse:\rssCnt[can] += 1\rnumItems = float(len(D))\rretList = []\rsupportData = {}\rfor key in ssCnt:\rsupport = ssCnt[key]/numItems\rif support \u0026gt;= minSupport:\rretList.insert(0,key)\rsupportData[key] = support\rreturn retList, supportData\r createC1构建了C1，也即大小为1的所有候选项集的集合，格式为{[1],[2]\u0026hellip;}\n而scanD有三个输入参数，分别是数据集、候选项集列表Ck以及感兴趣项集的最小支持度minSupport.该函数的核心作用就是筛选达到最小支持度的Ck列表，并返回ck中每一个值出现的频率，也即支持度.\n然后是Apriori算法的具体实现.Apriori算法的优化之处就在于对于支持度低的子集，舍弃其超集\ndef aprioriGen(Lk, k): #creates Ck\rretList = []\rlenLk = len(Lk)\rfor i in range(lenLk):\rfor j in range(i+1, lenLk):\r# print('+++')\rL1 = list(Lk[i])[:k-2]\rL2 = list(Lk[j])[:k-2]\rL1.sort(); L2.sort()\r# print(Lk)\r# print(k)\r# print(L1)\r# print(L2)\r# print('+++')\rif L1==L2: # 输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck\rretList.append(Lk[i] | Lk[j]) # 合并两个集合\r# print('ret is', retList)\rreturn retList\r# 获得频繁集项和其支持度\rdef apriori(dataSet, minSupport = 0.5):\rC1 = createC1(dataSet)\rD = list(map(set, dataSet))\rL1, supportData = scanD(D, C1, minSupport)\rL = [L1]\r# print('L is', L)\rk = 2\rwhile (len(L[k-2]) \u0026gt; 0):\rCk = aprioriGen(L[k-2], k)\r# print('-----')\r# print(L[k-2])\r# print(Ck)\r# print('-----')\rLk, supK = scanD(D, Ck, minSupport)# Ck是候选集列表.需要通过scanD来筛选掉频率低于最小支持度的项\rsupportData.update(supK)\rL.append(Lk)# 低位集合不频繁那么它的超集一定不频繁.L中只会添加频繁的集合\rk += 1\rreturn L, supportData\r 厘清CK，LK，supportData和L的关系.\nCK是将L输入到aprioriGen后得出的产物，其含义是输入元素的各种合并后的项集.AprioriGen函数的输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck.“输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck”的思想有效避免了计算的过度繁杂.\nLK是将CK输入到scanD后得出的产物.其含义是上述项集后筛选掉支持度小于最小支持度后的结果.\nsupportData是每次scanD返回的supportData的合集.里面存放每一个组合的支持度.\nL初始只有L1（即初始数据集扫描以后）中的内容.随着迭代不断进行，L中只会添加频繁的集合，直到达到迭代结束的条件.\n总的来说，apriori就是不断迭代数据，每次迭代产生一系列项集，筛选掉不频繁的，留下频繁的项集就是apriori的输出结果.\n3. 关联规则基本原理 关联规则，差不多可以理解为某个元素或者某个元素集合是否可能会推导出另一个元素.\n一条规则P ➞ H的可信度定义为support(P | H)/support(P)，其中(P | H)的含义是P和H的并集.在构造关联规则时，我们先生成一个可能的规则列表，然后测试每条规则的可信度。如果可信度不满足最小要求，则去掉该规则.\n与关联分析类似，如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求，这可以大大减少计算量.\n4.关联规则《实战》源码 # 频繁项集列表、包含那些频繁项集支持数据的字典、最小可信度阈值\rdef generateRules(L, supportData, minConf=0.7): #supportData is a dict coming from scanD\rbigRuleList = []\r# print('L is', L)\r# print('supp is,', suppData)\r# print('conf is', minConf)\rfor i in range(1, len(L)):# 只获取有两个或更多元素的集合.L[0]都只有一个元素 所以被舍弃\rfor freqSet in L[i]:\rH1 = [frozenset([item]) for item in freqSet]\r# print('---')\r# print('h1 is', H1)\r# print('freqSet is', freqSet)\rif (i \u0026gt; 1):\rrulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) # freqSet：频繁项集；H1：可以出现在规则右部的元素列表\relse:\rcalcConf(freqSet, H1, supportData, bigRuleList, minConf)\rreturn bigRuleList # 对规则进行评估\rdef calcConf(freqSet, H, supportData, brl, minConf=0.7):\rprunedH = [] #create new list to return\rfor conseq in H:\rconf = supportData[freqSet]/supportData[freqSet-conseq] # 计算可信度（一条规则P ➞ H的可信度定义为support(P | H)/support(P)。）\rif conf \u0026gt;= minConf: # 如果某条规则满足最小可信度值，那么将这些规则输出到屏幕显示。通过检查的规则也会被返回，并被用在下一个函数rulesFromConseq()中。同时也需要对列表brl进行填充，而brl是前面通过检查的bigRuleList。\rprint(freqSet-conseq,'--\u0026gt;',conseq,'conf:',conf)\rbrl.append((freqSet-conseq, conseq, conf))\rprunedH.append(conseq)\rreturn prunedH\r# 从最初的项集中生成更多的关联规则\rdef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\rm = len(H[0])# m为频繁集大小\r# print('m is', m)\rif (len(freqSet) \u0026gt; (m + 1)): # 频繁集大到可以移除为m的子集\rHmp1 = aprioriGen(H, m+1)# 生成H元素中的无重复组合.hmp1即下一次迭代的H列表，包含所有可能的规则\r# print('hmp1 is', Hmp1)\rHmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)# 所有通过检查的规则\rif (len(Hmp1) \u0026gt; 1): # 如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则。\rrulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\r 三个函数.generateRules是主函数，输入值为频繁项集列表、包含那些频繁项集支持数据的字典、最小可信度阈值.freqSet：频繁项集；H1：可以出现在规则右部的元素列表.\ncalcConf是对规则进行评估的函数.conf = supportData[freqSet]/supportData[freqSet-conseq]，即通过上述公式support(P | H)/support(P)计算可信度.如果某条规则满足最小可信度值，那么将这些规则输出到屏幕显示。通过检查的规则也会被返回，并被用在下一个函数rulesFromConseq()中。同时也需要对列表brl进行填充，而brl是前面通过检查的bigRuleList.\nrulesFromConseq的作用是从最初的项集中生成更多的关联规则.通过aprioriGen生成新的项集，并通过calcConf对项集进行筛选.如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则.\n回到主函数中.因为只获取有两个或更多元素的集合.L[0]都只有一个元素 所以被舍弃，故而循环是从L[1]开始的.从L[1]之后，不断从输入的项集中生成更多的关联规则.如果不止一条规则满足要求，那么使用Hmp1迭代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则.\n5. 总结 Apriori算法可以发现元素间的不同组合，并通过Apriori原理减少计算量.\n但是Apriori算法会重新扫描整个数据集。当数据集很大时，这会显著降低频繁项集发现的速度。\n6. p.s. ","date":"2022-06-22","permalink":"https://wudaore.github.io/post/apriori/","tags":["机器学习","algo"],"title":"Apriori算法实现关联分析"},{"content":"目录 基本原理 《实战》源码 模型评估 算法优化 特征降维 API接口实现 案例-消费预测 总结 ps. 1. 基本原理 原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点为簇的均值点，重新遍历\n2. 《实战》源码 2.1 普通k均值聚类 # 计算欧式距离\rdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)\r# 随机形成质心\rdef randCent(dataSet, k):\rn = shape(dataSet)[1]\rcentroids = mat(zeros((k,n)))#create centroid mat\rfor j in range(n):#create random cluster centers, within bounds of each dimension\rminJ = min(dataSet[:,j])\rrangeJ = float(max(dataSet[:,j]) - minJ)\rcentroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) # 通过最小值+（最大值和最小值的）差值*一个0-1的数的方式，随机生成一个区间内的质心\rreturn centroids\r# k均值算法\rdef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\rm = shape(dataSet)[0]\rclusterAssment = mat(zeros((m,2)))# 簇结果分配矩阵.第一列记录簇索引值，第二列记录误差\rcentroids = createCent(dataSet, k)\rclusterChanged = True\rwhile clusterChanged:\rclusterChanged = False\rfor i in range(m):# 对于每一行（每一个数据），计算距离， 直到不变为止\rminDist = inf; minIndex = -1\rfor j in range(k):\rdistJI = distMeas(centroids[j,:],dataSet[i,:])\rif distJI \u0026lt; minDist:\rminDist = distJI; minIndex = j\rif clusterAssment[i,0] != minIndex: clusterChanged = True\rclusterAssment[i,:] = minIndex,minDist**2\rfor cent in range(k):#recalculate centroids\rptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]# 得到这个簇的所有点\rcentroids[cent,:] = mean(ptsInClust, axis=0) # 将质心重置为均值\rreturn centroids, clusterAssment\r 返回值中centroids代表质心，clusterAssment用于记录簇的聚类结果和误差\n2.2 二分k均值聚类 普通k均值算法只能达到局部最优解.初始点的选择对算法的影响很大.无法达到全局最优.此时就需要进行后处理.\n聚类算法的指标是SSE（误差平方和），也即clusterAssment第二列之和，越小 代表数据点越接近质心，聚类效果好.后处理中降低SSE的方法，可以将最大的类再进行二分的聚类，然后为了保持簇的总数不变，需要合并两个使SSE增幅最小的簇的质心.对于高维数据并不好用.\nSSE在k-means中的计算如下：\n引入二分k均值聚类.初始将所有数据看做一个簇，然后循环.对于每个簇，先计算总误差， 然后将簇二分k均值后计算误差.最后选择使误差最小的簇进行划分.当簇的数量到达k时，退出循环.\n\rdef biKmeans(dataSet, k, distMeas=distEclud):\rm = shape(dataSet)[0]\rclusterAssment = mat(zeros((m,2)))# 记录最佳聚类的类别和距离，随迭代不断更新\rcentroid0 = mean(dataSet, axis=0).tolist()[0]\rcentList =[centroid0] #create a list with one centroid\rprint(centList)\rfor j in range(m):#calc initial Error\rclusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2\rwhile (len(centList) \u0026lt; k):\rlowestSSE = inf\rfor i in range(len(centList)):\rptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]# 获得簇i的所有数据点\rcentroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)# splitClustAss:簇分配矩阵，第一列索引第二列偏差；centroidMat：距离矩阵\rsseSplit = sum(splitClustAss[:,1])# 计算分割后的误差平方和（该公式上面已经提及）\rsseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])# 计算未分割的误差平方和\rprint (\u0026quot;sseSplit, and notSplit: \u0026quot;,sseSplit,sseNotSplit)\rif (sseSplit + sseNotSplit) \u0026lt; lowestSSE:\rbestCentToSplit = i\rbestNewCents = centroidMat\rbestClustAss = splitClustAss.copy()\rlowestSSE = sseSplit + sseNotSplit\rprint(nonzero(bestClustAss[:,0].A == 1))\rbestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)# 在二分的结果中找出类别“1”，将其替换到新的聚类类序号（如第三 类，第四类）\rbestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit# 在二分的结果中找出类别“0”，将其替换为最优分割点的类别\rprint ('the bestCentToSplit is: ',bestCentToSplit)\rprint ('the len of bestClustAss is: ', len(bestClustAss))\rcentList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]# 将质心替换为最优聚类时的质心\rcentList.append(bestNewCents[1,:].tolist()[0])\rclusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss# 重新分配簇和sse\rreturn mat(centList), clusterAssment\r 2.3 二分k均值聚类处理实际数据 《实战》中使用雅虎数据集测试算法.这里就不陈列获取数据的代码了，直接上画图的代码\ndef clusterClubs(numClust=5):\rdatList = []\rfor line in open('places.txt').readlines():\rlineArr = line.split('\\t')\rdatList.append([float(lineArr[4]), float(lineArr[3])])\rdatMat = mat(datList)\rmyCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)\rfig = plt.figure()\rrect=[0.1,0.1,0.8,0.8]# 该列表的作用：第一个左右移动，第二个上下移动，第三个左右缩放，第四个上下缩放\rscatterMarkers=['s', 'o', '^', '8', 'p', \\\r'd', 'v', 'h', '\u0026gt;', '\u0026lt;']\raxprops = dict(xticks=[], yticks=[])\rax0=fig.add_axes(rect, label='ax0', **axprops)\rimgP = plt.imread('Portland.png')\rax0.imshow(imgP)\rax1=fig.add_axes(rect, label='ax1', frameon=False)\rfor i in range(numClust):\rptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]\rmarkerStyle = scatterMarkers[i % len(scatterMarkers)]\rax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)\rax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=300)\rplt.show()\r 3. 模型评估 3.1 确定K的值-肘方法（Elbow method） 对于每个K的取值，一次计算其到簇中心的平方和.随着簇的增加，平方和会逐渐减少.\n下降过程中忽然出现拐点（下降速度变慢），被认为是最佳K值\n3.2 聚类的评估-轮廓系数法（Silhouette Coefficient， SC系数） 结合聚类的凝聚度和分离度，对聚类效果进行评估\n目的是实现内部距离最小化，外部距离最大化\n范围在-1-1之间，越接近1效果越好\n3.3 聚类的评估-CH系数法（Calinski-harabasz ） CH系数追求用尽量少的类别聚类尽量多的样本，并获取较好的效果\n计算方式如下图\ns值越大聚类效果越好\n4.算法优化 首先对传统kmeans进行总结：\n优点如下：\n1.原理简单实现容易\n2.空间复杂度为O(N)，时间复杂度为O(IKN)， k为簇数，I为迭代次数\n缺点如下：\n聚类效果依赖K的选择和初始点的选择\n中心店易偏移，对利群点，噪声敏感\n很难发现大小差别很大的簇进行增量计算\n只能得到局部最优解\n除了二分kmeans，还有其他优化算法\n4.1 使用canopy算法确定初始点 canopy的原理如下：选择随机初始点，以该点为圆心，不同的值（T1，T2）为半径画同心圆.然后不断寻找圆以外的点为圆心 ，不同的值为半径画同心圆，直到所有点都能包含在一个圆中.这么做可以使初始点尽可能的远.\ncanopy的优点如下：\nkmeans的抗干扰能比比较弱，使用canopy可以直接去掉较小数据点的簇，有利于抗干扰\ncanopy选出的中心点更精确\ncanopy的缺点如下：\n算法中T1，T2的确立，仍然会落入局部最优的问题\n4.2 使用kmeans++算法确定初始点 K-Means++算法使用距离平方求解，尽可能保证下一个质心到当前质心距离最远.\nK-Means++算法的初始化过程如下所示：\n从输入的数据点集合中随机选择一个点作为第一个聚类中心\n对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x).根据D（x）计算距离.距离计算公式如下:\n选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大（并非绝对，而是将距离映射到一个概率.详情参考博客）\n重复2和3直到k个聚类中心被选出来\n利用这k个初始的聚类中心来运行标准的k-means算法\n相比于kmeans，kmeans++的主要区别就在第三步.kmeans更新中心点为簇的均值点，而++更新中心为距离相对较远的点.\n4.3 使用二分kmeans算法设置阈值进行划分 当SSE达到阈值时或k值达到要求时，停止划分\n《实战》源码中已经提及和代码原理注释，不过多赘述\n4.4 使用kmedoids算法选取质心 与kmeans的区别在于，kmeans选取新的簇中心使用平均值，而kmedoids算法比较麻烦，需要计算除原中心点外的其他点成为中心点时，代价函数（如距离和）的值.选取最小值以成为新的中心点\n这么做，相较于kmeans，提高了抗噪声能力.但是样本多的时候运行非常慢.\n4.5 其他 Kernel kmeans通过将每一个样本投射到高维空间（SVM核函数），再将处理好的数据使用普通kmeans进行聚类\nISODATA类似于《实战》中的后处理方法，类别的数目会随着聚类的进行而改变.合并（样本数少，两个类距离太近）和分裂（类内方差过大）\nMinibatch 适用于大批量数据（10000+）.原理是从大批量数据中每次选取一批进行聚类，进行多次.是大数据集的分批聚类\n5. 特征降维 包括特征选择和主成分分析（可以理解成一种特征提取的方式）.\n5.1 特征选择 5.1.1 Filter(过滤式) 主要探究特征本身，特征与特征之间和目标值之间的关联\n5.1.1.1 低方差特征过滤 两列数据方差小，代表某个特征大部分的值比较接近.需要过滤.\nAPI接口\n5.1.1.2 相关系数 包括皮尔逊相关系数和斯皮尔曼相关系数\n皮尔逊相关系数\n非常麻烦的式子，计算公式如下图：\n其中x和y为两列，即两个特征.n为个数\n相关系数r的范围是-1到1.当|r|\u0026lt;0.4为低度相关,当0.4\u0026lt;|r|\u0026lt;0.7为显著性相关,当0.7\u0026lt;|r|\u0026lt;1为高度线性相关\nAPI接口\n斯皮尔曼相关系数\n计算公式如下，其中di为二列成队变量的等级差值：\nAPI接口\n5.1.2 Embedded（嵌入式） 算法自动选择特征（特征与目标值之间的关联）\n决策树，包括信息熵，信息增益,点击跳转\n正则化，包括L1，L2，线性回归中有提到,点击跳转\n深度学习（卷积）\n5.2 主成分分析（PCA） 高维信息转低维.此过程中可能舍弃原有数据，创建新的变量.应用在回归分析或聚类分析中.\n与特征选择的区别是特征选择删除特征，没有产生新特征.而PCA产生新特征\nAPI接口\n6. API接口实现 6.1 kmeans 接口非常简单\n由于聚类算法不同于分类 ，fit和predict可以一步实现\n打分标准和分类（监督学习）也不一样，通过CH方法进行评估.\ncalinski_harabaz_score(X, y_pre)\r 来实现.分数越高，分类效果越好.\n6.2 低方差特征过滤 默认0.0的参数是方差，必须设置，比较基本不会有完全相同的两个特征\ndef var_thr():\r\u0026quot;\u0026quot;\u0026quot;\r特征选择：低方差特征过滤\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rdata = pd.read_csv(\u0026quot;./data/factor_returns.csv\u0026quot;)\rprint(data)\rtransfer = VarianceThreshold(threshold=10)\rtrans_data = transfer.fit_transform(data.iloc[:, 1:10])\rprint(\u0026quot;之前数据的形状：\\n\u0026quot;,data.iloc[:, 1:10].shape)\rprint(\u0026quot;之后数据的形状：\\n\u0026quot;,trans_data.shape)\rprint(trans_data)\r 6.3 皮尔逊, 斯皮尔曼相关系数 from scipy.stats import pearsonr, spearmanr\r# 皮尔逊演示\rx1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]\rx2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]\rret = pearsonr(x1, x2)\rprint(\u0026quot;这两列数据的皮尔逊相关系数为：\\n\u0026quot;, ret)\rret = spearmanr(x1, x2)\rprint(\u0026quot;这两列数据的斯皮尔曼相关系数为：\\n\u0026quot;, ret)\r 皮尔逊计算得出两个值.第一个值越大越相关.第二个值越小越相关.当样本数大于500时，第二个数更实用.斯皮尔曼的结果与其类似\n6.4 主成分分析 from sklearn.decomposition import PCA\rdata = [[2, 8, 4, 5],\r[6, 3, 0, 8],\r[5, 4, 9, 1]]\r# 1.保留到多少维度\r# transfer = PCA(n_components=2)\r# trans_data = transfer.fit_transform(data)\r# print(trans_data)\r# 2.保留信息的百分比\rtransfer = PCA(n_components=0.95)\rtransfer_data = transfer.fit_transform(data)\rprint(transfer_data)\r 7. 案例-消费预测 现有四张表：order表，product表，order_product表，aisle表.首先需要将多表合并，然后通过特征降维获取输入数据，机器学习后得出模型\n# 1.获取数据\r# 2.数据基本处理\r# 2.1 合并表格\r# 2.2 交叉表合并\r# 2.3 数据截取\r# 3.特征工程 — pca\r# 4.机器学习（k-means）\r# 5.模型评估\rimport pandas as pd\rfrom sklearn.decomposition import PCA\rfrom sklearn.cluster import KMeans\rfrom sklearn.metrics import silhouette_score\r# 1.获取数据\rorder_product = pd.read_csv(\u0026quot;./data/instacart/order_products__prior.csv\u0026quot;)\rproducts = pd.read_csv(\u0026quot;./data/instacart/products.csv\u0026quot;)\rorders = pd.read_csv(\u0026quot;./data/instacart/orders.csv\u0026quot;)\raisles = pd.read_csv(\u0026quot;./data/instacart/aisles.csv\u0026quot;)\r# 2.数据基本处理\r# 2.1 合并表格\rtable1 = pd.merge(order_product, products, on=[\u0026quot;product_id\u0026quot;, \u0026quot;product_id\u0026quot;])\rtable2 = pd.merge(table1, orders, on=[\u0026quot;order_id\u0026quot;, \u0026quot;order_id\u0026quot;])\rtable = pd.merge(table2, aisles, on=[\u0026quot;aisle_id\u0026quot;, \u0026quot;aisle_id\u0026quot;])\r# 2.2 交叉表合并\rdata = pd.crosstab(table[\u0026quot;user_id\u0026quot;], table[\u0026quot;aisle\u0026quot;])\r# 2.3 数据截取\rnew_data = data[:1000]\r# 3.特征工程 — pca\rtransfer = PCA(n_components=0.9)\rtrans_data = transfer.fit_transform(new_data)\r# 4.机器学习（k-means）\restimator = KMeans(n_clusters=5)\ry_pre = estimator.fit_predict(trans_data)\r# 5.模型评估\rsilhouette_score(trans_data, y_pre)\r 8. 总结 本博客介绍了k均值算法，并对其不足之处陈列了一系列改进算法.同时介绍了特征降维两种方法，即特征提取和主成分分析.最后，通过一个消费预测的实例测试了算法\n9. p.s. pd.merge\n","date":"2022-06-20","permalink":"https://wudaore.github.io/post/k-means/","tags":["机器学习","algo"],"title":"k均值聚类算法"},{"content":"目录 基本原理 bagging boosting 总结 ps. 1. 基本原理 即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测.\n对于欠拟合问题，使用boosting逐步增强学习；对于过拟合问题，使用bagging采样学习集成.\n2. bagging 2.1 bagging 集成原理和过程 1.采样.从所有版本中，采样一部分\n2.学习.训练弱学习器\n3.集成.使用平权投票\n2.2 随机森林 随机森林，即bagging+决策树.其实现流程如下：\n1.随机选k条数据（一次随机选一个样本，有放回的抽样）\n2.随机选m个特征（m远小于特征总数）\n3.训练决策树（默认CART（基尼系数）决策树）\n4.重复1-3步构造n棵弱决策树\n5.平权投票集成所有决策树\n注：随机抽样是为了让每棵树的训练集不一样\n抽样必须是有放回的，不然可能会导致每棵树训练出来的结果有很大的差异.而随机森林取决于每棵树的投票\n2.3 随机森林sklearn接口 在使用随机森林时，需要对树的个数，树的深度等超参数进行调整.泰坦尼克号样例代码如下（与决策树代码类似，改一下模型就可）：\n# 4.机器学习（模型训练）\restimator = RandomForestClassifier()\rparam_grid = {\u0026quot;n_estimators\u0026quot;: [120,200,300,500,800,1200], \u0026quot;max_depth\u0026quot;: [5, 8, 15, 25, 30]}\restimator = GridSearchCV(estimator, param_grid=param_grid, cv=5)\restimator.fit(x_train, y_train)\rprint(estimator.score(x_train, y_train))\rprint(estimator.best_estimator_) # 打印最佳参数选择\r 2.4 bagging总结 bagging集成学习方法 = Bagging+决策树/逻辑回归深度学习\u0026hellip;\n经过上面方式组成的集成方法，可以提高一定的泛化准确率，并简单，方便，通用\n3. boosting 1.训练学习器\n2.调整数据分布（对于分类争取的数据减少其权重，分类正确则增加其权重），将训练注意力集中在错误数据上\n3.重复1,2.直到达到结束条件（《实战》中的结束条件为迭代次数到了或者再也没有错误分类了）\n4.各个学习器加权投票，得出结果\n3.1 AdaBoost\u0026ndash;《实战》源码 AdaBoost将结合《实战》中的代码进行学习.AdaBoost即自适应Boost（adaptive Boost）.其基本流程如下：\n1.初始化训练数据权重相等，训练第一个学习器\n D = mat(ones((m,1))/m) #init D to all equal\r 2.计算该学习器在训练数据中的错误率\n3.计算该学习器的投票权重αt.αt的计算公式如下：\nalpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0\rbestStump['alpha'] = alpha weakClassArr.append(bestStump) # 将最佳单层决策树存入数组中\r 4.根据投票权重，调整学习器的注意力到错误数据上，对训练数据重新赋权值.\nexpon = multiply(-1*alpha*mat(classLabels).T,classEst) # 权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1\rD = multiply(D,exp(expon)) # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值\rD = D/D.sum()\r 5.重复执行1-4步n次\n6.对所有学习器投票计算权重\n\r训练模型代码\r\rdef adaBoostTrainDS(dataArr,classLabels,numIt=40):\rweakClassArr = []\rm = shape(dataArr)[0]\rD = mat(ones((m,1))/m) #init D to all equal\raggClassEst = mat(zeros((m,1)))\rfor i in range(numIt):\rbestStump,error,classEst = buildStump(dataArr,classLabels,D)# 最优单层决策树的基本信息（特征下标，阈值和符号），最小误差，分类结果\r#print('result = ', bestStump)\r#print(error)\r#print(classEst)\r#print(\"D:\",D.T)\ralpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0\rbestStump['alpha'] = alpha weakClassArr.append(bestStump) # 将最佳单层决策树存入数组中\rexpon = multiply(-1*alpha*mat(classLabels).T,classEst) # 权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1\rD = multiply(D,exp(expon)) # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值\rD = D/D.sum()\r#calc training error of all classifiers, if this is 0 quit for loop early (use break)\raggClassEst += alpha*classEst\r#print(\"aggClassEst: \",aggClassEst.T)\r# addErrors存放分类错误的特征.分错为1分对为0\raggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\r#print('aggerr', aggErrors)\rerrorRate = aggErrors.sum()/m\rprint(\"total error: \",errorRate)\rif errorRate == 0.0: break\rreturn weakClassArr,aggClassEst\r\r\r\r利用模型分类 代码\r\rdef adaClassify(datToClass,classifierArr):\r# print(classifierArr)\r# 输入：由一个或多个待分类样例daToClass以及多个弱分类器组成的数组classifierArr\r# 输出：aggClassEst的符号，即aggClassEst大于0则返回+1，小于0则返回-1\rdataMatrix = mat(datToClass)\rm = shape(dataMatrix)[0]\raggClassEst = mat(zeros((m,1)))\rfor i in range(len(classifierArr)):\rclassEst = stumpClassify(dataMatrix,classifierArr[0][i]['dim'],\\\rclassifierArr[0][i]['thresh'],\\\rclassifierArr[0][i]['ineq'])\raggClassEst += classifierArr[0][i]['alpha']*classEst # 。输出的类别估计值乘上该单层决策树的alpha权重然后累加到aggClassEst上\r# print(aggClassEst)\rreturn sign(aggClassEst)\r\r\r3.2 bagging和boosting的区别 1.数据方面\nBagging：对数据进行采样训练\nBoosting：根据前一轮学习结果调整数据重要性\n2.投票方面\nBagging：所有学习器平权投票\nBoosting：所有学习器加权投票\n3.学习顺序\nBagging：Bagging是并行的，每个学习器没有依赖关系\nBoosting：学习是串行，有先后顺序\n4.主要作用\nBagging：解决过拟合（降低方差，提高泛化性能）\nBoosting：解决欠拟合（提高训练精度，降低偏差）\n3.3 GBDT GBDT = 梯度下降+Boosting+决策树\nGBDT使用梯度下降法优化代价函数，使用一层决策树作为弱学习器，负梯度作为目标值，最后利用boosting思想进行集成\nGBDT举例\n   编号 年龄 体重 身高     1 5 20 1.1   2 7 30 1.3   3 21 70 1.7   4 30 60 1.8   5 25 65 ?    预测身高，这是一个回归问题.回归问题的损失函数可以使用平方误差（二分类使用log，多分类使用softmax）\n第一步，计算损失函数，并求出第一个预测值.\n第二步，使用误差值替换目标值.求解划分点，使得方差最小\n第三步，通过划分点划分，求解得出h1x\ny,的求法类似于上面的h0x，即对损失函数求导使其等于0\n第四步，求解h2x\nh2x的求法也类似于h1x.首先求出误差值，使用误差值替换目标值.h2x的误差值等于上一轮的目标值-预测值.\n最后经过一系列计算，得出最后的预测模型：\n3.4 GBDT和AdaBoost的区别 GBDT与Adboost最主要的区别在于两者如何识别模型的问题。\nAdaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。\nGBDT通过负梯度来识别问题，通过计算负梯度来改进模型。\nGBDT每一轮训练时所关注的重点是本轮产生结果的残差，下一轮以本轮残差作为输入，尽量去拟合这个残差，使下一轮输出的残差不断变小。所以GBDT可以做到每一轮一定向损失函数减小的梯度方向变化，而传统的boosting算法只能是尽量向梯度方向减小，这是GBDT与传统boosting算法最大的区别，这也是为什么GBDT相比传统boosting算法可以用更少的树个数与深度达到更好的效果。\n和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足，而GBDT是通过算梯度来定位模型的不足。因此相比AdaBoost，GBDT可以使用更多种类的目标函数。\n3.5 XGBoost（简单了解） 基本原理：二阶泰勒展开，boosting，正则化，决策树\n","date":"2022-06-08","permalink":"https://wudaore.github.io/post/ensemble-learning/","tags":["机器学习","algo"],"title":"集成学习"},{"content":"目录 基本原理 度量方法 常用的剪枝方法 特征工程-特征提取 决策树的实现 决策树绘制 总结 ps. 1. 基本原理 决策树是一种树形结构，每个内部节点代表一个属性上的判断.每个叶节点代表一种分类结果.它的本质就是基于数据，通过问一系列的问题(if-else)去预测结果。\n2. 度量方法 所谓熵即混乱程度.越有序熵越低.\n2.1 香农熵 计算代码如下：\ndef calcShannonEnt(dataSet):\rnumEntries = len(dataSet)\rlabelCounts = {}\rfor featVec in dataSet: #the the number of unique elements and their occurance\rcurrentLabel = featVec[-1]\rif currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\rlabelCounts[currentLabel] += 1\rshannonEnt = 0.0\rfor key in labelCounts:\rprob = float(labelCounts[key])/numEntries\rshannonEnt -= prob * log(prob,2) #log base 2\rreturn shannonEnt\r 2.2 信息增益 信息增益，即以某特征值划分前后信息熵的差值.特征值A对数据集D的增益g(D,A) = H(D) - H(D|A)\n计算代码如下：\ndef chooseBestFeatureToSplit(dataSet):\rnumFeatures = len(dataSet[0]) - 1 #the last column is used for the labels\rbaseEntropy = calcShannonEnt(dataSet)\rbestInfoGain = 0.0; bestFeature = -1\rfor i in range(numFeatures): # 对于每一个特征\rfeatList = [example[i] for example in dataSet] # 提取出这个特征所有的值\runiqueVals = set(featList) # unique这个值\rnewEntropy = 0.0\rfor value in uniqueVals: # 对于某个特征的所有值（类别）\rsubDataSet = splitDataSet(dataSet, i, value)\rprob = len(subDataSet)/float(len(dataSet))\rnewEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # 计算信息增益\rif (infoGain \u0026gt; bestInfoGain): #compare this to the best gain so far\rbestInfoGain = infoGain #if better than current best, set to best\rbestFeature = i\rreturn bestFeature #returns an integer\r 举例 探究某论坛用户流失和性别，活跃度的相关性：\n   编号 性别 活跃度 是否流失     1 男 高 0   2 女 中 0   3 男 低 1   4 女 高 0   5 男 高 0   6 男 中 0   7 男 中 1   8 女 中 0   9 女 低 1   10 女 中 0   11 女 高 0   12 男 低 1   13 女 低 1   14 男 高 0   15 男 高 0    整理后的数据如下：\n    positive negative 汇总     整体 5 10 15   男 3 5 8   女 2 5 7   高活跃 0 6 6   中活跃 1 4 5   低活跃 4 0 4    首先计算整体熵.E(S) = -( 5/15 * log(5/15) + 10/15 * log(10/15) ) = 0.9182\n然后计算性别熵.根据条件熵的公式，需要依次计算男性和女性的熵值.\nE(male) = -( 3/8 * log(3/8) + 5/8 * log(5/8) ) = 0.9543\nE(female) = -( 2/7 * log(2/7) + 5/7 * log(5/7) ) = 0.8631\n因此，性别的信息增益GAIN = E(S) - 8/15 * E(male) - 7/15 * E(female) = 0.0064\n同理，可以求出活跃度的信息增益GAIN = 0.6776 ，远远大于性别的信息增益.也即活跃度对用户流失的影响比性别大.在特征选择和特征分析的时候要更多的考虑.\n表现在决策树上，也即信息增益高的因素在树上会被优先划分.\n2.3 信息增益比 不难发现，信息增益会更偏重分类多的属性(如上面的例子，若将编号也算作类别的话，编号的信息增益将会最大，这显然是不合理的.)，因此引入信息增益比.\n首先引入属性分裂信息度量H.对于每个属性（性别，是否活跃），都会有若干个可能的取值（性别能取男，女，是否活跃能取活跃，一般，不活跃.）令p(xi)为某个取值的占比，属性分裂信息度量的计算公式为：\n而信息增益比 = 信息增益 / 属性分裂信息度量.\n2.4 基尼增益. 与信息增益同样是度量的指标.基尼增益最大的属性作为决策树的根节点.基尼增益=总体基尼值-基尼指数\n2.4.1 基尼值 GiNi(D)代表从数据集D中随机抽取两个样本，计算其不一致的概率.基尼值越小，数据集纯度越高\n2.4.2 基尼指数 一般选择划分后基尼系数最小的属性为最优划分属性.\n举例：是否拖欠贷款和是否有房，年收入和婚姻状况的关系.数据表格如下(s代表singal单身，m代表married，已结婚,d表示divorced，已离婚)：\n   编号 是否有房 婚姻状况 年收入 是否拖欠贷款     1 1 s 125 0   2 0 m 100 0   3 0 s 70 0   4 1 m 120 0   5 0 d 95 1   6 0 m 60 0   7 1 d 220 0   8 0 s 85 1   9 0 m 75 0   10 0 s 90 1    与计算信息增益类似，基尼增益需要先算出分类以前的基尼值.然后根据各个属性的不同类别（比如婚姻状况属性就有三个类别），依次计算出他们的基尼值.最后用分类以前的基尼值依次进去[ 概率（这里指的是总概率，比如这个案例中的是否拖欠贷款）* 各个类别的基尼值 ]\n以本案例来说，根节点基尼系数为0.42.\n根据是否有房，计算基尼系数增益为0.077：\n婚姻状况不同于是否有房，其并不是一个是或否的问题.需要根据婚姻状况这个属性的三种类别分别划分数据，计算基尼增益.注意划分后只有某类别与其他类别.计算过程如下：\n其中最大的增益为0.12\n年收入是数值型数据，处理方法不同于前面两种.其基尼增益的求法为从小到大排序，两两之间求出相邻值中点，以此求基尼增益.\n以60和70中间点65为例，以该点为分割时，Gini(年收入) = 0.42 - [1/10 * (1-0-1)] - { 9/10 * [ 1-(3/9)2 - (6/9)2 ] } = 0.02\n其中 1/10是小于65的点的概率；3/9是大于65的点中分类为positive的点的占比.依次通过每个中间点分割数据，得到最大的基尼增益同样是0.12\n婚姻状况\u0026ndash;married的基尼增益也是0.12，因为先被计算出来所以先用它来分割根节点，**注意：**每次分割后剩余的数据需要重新计算基尼增益，来决定下一次由谁来分割.比如分割掉married后，是否有房的基尼增益变为最大，需要根据它来分.具体流程如下图：\n2.5 小结. 决策树的变量可以有数字型和名称型.\n数字型，如上文提到的工资，需要使用“\u0026gt;=,\u0026lt;”等比值符号进行分割.而名称型，如上文提到的婚姻状况，使用“=”分割.\n决策树构建的基本步骤如下：\n1.开始将所有记录看做一个节点\n2.遍历每个变量的每一种分割方式，找到最适合的分割方式，分割数据\n3.对分割后的数据重复分割，知道最后的节点足够纯为止\n根据使用信息增益，信息增益率和基尼指数，决策树分为ID3决策树，C4.5决策树和CART决策树\n   决策树类别 分支方式 优点 缺点 备注     ID3 信息增益  只能对离散属性的数据集构造决策树，偏向取值多的属性    C4.5 信息增益率 分类结果易于理解准确率高.可以处理连续数据.对于缺失值的处理.采用了后剪枝，避免树高无限增长，避免过拟合 对数据多次扫描排序，效率低.另外，只能处理训练集全部读入内存的情况.数据集过大就不好处理 优化后解决了ID3偏向取值多的属性.   CART 基尼指数 可以进行分类和回归.可以处理离散也可以处理连续  根据上面的例子可知，CART每次分类都只是二分    3. 常用的剪枝方法 由于噪声，样本冲突，或特征即属性不能完全作为分类标准，或因为数据量不够大造成的巧合的规律性，不剪枝的决策树容易过拟合.\n3.1 预剪枝 即一边构造树时一边剪枝.主要通过(1)限制节点包含的最小样本数(2)指定树的高度或深度(3)指定节点的熵小于某个值\n3.2 后剪枝 构造完树后进行从下往上的剪枝.C4.5就是后剪枝.因为要构造完树才能剪枝，当数据量太大导致无法全部读入内存时，C4.5无法运行\n4. 特征工程-特征提取 特征提取，即将数据转换为能使机器学习的数字特征.分为三类，字典特征提取（特征离散化），文本特征提取和图像特征提取（深度学习）.本博客介绍前两类.\nsklearn中特征提取的API为feature_extraction.点击跳转\n4.1 字典特征提取 对于特征中存在的类别信息，我们一般都会one-hot编码\n具体用法和标准化的接口非常类似\ndef dict_demo():\r\u0026quot;\u0026quot;\u0026quot;\r字典特征提取\r:return:\r\u0026quot;\u0026quot;\u0026quot;\rdata = [{'city': '北京', 'temperature': 100},\r{'city': '上海', 'temperature': 60},\r{'city': '深圳', 'temperature': 30}]\r# 字典特征提取\r# 1.实例化\rtransfer = DictVectorizer(sparse=False)\r# 2.调用fit_transform\rtrans_data = transfer.fit_transform(data)\rprint(\u0026quot;特征名字是：\\n\u0026quot;, transfer.get_feature_names())\rprint(trans_data)\r 运行结果如下：\n而在数据量特别大时，sparse设置为True会更直观，提高读取效率并节省内存\n4.2 文本特征提取 4.2.1 英文文本特征提取 def english_count_text_demo():\r\u0026quot;\u0026quot;\u0026quot;\r文本特征提取 -- 英文\r:return: NOne\r\u0026quot;\u0026quot;\u0026quot;\rdata = [\u0026quot;life is is short,i like python\u0026quot;,\r\u0026quot;life is too long,i dislike python\u0026quot;]\r# 1.实例化\r# transfer = CountVectorizer(sparse=False) # 注意，没有sparse这个参数\rtransfer = CountVectorizer(stop_words=[\u0026quot;dislike\u0026quot;])\r# 2.调用fit_transform\rtransfer_data = transfer.fit_transform(data)\rprint(transfer_data)\rprint(transfer.get_feature_names())\rprint(transfer_data.toarray())\r 4.2.2 中文文本特征提取 需要先进行分词处理.本篇使用jieba分词，后续可用分词效果更好的模型如hanlp\n需要先分词后，使用\u0026quot; \u0026ldquo;.join转换成用空格分开的句子.转换后再代入.\ndef cut_word(sen):\r\u0026quot;\u0026quot;\u0026quot;\r中文分词\r:return: sen\r\u0026quot;\u0026quot;\u0026quot;\r# print(\u0026quot; \u0026quot;.join(list(jieba.cut(sen))))\rreturn \u0026quot; \u0026quot;.join(list(jieba.cut(sen)))\r def chinese_count_text_demo2():\r\u0026quot;\u0026quot;\u0026quot;\r文本特征提取 -- 中文\r:return: NOne\r\u0026quot;\u0026quot;\u0026quot;\rdata = [\u0026quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\u0026quot;,\r\u0026quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\u0026quot;,\r\u0026quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\u0026quot;]\rlist = []\rfor temp in data:\r# print(temp)\rlist.append(cut_word(temp))\rprint(list)\r# 1.实例化\rtransfer = CountVectorizer(stop_words=[\u0026quot;一种\u0026quot;, \u0026quot;还是\u0026quot;])\r# 2.调用fit_transform\rtransfer_data = transfer.fit_transform(list)\rprint(transfer.get_feature_names())\rprint(transfer_data.toarray())\r 4.3 tf-idf 作用：评估一个词在一个文件中的重要作用.某次在本文章中出现的概率高（tf，词频率），在其他文件中出现的概率低（idf，逆向文档频率），那么就适合分类.\ntfidf = tf * idf.\ntf即词频，idf = log10(总文件数/出现该词的文件数)\n5. 决策树的实现 5.1 《实战》源码实现 《实战》决策树分类中只讲了ID3决策树的实现.实现选择信息增益最大的特征的函数已经在上面列出（chooseBestFeatureToSplit）.\n下面是分割数据集的函数：\ndef splitDataSet(dataSet, axis, value): # 分割特征下标为axis，值为value的数据\rretDataSet = []\rfor featVec in dataSet:\rif featVec[axis] == value:\rreducedFeatVec = featVec[:axis] #chop out axis used for splitting\rreducedFeatVec.extend(featVec[axis+1:])\rretDataSet.append(reducedFeatVec)\rreturn retDataSet\r 票选函数：\ndef majorityCnt(classList):\rclassCount={}\rfor vote in classList:\rif vote not in classCount.keys(): classCount[vote] = 0\rclassCount[vote] += 1\rsortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\rreturn sortedClassCount[0][0]\r 最后是递归构造决策树的函数.递归停止条件一为只剩一个类别了.二位没有特征可以继续分割了\ndef createTree(dataSet,labels):\rclassList = [example[-1] for example in dataSet] # yes or no 的一个list\rif classList.count(classList[0]) == len(classList): return classList[0] # 当只剩一种类别时，停止递归\rif len(dataSet[0]) == 1: # 没有特征可以分割了，停止递归\rreturn majorityCnt(classList) # 选取类别最多的作为结果\rbestFeat = chooseBestFeatureToSplit(dataSet)\rbestFeatLabel = labels[bestFeat]\rmyTree = {bestFeatLabel:{}}\rdel(labels[bestFeat])\rfeatValues = [example[bestFeat] for example in dataSet]\runiqueVals = set(featValues)\rfor value in uniqueVals:\rsubLabels = labels[:] #copy all of labels, so trees don't mess up existing labels\rmyTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\rreturn myTree  5.2 sklearn实现 具体接口信息点击跳转\n案例：泰坦尼克号乘客生存预测\n\r点击查看\r\r1.获取数据 2.数据基本处理 2.1 确定特征值,目标值 2.2 缺失值处理 2.3 数据集划分 3.特征工程(字典特征抽取) 4.机器学习(决策树) 5.模型评估 import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.feature_extraction import DictVectorizer\n1.获取数据 data = pd.read_csv(\u0026rdquo;./titanic.csv\u0026quot;)\n2.数据基本处理 -缺失值替换 2.1 确定特征值,目标值 train = data[[\u0026ldquo;pclass\u0026rdquo;,\u0026ldquo;age\u0026rdquo;,\u0026ldquo;sex\u0026rdquo;]] label = data[\u0026ldquo;survived\u0026rdquo;] print(train.isnull().sum())\n2.2 缺失值处理 train[\u0026lsquo;age\u0026rsquo;].fillna(value=train[\u0026lsquo;age\u0026rsquo;].mean(), inplace=True) train\n2.3 数据集划分 x_train, x_test, y_train, y_test = train_test_split(train, label, random_state=22, test_size=0.2)\n3.特征工程(字典特征抽取) 字典特征抽取需要先将输入转换为字典样式 orient参数可以有很多值，用到的时候再查 x_train = x_train.to_dict(orient=\u0026ldquo;records\u0026rdquo;) x_test = x_test.to_dict(orient=\u0026ldquo;records\u0026rdquo;) transfer = DictVectorizer() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test)\n4.机器学习(决策树) estimator = DecisionTreeClassifier(max_depth=5) estimator.fit(x_train, y_train) estimator.predict(x_test)\n5.模型评估 print(estimator.score(x_test, y_test))\n\n\r6. 决策树的绘制 6.1 《实战》源码 \r获取叶子节点个数\r\rdef getNumLeafs(myTree):\rnumLeafs = 0\rfirstStr = myTree.keys()[0]\rsecondDict = myTree[firstStr]\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rnumLeafs += getNumLeafs(secondDict[key])\relse: numLeafs +=1\rreturn numLeafs\r\r\r\r计算树的高度\r\rdef getTreeDepth(myTree):\rmaxDepth = 0\rfirstStr = myTree.keys()[0]\rsecondDict = myTree[firstStr]\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rthisDepth = 1 + getTreeDepth(secondDict[key])\relse: thisDepth = 1\rif thisDepth  maxDepth: maxDepth = thisDepth\rreturn maxDepth\r\r\r\r使用文本注解绘制树节点\r\rdef plotNode(nodeTxt, centerPt, parentPt, nodeType):\rcreatePlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',\rxytext=centerPt, textcoords='axes fraction',\rva=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args )\r\r\r\r绘制分支上的值，计算父节点和子节点的中间位置，添加简单的文本信息\r\rdef plotMidText(cntrPt, parentPt, txtString):\rxMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]\ryMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\rcreatePlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\r\r\r\r绘制树\r\rdef plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on\rnumLeafs = getNumLeafs(myTree) #this determines the x width of this tree\rdepth = getTreeDepth(myTree)\rfirstStr = list(myTree.keys())[0] #the text label for this node should be this\rcntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\rplotMidText(cntrPt, parentPt, nodeTxt)\rplotNode(firstStr, cntrPt, parentPt, decisionNode)\rsecondDict = myTree[firstStr]\r# 更新下一个节点的位置的Y值\rplotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD\rfor key in secondDict.keys():\rif type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\rplotTree(secondDict[key],cntrPt,str(key)) #recursion\relse: #it's a leaf node print the leaf node\rplotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\rplotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\rplotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\rplotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\r\r\r注：该段代码比较阴间，需要注解\n设该树所有叶节点水平位置上之间的距离 d=（ 1/叶节点的个数）。\ny的坐标很好计算，只需要使用上个y的坐标-1/树高 就可以了.以下只考虑x坐标的计算。分两种情况讨论，一是非叶节点，二是叶节点。\n对于非叶节点，每个非叶节点的x位置都可以分别用它左边最近的叶节点进行求解。设某非叶结点 A 有n个叶节点，它左边最近叶节点 a 位置为（xOff，yOff），那么A应位于属于它的所有叶节点的中间位置，即水平位置上离 a 的间隔是（n+1）/2。那么它的 x坐标=xOff+d*（n+1）/2。\n对于叶节点，每个叶节点的x位置也可以用它左边最近的叶节点进行求解。左边最近的叶节点坐标位置（xOff，yOff），即 某叶节点x坐标是 xOff+d。\n\r主函数，调用 绘制树 函数\r\rdef createPlot(inTree):\rfig = plt.figure(1, facecolor='white')\rfig.clf()\raxprops = dict(xticks=[], yticks=[])\rcreatePlot.ax1 = plt.subplot(111, frameon=False, **axprops) #no ticks\r#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses plotTree.totalW = float(getNumLeafs(inTree))\rplotTree.totalD = float(getTreeDepth(inTree))\r# 使用两个全局变量plotTree.xOff和plotTree.yOff追踪已经绘制的节点位置，以及放置下一\r# 个节点的恰当位置\rplotTree.xOff = -0.5/plotTree.totalW\rplotTree.yOff = 1.0;\rprint(plotTree.xOff)\rprint(plotTree.yOff)\rplotTree(inTree, (0.5, 1.0), '')\rplt.show()\r\r\r6.2 sklearn实现 运用上面的泰坦尼克号的案例.sklearn提供了可视化决策树的接口\n\r决策树可视化\r\rfrom sklearn.tree import export_graphviz\rexport_graphviz(estimator, out_file=\"./data/tree.dot\", feature_names=['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', '女性', '男性'])\r\r\r运行后导出一个dot文件.可以直接查看但是比较糊.可以将文件内容输入到这个网站进行查看\n6. 总结 决策树有两个优点：\n一是得到的模型很容易可视化，非专家也很容易理解（至少对于较小的树而言）；\n二是算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。\n特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好\n决策树的主要缺点在于：\n即使做了预剪枝，它也经常会过拟合，泛化性能很差。\n7. p.s. 决策树可视化时特征名字和训练集不一致，最好用代码看一下训练的特征顺序，才好判定\n\rfor i in x_train:\rprint(i.toarray())\r ","date":"2022-06-05","permalink":"https://wudaore.github.io/post/decision-tree/","tags":["机器学习","algo"],"title":"决策树学习"},{"content":"目录 基本原理 逻辑回归的损失函数 逻辑回归的优化 随机梯度上升 分类评估方法 sklearn实现 总结 ps. 1. 基本原理 虽然叫回归但是其实解决的是二分类问题.逻辑回归的输入值为线性回归的输出值，即\n之所以能解决二分类问题，是因为逻辑回归将输入赋予sigmod函数，并设置阈值，可以达到分类的效果\nsigmod函数：\n逻辑回归的工作流程如下：\n2. 对数似然函数 区别于线性回归的平方损失函数，逻辑回归使用对数似然函数.\n也可以这么写（综合完整损失函数.）\n为什么要使用这个损失函数呢？试想：当样本值（y）为1而线性回归预估值比较小（比如h(θ)接近0）时，通过对数似然函数进行计算，会得到一个很大的值，给予模型最大的惩罚力度；当样本值（y）为1而线性回归预估值比较大（比如h(θ)接近1）时，计算得出0，不给模型惩罚.就很巧妙.\n3. 逻辑回归的优化 根据上面的综合完整损失函数，可以推导出损失函数的优化是这样的：要使求和的值最小，就要提升原本是类别1的概率，降低原本是类别0的概率.\n同样可以使用梯度下降（上升）进行优化.\n需要注意的是 《实战》中并没有具体推导对数似然函数的优化过程.以下是推导过程：\n于是，《实战》中梯度上升的代码就能被解释了.\n# 梯度上升迭代参数\rdef gradAscent(dataMatIn, classLabels):\rdataMatrix = mat(dataMatIn) #convert to NumPy matrix\rlabelMat = mat(classLabels).transpose() #转置.\rm,n = shape(dataMatrix)\ralpha = 0.001\rmaxCycles = 500\rweights = ones((n,1))\rfor k in range(maxCycles): #maxCycles 迭代次数\rh = sigmoid(dataMatrix*weights) #matrix mult\rerror = (labelMat - h) #vector subtraction\rweights = weights + alpha * dataMatrix.transpose()* error # 上面推导出的结果\rreturn weights\r # 画出最优曲线\rdef plotBestFit(weights):\rimport matplotlib.pyplot as plt\rdataMat,labelMat=loadDataSet()\rdataArr = array(dataMat)\rn = shape(dataArr)[0] xcord1 = []; ycord1 = []\rxcord2 = []; ycord2 = []\rfor i in range(n):\rif int(labelMat[i])== 1:\rxcord1.append(dataArr[i,1])\rycord1.append(dataArr[i,2])\relse:\rxcord2.append(dataArr[i,1])\rycord2.append(dataArr[i,2])\rfig = plt.figure()\rax = fig.add_subplot(111)\rax.scatter(xcord1, ycord1, s=30, c='blue', marker='s')\rax.scatter(xcord2, ycord2, s=30, c='green')\rx = arange(-3.0, 3.0, 0.1)\r# 这里的Y其实是X2. 须知y=xw，x0=0，而根据sigmod函数图像，当xw=0时可以分割两类曲线.因此0=xw，可以推导出x1和x2（这里的Y）的关系\ry = (-weights[0]-weights[1]*x)/weights[2]\rax.plot(x, y)\rplt.xlabel('X1'); plt.ylabel('X2');\rplt.show()\r 4. 随机的梯度上升  gradAscent中dataMatrix * weights是矩阵相乘，实际进行了300次，即需要遍历整个数据集，在数据集很大时效率不好.需要进行优化.\n使用随机梯度上升，每次只计算一个数据的梯度而非整个数据集，可以达到更快的速度.\n\r# 随机梯度上升迭代参数\rdef stocGradAscent0(dataMatrix, classLabels):\rm,n = shape(dataMatrix)\ralpha = 0.01\rweights = ones(n) #initialize to all ones\rfor i in range(m):\rh = sigmoid(sum(dataMatrix[i]*weights))\rerror = classLabels[i] - h\rweights = weights + alpha * error * dataMatrix[i]\rreturn weights\r 当存在非线性可分的点时，会导致出现较大的波动。另外，需要加快收敛速度.故而引入改进的随机梯度上升.\n\rdef stocGradAscent1(dataMatrix, classLabels, numIter=150):\rm,n = shape(dataMatrix)\rweights = ones(n) #initialize to all ones\rfor j in range(numIter):\rdataIndex = range(m)\rfor i in range(m):\ralpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant\rh = sigmoid(sum(dataMatrix[randIndex]*weights))\rerror = classLabels[randIndex] - h\rweights = weights + alpha * error * dataMatrix[randIndex]\rdel(dataIndex[randIndex])\rreturn weights\r alpha = 4/(1.0+j+i)+0.0001 是为了让α随迭代逐步减小到逼近0.（但是并不是严格单调递减）\n另外，区别于上个算法，这个算法使用随机的样本来更新权重系数，这样做的好处是能能够减少周期波动.\n5.分类评估方法  混淆矩阵：\n其中TP和TN是正确预测的.\n准确率(accuracy)：含义为所有样本中预测正确的占比 (TP+TN) / (TP+TN+FP+FN)\n精确率(precision)：含义为预测为正例的样本中真实为正的占比，代表查的准不准 (TP) / (TP+FP)\n召回率(recall)：含义为真实为正例中预测为正的占比，表示对正样本的区分能力，查的全不全 (TP) / (TP+FN)\nF1-score：含义为模型的稳健性 (2TP) / (2TP+FN+FP) = 2precisionrecall / recall+precision\n这些评估方法在sklearn中的实现如下图所示：\n真阳率TPR，即召回率：含义为检测出来的真阳性样本数除以所有真实阳性样本数 (TP) / (TP+FN)\n假阳率FPR：含义为检测出来的假阳性样本数除以所有真实阴性样本数 (FP) / (TN+FP)\nROC曲线：x轴为FPR，y轴为TPR.当预测所有样本为1时，曲线的结果为中间的虚线.\nAUC指标，即随机取一样本，正样本大于负样本的概率.范围在[0.5,1]之间.当为1时为完美分类器.这个指标一般用于不平衡二分类问题\n需要注意的是，在sklearn中使用AUC计算API时，必须0为反例1为正例（一般取样本多的为正例，比如本实验中，4代表恶性，样本多，取为1）.\nROC曲线的绘制过程如下：\n1.先将各个点的概率从高到低排序.\n2.挑选最高的，假设其为正例.如果其概率大于阈值，那么就是真正例，否则就是假正例.每次挑选，都要就是那个FPR和TPR\n3.迭代直到所有点都求出TPR和FPR.在轴上描点，结果就是ROC曲线.如下图所示，取阈值为0.75\nAUC其实就是ROC曲线的积分，也即面积.其表示分对的概率\n6. sklearn实现  sklearn-linearRegression\n下面是该api的一些常见参数\n其中penalty 有L1和L2，详见线性回归的博客；C代表正则化力度，C越大，惩罚越大；solver代表使用的梯度下降算法.\nimport pandas as pd\rimport numpy as np\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import roc_auc_score\r# 1.获取数据\r# 2.基本数据处理\r# 2.1 缺失值处理\r# 2.2 确定特征值,目标值\r# 2.3 分割数据\r# 3.特征工程(标准化)\r# 4.机器学习(逻辑回归)\r# 5.模型评估\r# 1.获取数据\rnames = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\r'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\r'Normal Nucleoli', 'Mitoses', 'Class']\rdata = pd.read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\u0026quot;,names=names)\r# 2.基本数据处理\r# 2.1 缺失值处理\rdata = data.replace(to_replace=\u0026quot;?\u0026quot;, value=np.nan)\rdata = data.dropna()\rx = data.iloc[:, 1:-1]\ry = data[\u0026quot;Class\u0026quot;]\r# 2.3 分割数据\rx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2, test_size=0.2)\r# 3.特征工程(标准化)\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(逻辑回归)\restimator = LogisticRegression()\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 基本评估\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是：\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是：\\n\u0026quot;, score)\r# 5.2 其他评估\rret = classification_report(y_test, y_pre, labels=(2,4), target_names=(\u0026quot;良性\u0026quot;, \u0026quot;恶性\u0026quot;))\rprint(ret)\r# 不平衡二分类问题评估方法\ry_test = np.where(y_test\u0026gt;3, 1, 0)\rroc_auc_score(y_true=y_test, y_score=y_pre)\r 7. 总结 本文主要介绍了逻辑回归的原理和源码，并使用梯度上升\u0026ndash;随机梯度上升来更新参数，求最优解.另外，本文介绍了一些分类评估的方法.最后，对于上述内容，本文介绍了sklearn API的详细实现过程.\n逻辑回归的优点：\n(1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;\n(2)简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响;\n(3)适合二分类问题，不需要缩放输入特征;\n(4)内存资源占用小，因为只需要存储各个维度的特征值;\n(5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题\n(6)以概率的形式输出，而非知识0.1判定，对许多利用概率辅助决策的任务很有用\n逻辑回归的缺点：\n(1)不能用逻辑回归去解决非线性问题，因为Logistic的决策面试线性的;\n(2)对多重共线性数据较为敏感;\n(3)很难处理数据不平衡的问题;\n(4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;\n(5)逻辑回归本身无法筛选特征，有时会用gbdt来筛选特征，然后再上逻辑回归。\n逻辑回归的适用场景：\n如是否患病，金融诈骗，虚假账号等二分类问题\n8. ps.  numpy中mat.getA 将矩阵转换为ndarray\nones((n, 1))，n代表几行，1代表每行有一个数.\n关于loc和iloc\n前者是根据标签索引进行查找，后者是根据整数索引进行查找.\n使用案例：\ndf.loc([['a','j'], ['name','score']]) #定位行索引值为a，j和列索引值为name和score的dataframe\rdf.iloc([[0,9],[0,2]]) #定位第0,9行，第0,2列的dataframe\r# 切片用法，loc前后都闭合，而iloc前闭后开\rdf.loc(['a':'j', 'name':'score']) #定位行索引值为a到j和列索引值为name到score的dataframe\rdf.iloc([0:9,0:2]) #定位第0到8行，第0到1列的dataframe\r drop的用法\n像本案例的情况 还是使用replace更好\n","date":"2022-06-03","permalink":"https://wudaore.github.io/post/logistic-regression/","tags":["机器学习","algo"],"title":"逻辑回归学习"},{"content":"目录 基本原理 线性回归的损失优化 局部加权线性回归 正则化线性模型 逐步向前回归 sklearn用法 总结 ps. 1. 基本原理 1.1 分类和回归 区别在于分类问题是定性的，输出离散值（如+1，-1）.而回归问题是定量的，输出连续值（如预测明天的气温为36.5度）\n1.2 基本原理 只有在线性可分的情况下才能使用.大概就是求一组参数w使得y能拟合wx\n2. 线性回归的损失优化 2.1 损失\u0026mdash;最小二乘法 最小，即最小化.二乘，即真实值-预测值的平法.普通最小二乘法就是要最小化这个平方值.\n由此，可以得到模型的损失函数\n2.2 优化\u0026mdash;正规方程 将上式写作矩阵形式.因为要求差值（j(θ)）的极值，所以两边对θ求导=0，解得θ的值\n最后解得w(即θ)的最优解\n特点：一蹴而就，一下子就能算出来，但是只适合样本和特征比较少的情况.\n2.3 优化\u0026mdash;梯度下降（更多用到） 梯度下降的公式如下：\n原理就是逐步降低梯度，更新数据，如此循环，知道梯度无限趋近于0，找到一定区间内的极小值.梯度下降无法保证找到最小值.\n其中参数α为步长，步长太大容易导致跳过极值点，太小导致计算缓慢.\n2.4 各种梯度下降算法 fg，全梯度下降 计算所有样本的误差平均值作为目标函数. 时间长，内存消耗大（一般不用）\nsag，随机平均梯度下降给每个样本都维持一个平均值.后期计算的时候参考这个平均值. 初期不佳，优化慢，因为该算法将初始梯度设为1，每轮梯度更新都结合上一轮.（首选）\nsg，随机梯度下降每次只选择一个样本 . 能快速将平均损失函数降到很低，但是必须注意步长，且无法代表整体样本（一般不用）\nmini-batch，小批量梯度下降选择一部分样本. 介于SG和FG之间（次选）\n2.5 正规方程和梯度下降两者对比 3. 局部加权线性回归LWLR  线性回归容易出现欠拟合.所以可以引入一些误差来降低均方误差.\n对待求点附近每个点加权（即赋予核），对于更近的点，将赋予更高的权重.\n对于权重W，《实战》中使用的是高斯核.显然，越近的点权重越大.\n公式中还有一个人为规定的参数k.k越大，越多的数据被用于训练.\n\rdef lwlr(testPoint,xArr,yArr,k=1.0):\rxMat = mat(xArr); yMat = mat(yArr).T\rm = shape(xMat)[0]\rweights = mat(eye((m)))\rfor j in range(m):\rdiffMat = testPoint - xMat[j,:]\rweights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))\rxTx = xMat.T * (weights * xMat)\rif linalg.det(xTx) == 0.0: #需要检测矩阵是否可逆.\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = xTx.I * (xMat.T * (weights * yMat))\rreturn testPoint * ws\rdef lwlrTest(testArr,xArr,yArr,k=1.0): #对于每个点，都要遍历xarr计算权重\rm = shape(testArr)[0]\ryHat = zeros(m)\rfor i in range(m):\ryHat[i] = lwlr(testArr[i],xArr,yArr,k)\rreturn yHat\r 4.正则化线性模型（岭回归和lasso回归）  4.0 欠拟合和过拟合. 欠：在训练集和测试集上表现都不包\n解决方法：\n1.添加其他特征\n2.添加多项式特征.\n过：在训练集上好 测试集上不好.\n解决方法：\n1.重新清洗数据\n2.增大训练量\n3.正则化 即尽量减少高次项的影响.分为L1正则化（直接使w的一些值为0，如LASSO回归）和L2正则化(使得一些高次项的w值接近0，削弱某个特征的影响，如ridge岭回归)\n4.减少特征维度，防止维度灾难\n4.1 岭回归（推荐） 线性回归和局部加权的线性回归都存在求逆的问题.而当矩阵不可逆时（或者输入线性相关，约等于不可逆），可以引入岭回归.岭回归不仅解决矩阵不可逆的情况，还引入惩罚项，防止过拟合.\n相较于线性回归，岭回归引入了惩罚项如下：\n经过与线性回归相似的推导（求导，使极值为0）,得到岭回归的w最优解：\n岭回归的具体推导过程\n\rdef rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays\rreturn ((yArr-yHatArr)**2).sum()\rdef ridgeRegres(xMat,yMat,lam=0.2):\rxTx = xMat.T*xMat\rdenom = xTx + eye(shape(xMat)[1])*lam\rif linalg.det(denom) == 0.0:\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = denom.I * (xMat.T*yMat)\rreturn ws\rdef ridgeTest(xArr,yArr):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #to eliminate X0 take mean off of Y\r#regularize X's\rxMeans = mean(xMat,0) #calc mean then subtract it off\rxVar = var(xMat,0) #calc variance of Xi then divide by it\rxMat = (xMat - xMeans)/xVar\rnumTestPts = 30\rwMat = zeros((numTestPts,shape(xMat)[1]))\rfor i in range(numTestPts):\rws = ridgeRegres(xMat,yMat,exp(i-10))\rwMat[i,:]=ws.T\rreturn wMat\r 4.2 lasso 相较于岭回归，lasso只是更改了惩罚项.\n岭回归通过增加平方项，并结合约束条件来约束高次项w的值（L2正则），顾其图线会是圆润的.而lasso回归通过增加绝对值项，并结合约束条件使得一些w项为0，表现在图线上就是不可导点，是尖锐的（在添加约束条件\u0026ndash;wk 的绝对值求和小于λ\u0026ndash;后，当λ足够小，会使一些系数被迫降到0.）.\n通过这种方法，能够自动进行特征选择.\nlasso可以更好的帮助分析数据（庞大特征数量下的特征选择），但是却大大增加了计算量且不太稳定.所以提出既能分析数据又能减少计算量的方法\u0026ndash;逐步向前回归\n4.3 Elastic Net 弹性网络 弹性网络通过混合比参数r控制岭回归和lasso回归.\n一般情况下优先使用岭回归，岭回归不适合的话才考虑弹性网络，最后才是lasso\n4.4 Early Stopping 当错误率达到阈值时，停止迭代.\n5.逐步向前回归  逐步向前回归的原理大概是:标准化数据后，经过若干次迭代，每次迭代改变一个系数得到新的w（分为增大和减小）.如新的w误差更小，则取新的w.\n算法共有两个可以调节的参数，即迭代次数和迭代步长.\ndef stageWise(xArr,yArr,eps=0.01,numIt=100):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #can also regularize ys but will get smaller coef\rxMat = regularize(xMat)\rm,n=shape(xMat)\rreturnMat = zeros((numIt,n)) #testing code remove\rws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()\rfor i in range(numIt):\rprint(ws.T)\rlowestError = inf; for j in range(n):\rfor sign in [-1,1]:\rwsTest = ws.copy()\rwsTest[j] += eps*sign\ryTest = xMat*wsTest\rrssE = rssError(yMat.A,yTest.A)\rif rssE \u0026lt; lowestError:\rlowestError = rssE\rwsMax = wsTest\rws = wsMax.copy()\rreturnMat[i,:]=ws.T\rreturn returnMat\r 6.sklearn用法  6.1 线性回归 可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n正规方程：\n梯度下降：\n具体案例使用如下（使用正规方程和梯度下降）：\n预测波士顿房价\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据\r2.数据基本处理\r2.1 数据集划分\r3.特征工程 --标准化\r4.机器学习(线性回归)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_boston\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, RidgeCV\rfrom sklearn.metrics import mean_squared_error\rdef linear_model1():\r\u0026quot;\u0026quot;\u0026quot;\r正规方程\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(线性回归)\restimator = LinearRegression()\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rdef linear_model2():\r\u0026quot;\u0026quot;\u0026quot;\r梯度下降法\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\restimator = SGDRegressor(max_iter=1000, learning_rate=\u0026quot;constant\u0026quot;, eta0=0.001)\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rif __name__ == '__main__':\rlinear_model1()\rlinear_model2()\rlinear_model3()\r 6.2 岭回归 岭回归是线性回归的正则化版本.在sklearn中，使用SGDRegressor，设置参数penalty=l2，也可以实现岭回归，但是只能实现普通的梯度下降.而使用 Ridge可以实现随机平均梯度下降SAG\n对于参数alpha，根据上面岭回归的公式，alpha越大，正则化力度越大，权重系数就越小.\n对于参数normalize，如果为true，就可以不必进行上面的标准化.\n可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n另外，该博客对参数和返回值做了一些中文解释，方便阅读和理解.点击跳转\n\rdef linear_model3():\r\u0026quot;\u0026quot;\u0026quot;\r岭回归\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\r# estimator = Ridge()\restimator = RidgeCV(alphas=(0.001, 0.1, 1, 10, 100))\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\r 6.3 lasso 官网\n注释参数博客\n7. 总结  从本文总结的算法来看，主要经过以下流程：线性回归(容易欠拟合)\u0026ndash;\u0026gt;加权线性回归(容易过拟合且无法处理不可逆矩阵)\u0026ndash;\u0026gt;岭回归\u0026ndash;(不好分析数据)\u0026ndash;\u0026gt;lasso(计算量大)\u0026ndash;\u0026gt;逐步向前回归.\n通过加权线性回归我们知道，越小的核越能拟合训练数据.当然，也越容易过拟合.而lasso和逐步向前回归允许我们参考输入向量中每一维数据的作用大小，从而可以对数据进行缩减.\n然而，上述算法终究只适合线性的数据.对于非线性的数据，需要使用其他算法.\n8.ps.  1.numpy.matrix numpy.matrix表示矩阵，使用numpy.matrix.A方法可以像数组一样根据下标取出值，不能直接用下标.\n官方文档\n2.numpy.eye() 生成对角阵.\n具体链接\n3.矩阵求导 在推导正规方程时使用了矩阵的求导公式，点击跳转\n","date":"2022-06-02","permalink":"https://wudaore.github.io/post/linear-regression/","tags":["机器学习","algo"],"title":"线性回归学习"},{"content":"Introduction 基本原理 各种距离 距离调参 KNN优化-KD树 数据处理（归一化和标准化） 实战\u0026ndash;鸢尾花数据集 总结 1. 基本原理 非常朴素的原理。根据距离最近的K个点来判断目标点的类别.\n2. 各种距离 标准距离\n汉明距离\n杰卡德距离\n马氏距离\n余弦距离\n3. 距离调参 p=1 曼哈顿距离 2 欧式距离 无穷 切比雪夫距离 三者都是闵式距离.\n4.KNN优化-KD树 4.1 KD树基本原理 根据KNN的基本原理可知，当需要预测一个点时，需要计算训练集中每个店到它的距离.当数据集很大时，对于N个样本，D个特征的数据集，算法的时间复杂度到达O(D*N^2)\nKD树的基本原理是通过树来分割数据，假设A和B的距离很远，B和C的距离很近，那么A和C的距离一定很远.通过这种方法，可以跳过很多距离远的点，减少计算成本.\n优化后算法的复杂度为O(DNlog(N))\n4.2 KD树的实现 假设有数据点{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}\n要根据X和Y的方差计算第一次分割的平面.X的方差更大所以选择首先分割X轴，选择其中间的(7,2),切割\n第二次根据Y进行分割平面.\n左：(2,3),(4,7),(5,4) \u0026ndash;\u0026gt;3,4,7\n右：(8,1),(9,6) \u0026ndash;\u0026gt;1,6\n对点(5,4)和(9,6)进行切割\n第三次数据集中的点已经很少了，直接进行切割.得到结果如下：\n4.3 KD树的最近邻查找 以上述数据集为例，现在要查找点(2.1,3.1)则根据x-y-x的顺序遍历构造好的KD树，发现经过以下路径：\n\u0026lt;(7,2),(5,4),(2,3)\u0026gt; 反向遍历该路径.\n先假设(2,3)为最近点，计算与其欧式距离为0.141.以(2.1,3.1)为圆心，以0.141这个距离为半径画圆，发现不能与Y=4相交.因为(5,4)点是根据Y分割的，所以没必要到该点的右平面去查找.\n同样，圆不能与x=7相，所以也没必要到(7,2)的右平面查找.可知最近点为(2,3)\n而以点(2,4.5)为例，得到查找路径 \u0026lt;(7,2),(5,4),(4,7)\u0026gt; .先假设(4,7)为最优解，并按照上述步骤画圆.(4,7)和(5,4)出队列后发现，该圆能将(5,4)包括进去.意味着此时的最优点已经发生了变化，半径也变成了3.04.所以需要到(5,4)的左平面查找.此时的查找队列为\u0026lt;(7,2),(2,3)\u0026gt;\n继续回溯队列到(2,3)，此时发现该点更近，更新为最优解.距离更新为1.5\n最后回溯至(7,2)发现圆无法包括，不需要找(7,2)的右平面.查找完毕.\n以上只是举了例子，更详细的原理：https://www.bilibili.com/video/BV19f4y1R7vh?spm_id_from=333.337.search-card.all.click\n5.数据处理 5.1 归一化 将数据映射到[0,1]区间内.公式如下图：\nsklearn提供了对应的API接口.具体实现代码如下：\n# # 1.归一化\r# # 1.1 实例化一个转换器\rtransfer = MinMaxScaler(feature_range=(0, 1))\r# # 1.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过归一化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 5.1 标准化 异常点对归一化的影响特别大，只适合传统精确小数，鲁棒性较差.\n标准化：通过转换将数据转换为均值为0，标准差为1的范围内，公式如下：\nsklearn提供了对应的API接口.具体实现代码如下：\n# 2.标准化\r# 2.1 实例化一个转换器\rtransfer = StandardScaler()\r# 2.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过标准化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 6.实战\u0026ndash;鸢尾花数据集(交叉验证，网格搜索) KNeighborsClassifier 可以使用algorithm参数指定使用的算法，包括ball树，暴力和kd树.具体见sklearn官网https://scikit-learn.org.cn/view/85.html\n交叉验证：将训练集分为训练集和验证集.交叉验证并不能提高准确率 但是能使模型更加可信 网格搜索：即将超参数(需要手动指定的参数)通过字典的形式传入，进行最优选择\nGridSearchCV的参数如下：https://scikit-learn.org.cn/view/655.html\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据集\r2.数据基本处理\r3.特征工程\r4.机器学习(模型训练)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_iris\rfrom sklearn.model_selection import train_test_split, GridSearchCV\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.neighbors import KNeighborsClassifier\r# 1.获取数据集\riris = load_iris()\r# 2.数据基本处理\r# 2.1 数据分割\rx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\r# 3.特征工程\r# 3.1 实例化一个转换器\rtransfer = StandardScaler()\r# 3.2 调用fit_transform方法\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(模型训练)\r# 4.1 实例化一个估计器\restimator = KNeighborsClassifier()\r# 4.2 调用交叉验证网格搜索模型\rparam_grid = {\u0026quot;n_neighbors\u0026quot;: [1, 3, 5, 7, 9]}\restimator = GridSearchCV(estimator, param_grid=param_grid, cv=10, n_jobs=-1)\r# 4.3 模型训练\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 输出预测值\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rprint(\u0026quot;预测值和真实值对比:\\n\u0026quot;, y_pre == y_test)\r# 5.2 输出准确率\rret = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, ret)\r# 5.3 其他评价指标\rprint(\u0026quot;最好的模型：\\n\u0026quot;, estimator.best_estimator_)\rprint(\u0026quot;最好的结果:\\n\u0026quot;, estimator.best_score_)\rprint(\u0026quot;整体模型结果:\\n\u0026quot;, estimator.cv_results_)\r 7.总结  k近邻的优点如下：\n1.简单有效\n2.重新训练代价低\n3.适合大样本自动分类\n该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n4.适合类域交叉样本\nKNN方法主要靠周围有限的邻近的样本,而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合\n缺点如下：\n1.惰性学习\nKNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多\n2.类别评分非规格化\n不像一些通过概率评分的分类\n3.输出的可解释性不强\n例如决策树的输出可解释性就较强\n4.不擅长不均衡样本\n当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。\n5.计算量太大\n","date":"2022-05-29","permalink":"https://wudaore.github.io/post/knn/","tags":["机器学习","algo"],"title":"KNN学习"},{"content":"Introduction 对matplotlib的简易学习\nimport matplotlib.pyplot as plt import random  matplotlib图像的绘制 # 1. 创建画布 # figsize 画布大小 dpi 像素 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 3.图像展示 plt.show()  # help(plt.figure)  图像保存 # 1. 创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 2.1 图像保存 plt.savefig(\u0026quot;./data/test.png\u0026quot;) # 3.图像展示 plt.show() # 图像保存一定要放到show前面 # # 2.1 图像保存 # plt.savefig(\u0026quot;./data/test.png\u0026quot;)  案例：显示温度变化状况 图像基本绘制功能演示 # 解决中文显示不正常 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False # 0.生成数据 x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图形绘制 plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] plt.yticks(y_ticks[::5]) plt.xticks(x[::5], x_ticks_labels[::5]) # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 # 第一个参数数字的功能大概是横坐标的实际个数 # 2.2 添加网格 plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # 2.3 添加描述 plt.xlabel(\u0026quot;时间\u0026quot;) plt.ylabel(\u0026quot;温度\u0026quot;) plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # 图例对应plt.label plt.legend(loc='best') # 3.图像展示 plt.show()  多个坐标系显示图像 # 0.生成数据 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] y_tianjin = [random.uniform(10, 19) for i in x] y_zhejiang = [random.uniform(12, 25) for i in x] # 1.创建画布 # plt.figure(figsize=(20, 8), dpi=100) # 多个坐标系 使用subplots 参数：几行几列 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 16), dpi=100) # 2.图形绘制 # plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) # plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[0][0].plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[0][1].plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[1][0].plot(x, y_tianjin, label=\u0026quot;天津\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[1][1].plot(x, y_zhejiang, label=\u0026quot;浙江\u0026quot;) # # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] # plt.yticks(y_ticks[::5]) # plt.xticks(x[::5], x_ticks_labels[::5]) # # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 axes[0][0].set_xticks(x[::5]) axes[0][0].set_yticks(y_ticks[::5]) axes[0][0].set_xticklabels(x_ticks_labels[::5]) axes[0][1].set_xticks(x[::5]) axes[0][1].set_yticks(y_ticks[::5]) axes[0][1].set_xticklabels(x_ticks_labels[::5]) # # 2.2 添加网格 # plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][0].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][1].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # # 2.3 添加描述 # plt.xlabel(\u0026quot;时间\u0026quot;) # plt.ylabel(\u0026quot;温度\u0026quot;) # plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) axes[0][0].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][0].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][0].set_title(\u0026quot;北京一小时温度变化图\u0026quot;, fontsize=20) axes[0][1].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][1].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][1].set_title(\u0026quot;上海一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # plt.legend(loc=0) axes[0][0].legend(loc=0) axes[0][1].legend(loc=0) # 3.图像展示 plt.show()  plot绘制数学图像 import numpy as np  # 0.生成数据 x = np.linspace(-10, 10, 1000) y = x*x*x # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.绘制 plt.plot(x, y) plt.grid() # 3.显示 plt.show()    ","date":"2022-05-24","permalink":"https://wudaore.github.io/post/hello_matplotlib/","tags":["机器学习"],"title":"matplotlib基础"},{"content":"Introduction \u0026laquo;实战\u0026raquo;中的svm数学推导实在是有些晦涩难懂.\nbilibili视频教程：https://www.bilibili.com/video/BV1Hs411w7ci?spm_id_from=333.337.search-card.all.click\n本文主要总结一部分svm的数学推导.后续再补全以及学习SVM的使用\n在开始之前，需要了解一些基本的概念\n1. 基础概念 1.1 wTx=b 以二维空间为例子，wTx 是指向量x在向量W上的投影长度乘以向量W的长度.wTx=b几何上来说所有乘积为b的向量x构成的一个平面.且该平面显然垂直于向量x.\n当出现空间中的另一个点a，想要计算该点到上述超平面的距离，可先将该点与x向量与w向量的起始点连接，形成x,向量.根据公式\n进行计算.详细推导见博客https://blog.csdn.net/Zelf0914/article/details/95811715\n理解之后不难发现，参数b的作用是控制超平面前后移动\nsvm \u0026mdash;间隔 对偶 核技巧\n2.硬间隔SVM 即最大间隔分类器.适用于数据线性可分.\n核心就是要找出一个超平面，使其对于“最近的点”的距离最大.（数据集中有很多点，对当前点的距离大了会导致和其他点的距离变小，这个时候其他点就变成“最近的点”了.)硬间隔SVM就是要求出对于“最近的点”的距离的最大值，即\n其中yi对应书中的label，当wTx\u0026gt;0时yi=1，反之为-1\n而|wTx+b|等价于yi（wTx+b） （因为yi只有可能是正负1且wTx+b时yi为+1，反之为-1.这样的转换是等价的.）\n替换后提出1/||w||（因为min内计算的是点到直线的距离，和向量w无关.||w||表示n维向量的模，称为范数），令min[yi（wTx+b）]=1（wTx+b 本质是一个向量，对其进行缩放不影响，就比如二维空间中的直线，所以设为1其实是设了个最小值）\n此时等式就变成了求max（1/||w||），等价于求min（1/2 wTw）(加1/2是为了方便求最小值？)\n推导过程见下图\n式子的目标函数是二次函数，限制条件是N个线性不等式条件，所以这是凸二次优化问题，可以用拉格朗日乘子法和KKT条件进行求解。拉格朗日乘子法可以寻找多元函数在一组约束条件下的极值，将d个变量与k个约束的最优化问题转变成d+k个变量的无约束问题。\n2.1 拉格朗日乘子法 经过上述推导，问题就变成了带有不等式约束条件的最优化问题（约束条件为yi（wTx+b）\u0026gt;=1）.\n借助拉格朗日乘子 将带约束问题转换成无约束问题（考研数学做过类似的题型，也可参考博客https://blog.csdn.net/lijil168/article/details/69395023），\n同时，1/2wTw 又等价于maxλL(w，b，λ)，故而可以如下进行转换，如下图：\n该问题的目标函数是二次的，约束又是线性的，故而满足强对偶关系.故而进行以下转换：\nL先对b求偏导，令结果=0，并带回L原式：\n继续对w求偏导.因为wTw=wwT，有以下转换：\n此时的w已经是最优解（？）\n最终得到如下转换\n2.2 KKT条件 满足强对偶关系（如上述，目标函数是二次且约束是线性）的充要条件是满足KKt条件.\nKKT条件：\n可知w和b都是关于x和y的组合.同时，λi只在支持向量上有意义，其他时候都是0\n支持向量即分类结果中平行于分类平面的过最近点的两个向量（不知道这个理解对不对），只有在支持向量上，wwT+b的绝对值才等于1，λi才有意义.\n软间隔SVM soft 指允许一点点误差.只需要在原目标函数后加一个误差函数loss即可：min（1/2 wTw）+loss\nloss如果使用yi(wwT+b)的点的个数的话会导致结果离散，不好求导.所以一般使用距离。即loss = max{0, 1-yi(wwT+b)},这就是hinge loss\n引入ξ表示1-yi(wwT+b)，即误差的距离.调整约束条件和目标函数，的软间隔svm模型：\n写在最后 非数学系，数学功底不好，做这个东西真的困难.还是了解一下，不做深入证明（如强弱对偶性的证明，太头疼），更偏向代码和运用一些.\n","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm1/","tags":["机器学习","algo"],"title":"svm学习(1)--基础概念和公式"},{"content":"Introduction SMO算法 即序列最小优化算法 对\u0026laquo;实战\u0026raquo;中的smoSimple函数尝试理解\n \rdef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\r\u0026quot;\u0026quot;\u0026quot;smoSimple\rArgs:\rdataMatIn 特征集合\rclassLabels 类别标签\rC 松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。\r控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。\r可以通过调节该参数达到不同的结果。\r其实就是上一篇博客中提到的loss值\rtoler 容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）\rmaxIter 退出前最大的循环次数\rReturns:\rb 模型的常量值\ralphas 拉格朗日乘子\r\u0026quot;\u0026quot;\u0026quot;\rdataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\rb = 0; m,n = shape(dataMatrix)\r# alpha 初始全为0\ralphas = mat(zeros((m,1)))\rprint(len(alphas))\rprint(len(dataMatrix))\riter = 0\rwhile (iter \u0026lt; maxIter):\r# 记录alpha是否已经优化\ralphaPairsChanged = 0\rfor i in range(m):\r# 预测的类别\r# 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*label[n]*x[n]\r# 理解了半天 被这个括号害死了\rfXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\r# 预测结果与真是结果比对 得到误差\rEi = fXi - float(labelMat[i])#if checks if an example violates KKT conditions\r# 如果误差大的话，就要进行优化.选择另一个向量j\rif ((labelMat[i]*Ei \u0026lt; -toler) and (alphas[i] \u0026lt; C)) or ((labelMat[i]*Ei \u0026gt; toler) and (alphas[i] \u0026gt; 0)):\rj = selectJrand(i,m)\rfXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\rEj = fXj - float(labelMat[j])\ralphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\r# L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句\r# labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。\rif (labelMat[i] != labelMat[j]):\rL = max(0, alphas[j] - alphas[i])\rH = min(C, C + alphas[j] - alphas[i])\relse:\rL = max(0, alphas[j] + alphas[i] - C)\rH = min(C, alphas[j] + alphas[i])\rif L==H: print (\u0026quot;L==H\u0026quot;); continue\r# eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程\r# 参考《统计学习方法》李航-P125~P128\u0026lt;序列最小最优化算法\u0026gt;\reta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\rif eta \u0026gt;= 0: print (\u0026quot;eta\u0026gt;=0\u0026quot;); continue\r# 计算出一个新的alphas[j]值\ralphas[j] -= labelMat[j]*(Ei - Ej)/eta\r# 并使用辅助函数，以及L和H对其进行调整\ralphas[j] = clipAlpha(alphas[j],H,L)\rif (abs(alphas[j] - alphaJold) \u0026lt; 0.00001): print(\u0026quot;j not moving enough\u0026quot;); continue\r# 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反\ralphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\r# 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。\r# w= Σ[1~n] ai*yi*xi =\u0026gt; b = yj- Σ[1~n] ai*yi(xi*xj)\r# 所以： b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)\r# 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍\rb1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\rb2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\rif (0 \u0026lt; alphas[i]) and (C \u0026gt; alphas[i]): b = b1\relif (0 \u0026lt; alphas[j]) and (C \u0026gt; alphas[j]): b = b2\relse:\rb = (b1 + b2)/2.0\ralphaPairsChanged += 1\rprint(\u0026quot;iter: %d i:%d, pairs changed %d\u0026quot; % (iter, i, alphaPairsChanged))\r# 在for循环外，检查alpha值是否做了更新，如果更新则将iter设为0后继续运行程序\r# 直到更新完毕后，iter次循环无变化，才退出循环。\rif (alphaPairsChanged == 0):\riter += 1\relse:\riter = 0\rprint(\u0026quot;iteration number: %d\u0026quot; % iter)\rreturn b, alphas\r ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm2/","tags":["机器学习","algo"],"title":"svm学习(2)--简易SMO算法代码理解"},{"content":"Introduction sklearn的SVM算法可以调整c值和核函数.本篇并未介绍核函数\n 导入数据和数据-标签的处理\nfrom sklearn.svm import SVC\rfrom sklearn import datasets\riris = datasets.load_iris()\rX = iris['data']\rX = iris['data'][:,(2,3)]\ry = iris['target']\r SVM训练.c值设置为无穷大代表几乎不容错 使用线性核函数\nsetosa_or_versicolor = (y==0)|(y==1)\rX = X[setosa_or_versicolor]\ry = y[setosa_or_versicolor]\rsvm_clf = SVC(kernel='linear', C=float('inf'))\rsvm_clf.fit(X,y)\rw = svm_clf.coef_\rb = svm_clf.intercept_\rw\r 使用matplotlib绘图\nimport numpy as np\rimport matplotlib\rimport os\rimport matplotlib.pyplot as plt\rimport math\r%matplotlib inline\rplt.rcParams['axes.labelsize']=14\rplt.rcParams['xtick.labelsize']=12\rplt.rcParams['ytick.labelsize']=12\rx0=np.linspace(0, 5.5, 200) pred_1=5*x0-20 pred_2=x0-1.8 pred_3=0.1*x0+0.5 def plot_svc_decision_boundary(svm_clf,xmin,xmax,sv=True):\rw= svm_clf.coef_[0]\rb=svm_clf.intercept_[0]\rprint(w)\rx0=np.linspace(xmin,xmax,200)\rdecision_boundary=-w[0]/w[1] *x0-b/w[1]\r#这里的margin算的并不是1/||w||,而是1/w[1]，是为了方便绘制支持向量\r#margin = 1 / np.sqrt(np.sum(svm_clf.coef_**2))\rmargin=1/w[1]\rprint(margin)\rgutter_up=decision_boundary+margin\rgutter_down=decision_boundary-margin\rif sv:\rsvs = svm_clf.support_vectors_\rplt.scatter(svs[:,0], svs[:,1], s=180, facecolors='#FFAAAA')\rplt.plot(x0, decision_boundary, 'k-', linewidth=2)\rplt.plot(x0, gutter_up, 'k-', linewidth=2)\rplt.plot(x0, gutter_down, 'k-', linewidth=2)\rplt.figure(figsize=(14,4))\rplt.subplot(121)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.plot(x0,pred_1,'g--',linewidth=2)\rplt.plot(x0,pred_2,'m-',linewidth=2)\rplt.plot(x0,pred_3,'r-',linewidth=2)\rplt.axis([0,5.5,0,2])\rplt.subplot(122)\rplot_svc_decision_boundary(svm_clf, 0, 5.5)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.axis([0,5.5,0,2])\r 结果如下： ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm3/","tags":["机器学习","algo"],"title":"svm学习(3)--SVM实现鸢尾花数据分类"},{"content":"Introduction 系统的学习Linux对日常学习和之后的工作都有帮助\n本篇博客记录学习ubuntu文件系统时遇到的问题和需要记录的点\n1.ubuntu 常见目录 ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/ubuntu-filesys/","tags":["linux"],"title":"ubuntu学习(1)--文件系统"},{"content":"Introduction 本文主要介绍如何在windows下使用hugo搭建自己的博客并通过github挂载，同时绑定自己的域名\nbilibili视频教程：https://www.bilibili.com/video/BV13c411h7k7?spm_id_from=333.999.0.0\n本文主要总结一些踩的坑以及功能拓展\nstep1 安装配置hugo hugo中文文档 ：https://www.gohugo.org/\nhugo编译文件下载地址：https://github.com/gohugoio/hugo/releases\n解压后放入bin文件夹内，将bin添加到系统变量，如下图所示\n打开cmd输入hugo version，若能显示版本，则配置成功\nstep2 创建博客 2.1创建项目文件 在cmd中输入以下指令创建项目文件.\nhugo new site yourname.com  2.2选择博客样式 在https://themes.gohugo.io/中选择下载博客的样式.本博客使用fuji模板，https://themes.gohugo.io/themes/hugo-theme-fuji/\n样式下载方法：直接git clone到项目文件夹下的themes中（ssh），或下载压缩包解压到themes中（http），如下图所示\n下载后需要在项目文件夹下的config.toml中修改样式名.同时可以修改博客名等字段\n注意项目名为应该与模板中的文件夹名相同，如下图所示，项目名就应该是hermit.\n进入hermit（以hermit模板为例）下的ExampleSite文件夹中，复制content和config.toml粘贴到项目文件夹xxx.com下.其中content为博客模板，是markdown文件，config.toml是配置项.\n导入完毕后在项目文件夹xxx.com下的cmd中执行命令.\nhugo  即可编译项目.\nstep3 通过github挂载 3.1建立个人仓库 注册github账号后点击头像-Your Repositories即可新建/管理仓库\n个人仓库名必须是github用户名+github.io，否则会报错.readme文档暂时不添加，防止冲突\n作者使用的是master分支而非main，这需要进入setting-branch中进行配置.还需要使用git push origin -d main 删除main分支\n建立仓库后通过git-scm.com下载最新版git，同样需要将git的bin文件加入系统变量\n3.2本地代码同步到git 首先在项目文件夹.com下的cmd中输入hugo编译项目.编译完成后出现public文件夹，即编译后的文件.\n进入public，右键Git Bash Here进入Git命令行.相继输入\ngit init git add -A git commit -m\u0026quot;init\u0026quot; git remote add origin https://github.com/你的用户名/你的用户名.github.io.git git push -f origin master  如果是第一次使用git，还需要进行验证（如果是http方式则会跳出窗口，如果是ssh方式则需要设置密钥公钥，详见博客https://www.xuanfengge.com/using-ssh-key-link-github-photo-tour.html）\npush完毕后打开github仓库，就能发现public中的内容已经被存到仓库中了\n此时浏览器访问用户名.github.io即可进入博客.\nstep4 关联到域名 4.1域名解析 本文使用的是腾讯云.进入控制台选择域名解析，添加以下配置：\n4.2github中的配置 进入项目仓库-settings-page，在Custom domain中配置自己的域名，等待解析完成即可.此页面也可配置默认分支为master，就不需要每次都修改.\n设置完毕后就可以通过自己的域名访问博客.\nstep5 新增，删除或者修改博客 所有的博客都保存在content/posts中（当然也可改名为blogs等，无伤大雅）.修改完毕后需要回退到项目文件夹.com下，cmd执行hugo命令重新编译，并进入public文件夹输入以下代码上传\ngit add -A git commit -m\u0026quot;edit\u0026quot; git push  step6 补充 有些博客模板在本地运行没有问题，但是绑定域名后出错，需要仔细筛选.\nHugo官网：https://gohugo.io/content-management 要学会查找官方文档，解决开发中遇到的问题.\n","date":"2022-05-12","permalink":"https://wudaore.github.io/post/creating-a-boke/","tags":["test","others"],"title":"windows下使用hugo搭建博客"},{"content":"\r\rThis post is for in-post APlayer test, above is previous post-player.\nMusic files are all downloaded from Free Music Archive.\nSingle file \r\rMultiple files You can open the playlist to check other musics.\n\r\rSpaces between multiple items can be omited.\n","date":"2021-01-10","permalink":"https://wudaore.github.io/post/aplayer-test/","tags":["test"],"title":"In-post APlayer Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot; /\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item   First Sub-item Second Sub-item  Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-05-11","permalink":"https://wudaore.github.io/post/markdown-syntax/","tags":["test"],"title":"Markdown Syntax"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }}\r{{ partial \u0026quot;math.html\u0026quot; . }}\r{{ end }}\r  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files.  Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $ \\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887… $\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2020-03-08","permalink":"https://wudaore.github.io/post/math-typesetting/","tags":null,"title":"Math Typesetting"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09","permalink":"https://wudaore.github.io/post/placeholder-text/","tags":["markdown","text"],"title":"Placeholder Text"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n:s ee_no_evil:[Remove the space] 🙈 :h ear_no_evil:[Remove the space] 🙉 :s peak_no_evil:[Remove the space] 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji {\rfont-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;\r}\r","date":"2019-03-05","permalink":"https://wudaore.github.io/post/emoji-support/","tags":["test"],"title":"Emoji Support"},{"content":"The following is part of the CJK text, this page is for test use only.\nCJK Radicals Supplement ⺀ ⺁ ⺂ ⺃ ⺄ ⺅ ⺆ ⺇ ⺈ ⺉ ⺊ ⺋ ⺌ ⺍ ⺎ ⺏ ⺐ ⺑ ⺒ ⺓ ⺔ ⺕ ⺖ ⺗ ⺘ ⺙ ⺛ ⺜ ⺝ ⺞ ⺟ ⺠ ⺡ ⺢ ⺣ ⺤ ⺥ ⺦ ⺧ ⺨ ⺩ ⺪ ⺫ ⺬ ⺭ ⺮ ⺯ ⺰ ⺱ ⺲ ⺳ ⺴ ⺵ ⺶ ⺷ ⺸ ⺹ ⺺ ⺻ ⺼ ⺽ ⺾ ⺿ ⻀ ⻁ ⻂ ⻃ ⻄ ⻅ ⻆ ⻇ ⻈ ⻉ ⻊ ⻋ ⻌ ⻍ ⻎ ⻏ ⻐ ⻑ ⻒ ⻓ ⻔ ⻕ ⻖ ⻗ ⻘ ⻙ ⻚ ⻛ ⻜ ⻝ ⻞ ⻟ ⻠ ⻡ ⻢ ⻣ ⻤ ⻥ ⻦ ⻧ ⻨ ⻩ ⻪ ⻫ ⻬ ⻭ ⻮ ⻯ ⻰ ⻱ ⻲ ⻳\nKangxi Radicals ⼀ ⼁ ⼂ ⼃ ⼄ ⼅ ⼆ ⼇ ⼈ ⼉ ⼊ ⼋ ⼌ ⼍ ⼎ ⼏ ⼐ ⼑ ⼒ ⼓ ⼔ ⼕ ⼖ ⼗ ⼘ ⼙ ⼚ ⼛ ⼜ ⼝ ⼞ ⼟ ⼠ ⼡ ⼢ ⼣ ⼤ ⼥ ⼦ ⼧ ⼨ ⼩ ⼪ ⼫ ⼬ ⼭ ⼮ ⼯ ⼰ ⼱ ⼲ ⼳ ⼴ ⼵ ⼶ ⼷ ⼸ ⼹ ⼺ ⼻ ⼼ ⼽ ⼾ ⼿ ⽀ ⽁ ⽂ ⽃ ⽄ ⽅ ⽆ ⽇ ⽈ ⽉ ⽊ ⽋ ⽌ ⽍ ⽎ ⽏ ⽐ ⽑ ⽒ ⽓ ⽔ ⽕ ⽖ ⽗ ⽘ ⽙ ⽚ ⽛ ⽜ ⽝ ⽞ ⽟ ⽠ ⽡ ⽢ ⽣ ⽤ ⽥ ⽦ ⽧ ⽨ ⽩ ⽪ ⽫ ⽬ ⽭ ⽮ ⽯ ⽰ ⽱ ⽲ ⽳ ⽴ ⽵ ⽶ ⽷ ⽸ ⽹ ⽺ ⽻ ⽼ ⽽ ⽾ ⽿ \u0026hellip;\nCJK Symbols and Punctuation 、 。 〃 〄 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〓 〔 〕 〖 〗 〘 〙 〚 〛 〜 〝 〞 〟 〠 〡 〢 〣 〤 〥 〦 〧 〨 〩 〪 〫 〬 〭 〮 〯 〰 〱 〲 〳 〴 〵 〶 〷 〸 〹 〺 〻 〼 〽 〾 〿\nHiragana ぁ あ ぃ い ぅ う ぇ え ぉ お か が き ぎ く ぐ け げ こ ご さ ざ し じ す ず せ ぜ そ ぞ た だ ち ぢ っ つ づ て で と ど な に ぬ ね の は ば ぱ ひ び ぴ ふ ぶ ぷ へ べ ぺ ほ ぼ ぽ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ ゎ わ ゐ ゑ を ん ゔ ゕ ゖ ゙ ゚ ゛ ゜ ゝ ゞ ゟ\nKatakana ゠ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ ガ キ ギ ク グ ケ ゲ コ ゴ サ ザ シ ジ ス ズ セ ゼ ソ ゾ タ ダ チ ヂ ッ ツ ヅ テ デ ト ド ナ ニ ヌ ネ ノ ハ バ パ ヒ ビ ピ フ ブ プ ヘ ベ ペ ホ ボ ポ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ヮ ワ ヰ ヱ ヲ ン ヴ ヵ ヶ ヷ ヸ ヹ ヺ ・ ー ヽ ヾ ヿ\nBopomofo ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ ㄚ ㄛ ㄜ ㄝ ㄞ ㄟ ㄠ ㄡ ㄢ ㄣ ㄤ ㄥ ㄦ ㄧ ㄨ ㄩ ㄪ ㄫ ㄬ\nHangul Compatibility Jamo ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄸ ㄹ ㄺ ㄻ ㄼ ㄽ ㄾ ㄿ ㅀ ㅁ ㅂ ㅃ ㅄ ㅅ ㅆ ㅇ ㅈ ㅉ ㅊ ㅋ ㅌ ㅍ ㅎ ㅏ ㅐ ㅑ ㅒ ㅓ ㅔ ㅕ ㅖ ㅗ ㅘ ㅙ ㅚ ㅛ ㅜ ㅝ ㅞ ㅟ ㅠ ㅡ ㅢ ㅣ ㅤ ㅥ ㅦ ㅧ ㅨ ㅩ ㅪ ㅫ ㅬ ㅭ ㅮ ㅯ ㅰ ㅱ ㅲ ㅳ ㅴ ㅵ ㅶ ㅷ ㅸ ㅹ ㅺ ㅻ ㅼ ㅽ ㅾ ㅿ ㆀ ㆁ ㆂ ㆃ ㆄ ㆅ ㆆ ㆇ ㆈ ㆉ ㆊ ㆋ ㆌ ㆍ ㆎ\nKanbun ㆐ ㆑ ㆒ ㆓ ㆔ ㆕ ㆖ ㆗ ㆘ ㆙ ㆚ ㆛ ㆜ ㆝ ㆞ ㆟\nBopomofo Extended ㆠ ㆡ ㆢ ㆣ ㆤ ㆥ ㆦ ㆧ ㆨ ㆩ ㆪ ㆫ ㆬ ㆭ ㆮ ㆯ ㆰ ㆱ ㆲ ㆳ ㆴ ㆵ ㆶ ㆷ\nKatakana Phonetic Extensions ㇰ ㇱ ㇲ ㇳ ㇴ ㇵ ㇶ ㇷ ㇸ ㇹ ㇺ ㇻ ㇼ ㇽ ㇾ ㇿ\nEnclosed CJK Letters and Months ㈀ ㈁ ㈂ ㈃ ㈄ ㈅ ㈆ ㈇ ㈈ ㈉ ㈊ ㈋ ㈌ ㈍ ㈎ ㈏ ㈐ ㈑ ㈒ ㈓ ㈔ ㈕ ㈖ ㈗ ㈘ ㈙ ㈚ ㈛ ㈜ ㈠ ㈡ ㈢ ㈣ ㈤ ㈥ ㈦ ㈧ ㈨ ㈩ ㈪ ㈫ ㈬ ㈭ ㈮ ㈯ ㈰ ㈱ ㈲ ㈳ ㈴ ㈵ ㈶ ㈷ ㈸ ㈹ ㈺ ㈻ ㈼ ㈽ ㈾ ㈿ ㉀ ㉁ ㉂ ㉃ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㉠ ㉡ ㉢ ㉣ ㉤ ㉥ ㉦ ㉧ ㉨ ㉩ ㉪ ㉫ ㉬ ㉭ ㉮ ㉯ ㉰ ㉱ ㉲ ㉳ ㉴ ㉵ ㉶ ㉷ ㉸ ㉹ ㉺ ㉻ ㉿ ㊀ ㊁ ㊂ ㊃ ㊄ ㊅ ㊆ ㊇ ㊈ ㊉ ㊊ ㊋ ㊌ ㊍ ㊎ ㊏ ㊐ ㊑ ㊒ \u0026hellip;\nCJK Compatibility ㌀ ㌁ ㌂ ㌃ ㌄ ㌅ ㌆ ㌇ ㌈ ㌉ ㌊ ㌋ ㌌ ㌍ ㌎ ㌏ ㌐ ㌑ ㌒ ㌓ ㌔ ㌕ ㌖ ㌗ ㌘ ㌙ ㌚ ㌛ ㌜ ㌝ ㌞ ㌟ ㌠ ㌡ ㌢ ㌣ ㌤ ㌥ ㌦ ㌧ ㌨ ㌩ ㌪ ㌫ ㌬ ㌭ ㌮ ㌯ ㌰ ㌱ ㌲ ㌳ ㌴ ㌵ ㌶ ㌷ ㌸ ㌹ ㌺ ㌻ ㌼ ㌽ ㌾ ㌿ ㍀ ㍁ ㍂ ㍃ ㍄ ㍅ ㍆ ㍇ ㍈ ㍉ ㍊ ㍋ ㍌ ㍍ ㍎ ㍏ ㍐ ㍑ ㍒ ㍓ ㍔ ㍕ ㍖ ㍗ ㍘ ㍙ ㍚ ㍛ ㍜ ㍝ ㍞ ㍟ ㍠ ㍡ ㍢ ㍣ ㍤ ㍥ ㍦ ㍧ ㍨ ㍩ ㍪ ㍫ ㍬ ㍭ ㍮ ㍯ ㍰ ㍱ ㍲ ㍳ ㍴ ㍵ ㍶ ㍻ ㍼ ㍽ ㍾ ㍿ ㎀ ㎁ ㎂ ㎃ \u0026hellip;\nCJK Unified Ideographs Extension A 㐀 㐁 㐂 㐃 㐄 㐅 㐆 㐇 㐈 㐉 㐊 㐋 㐌 㐍 㐎 㐏 㐐 㐑 㐒 㐓 㐔 㐕 㐖 㐗 㐘 㐙 㐚 㐛 㐜 㐝 㐞 㐟 㐠 㐡 㐢 㐣 㐤 㐥 㐦 㐧 㐨 㐩 㐪 㐫 㐬 㐭 㐮 㐯 㐰 㐱 㐲 㐳 㐴 㐵 㐶 㐷 㐸 㐹 㐺 㐻 㐼 㐽 㐾 㐿 㑀 㑁 㑂 㑃 㑄 㑅 㑆 㑇 㑈 㑉 㑊 㑋 㑌 㑍 㑎 㑏 㑐 㑑 㑒 㑓 㑔 㑕 㑖 㑗 㑘 㑙 㑚 㑛 㑜 㑝 㑞 㑟 㑠 㑡 㑢 㑣 㑤 㑥 㑦 㑧 㑨 㑩 㑪 㑫 㑬 㑭 㑮 㑯 㑰 㑱 㑲 㑳 㑴 㑵 㑶 㑷 㑸 㑹 㑺 㑻 㑼 㑽 㑾 㑿 \u0026hellip;\nCJK Unified Ideographs 一 丁 丂 七 丄 丅 丆 万 丈 三 上 下 丌 不 与 丏 丐 丑 丒 专 且 丕 世 丗 丘 丙 业 丛 东 丝 丞 丟 丠 両 丢 丣 两 严 並 丧 丨 丩 个 丫 丬 中 丮 丯 丰 丱 串 丳 临 丵 丶 丷 丸 丹 为 主 丼 丽 举 丿 乀 乁 乂 乃 乄 久 乆 乇 么 义 乊 之 乌 乍 乎 乏 乐 乑 乒 乓 乔 乕 乖 乗 乘 乙 乚 乛 乜 九 乞 也 习 乡 乢 乣 乤 乥 书 乧 乨 乩 乪 乫 乬 乭 乮 乯 买 乱 乲 乳 乴 乵 乶 乷 乸 乹 乺 乻 乼 乽 乾 乿 \u0026hellip;\nHangul Syllables 가 각 갂 갃 간 갅 갆 갇 갈 갉 갊 갋 갌 갍 갎 갏 감 갑 값 갓 갔 강 갖 갗 갘 같 갚 갛 개 객 갞 갟 갠 갡 갢 갣 갤 갥 갦 갧 갨 갩 갪 갫 갬 갭 갮 갯 갰 갱 갲 갳 갴 갵 갶 갷 갸 갹 갺 갻 갼 갽 갾 갿 걀 걁 걂 걃 걄 걅 걆 걇 걈 걉 걊 걋 걌 걍 걎 걏 걐 걑 걒 걓 걔 걕 걖 걗 걘 걙 걚 걛 걜 걝 걞 걟 걠 걡 걢 걣 걤 걥 걦 걧 걨 걩 걪 걫 걬 걭 걮 걯 거 걱 걲 걳 건 걵 걶 걷 걸 걹 걺 걻 걼 걽 걾 걿 \u0026hellip;\nCJK Compatibility Ideographs 豈 更 車 賈 滑 串 句 龜 龜 契 金 喇 奈 懶 癩 羅 蘿 螺 裸 邏 樂 洛 烙 珞 落 酪 駱 亂 卵 欄 爛 蘭 鸞 嵐 濫 藍 襤 拉 臘 蠟 廊 朗 浪 狼 郎 來 冷 勞 擄 櫓 爐 盧 老 蘆 虜 路 露 魯 鷺 碌 祿 綠 菉 錄 鹿 論 壟 弄 籠 聾 牢 磊 賂 雷 壘 屢 樓 淚 漏 累 縷 陋 勒 肋 凜 凌 稜 綾 菱 陵 讀 拏 樂 諾 丹 寧 怒 率 異 北 磻 便 復 不 泌 數 索 參 塞 省 葉 說 殺 辰 沈 拾 若 掠 略 亮 兩 凉 梁 糧 良 諒 量 勵 \u0026hellip;\nCJK Compatibility Forms ︰ ︱ ︲ ︳ ︴ ︵ ︶ ︷ ︸ ︹ ︺ ︻ ︼ ︽ ︾ ︿ ﹀ ﹁ ﹂ ﹃ ﹄ ﹅ ﹆ ﹉ ﹊ ﹋ ﹌ ﹍ ﹎ ﹏\n","date":"2018-03-09","permalink":"https://wudaore.github.io/post/cjk-unicode-test/","tags":["test"],"title":"CJK Unicode Test"},{"content":"本文内容无实际意义，由狗屁不通文章生成器自动生成，不代表作者本人观点。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，到底应该如何实现。马克思在不经意间这样说过，一切节省，归根到底都归结为时间的节省。这不禁令我深思。马克思曾经说过，一切节省，归根到底都归结为时间的节省。\n带着这句话，我们还要更加慎重的审视这个问题：对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如何，我们不得不面对一个非常尴尬的事实，那就是，亚伯拉罕·林肯曾经提到过，你活了多少岁不算什么，重要的是你是如何度过这些岁月的。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我认为，在这种困难的抉择下，本人思来想去，寝食难安。所谓一段代码写一天，关键是一段代码写一天需要如何写。裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。总结的来说，带着这些问题，我们来审视一下一段代码写一天。西班牙曾经说过，自知之明是最难得的知识。这不禁令我深思。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。贝多芬在不经意间这样说过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。\n每个人都不得不面对这些问题。在面对这种问题时，歌德曾经提到过，读一本好书，就如同和一个高尚的人在交谈。这似乎解答了我的疑惑。歌德在不经意间这样说过，读一本好书，就如同和一个高尚的人在交谈。我希望诸位也能好好地体会这句话。从这个角度来看，一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。现在，解决一段代码写一天的问题，是非常非常重要的。所以，马克思曾经提到过，一切节省，归根到底都归结为时间的节省。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。我希望诸位也能好好地体会这句话。既然如此，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。白哲特曾经说过，坚强的信念能赢得强者的心，并使他们变得更坚强。带着这句话，我们还要更加慎重的审视这个问题：富勒在不经意间这样说过，苦难磨炼一些人，也毁灭另一些人。带着这句话，我们还要更加慎重的审视这个问题：这样看来，一般来讲，我们都必须务必慎重的考虑考虑。从这个角度来看，从这个角度来看，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。而这些并不是完全重要，更加重要的问题是。\n带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。经过上述讨论，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。佚名曾经提到过，感激每一个新的挑战，因为它会锻造你的意志和品格。这句话语虽然很短，但令我浮想联翩。现在，解决一段代码写一天的问题，是非常非常重要的。所以，每个人都不得不面对这些问题。在面对这种问题时，我们都知道，只要有意义，那么就必须慎重考虑。经过上述讨论。\n了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一般来说，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。笛卡儿在不经意间这样说过，阅读一切好书如同和过去最杰出的人谈话。我希望诸位也能好好地体会这句话。在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。莎士比亚曾经说过，抛弃时间的人，时间也抛弃他。我希望诸位也能好好地体会这句话。\n笛卡儿说过一句富有哲理的话，阅读一切好书如同和过去最杰出的人谈话。这句话语虽然很短，但令我浮想联翩。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。一段代码写一天因何而发生？一段代码写一天因何而发生？我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。邓拓曾经说过，越是没有本领的就越加自命不凡。这启发了我，从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说。\n对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。总结的来说，从这个角度来看，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。德谟克利特说过一句富有哲理的话，节制使快乐增加并使享受加强。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们都知道，只要有意义，那么就必须慎重考虑。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。史美尔斯曾经提到过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这句话语虽然很短，但令我浮想联翩。一般来讲，我们都必须务必慎重的考虑考虑。既然如此，我们都知道，只要有意义，那么就必须慎重考虑。这样看来，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。德国曾经提到过，只有在人群中间，才能认识自己。我希望诸位也能好好地体会这句话。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，从这个角度来看，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。莎士比亚在不经意间这样说过，意志命运往往背道而驰，决心到最后会全部推倒。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。那么，一段代码写一天，到底应该如何实现。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，发生了会如何，不发生又会如何。在这种困难的抉择下，本人思来想去，寝食难安。塞涅卡在不经意间这样说过，生命如同寓言，其价值不在与长短，而在与内容。这不禁令我深思。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。所谓一段代码写一天，关键是一段代码写一天需要如何写。我认为，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。在这种困难的抉择下，本人思来想去，寝食难安。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，到底应该如何实现。一段代码写一天，发生了会如何，不发生又会如何。既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。总结的来说，那么，带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天，发生了会如何，不发生又会如何。西班牙曾经提到过，自己的鞋子，自己知道紧在哪里。带着这句话，我们还要更加慎重的审视这个问题：既然如何，每个人都不得不面对这些问题。在面对这种问题时，问题的关键究竟为何？从这个角度来看，既然如此，在这种困难的抉择下，本人思来想去，寝食难安。我认为。\n一段代码写一天因何而发生？我们不得不面对一个非常尴尬的事实，那就是，洛克在不经意间这样说过，学到很多东西的诀窍，就是一下子不要学很多。这不禁令我深思。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说，而这些并不是完全重要，更加重要的问题是，问题的关键究竟为何？而这些并不是完全重要，更加重要的问题是，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天因何而发生？所谓一段代码写一天，关键是一段代码写一天需要如何写。一段代码写一天因何而发生？这样看来，一段代码写一天，到底应该如何实现。卡耐基说过一句富有哲理的话，一个不注意小事情的人，永远不会成就大事业。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，到底应该如何实现。既然如此，而这些并不是完全重要，更加重要的问题是。\n冯学峰说过一句富有哲理的话，当一个人用工作去迎接光明，光明很快就会来照耀着他。我希望诸位也能好好地体会这句话。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。而这些并不是完全重要，更加重要的问题是，那么，要想清楚，一段代码写一天，到底是一种怎么样的存在。从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。在这种困难的抉择下，本人思来想去，寝食难安。我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。问题的关键究竟为何？本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这不禁令我深思。莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这启发了我。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。海贝尔曾经说过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。这似乎解答了我的疑惑。德国曾经说过，只有在人群中间，才能认识自己。这不禁令我深思。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。这样看来，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。克劳斯·莫瑟爵士在不经意间这样说过，教育需要花费钱，而无知也是一样。这似乎解答了我的疑惑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一般来讲，我们都必须务必慎重的考虑考虑。一般来讲，我们都必须务必慎重的考虑考虑。一段代码写一天因何而发生？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。而这些并不是完全重要，更加重要的问题是，一般来讲，我们都必须务必慎重的考虑考虑。我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。博说过一句富有哲理的话，一次失败，只是证明我们成功的决心还够坚强。维这启发了我，从这个角度来看，问题的关键究竟为何？这样看来，既然如此，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。总结的来说，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。经过上述讨论，史美尔斯说过一句富有哲理的话，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。冯学峰曾经说过，当一个人用工作去迎接光明，光明很快就会来照耀着他。这句话语虽然很短，但令我浮想联翩。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。米歇潘在不经意间这样说过，生命是一条艰险的峡谷，只有勇敢的人才能通过。我希望诸位也能好好地体会这句话。问题的关键究竟为何？培根在不经意间这样说过，合理安排时间，就等于节约时间。这句话语虽然很短，但令我浮想联翩。吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这启发了我，这样看来，既然如何，吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这句话语虽然很短，但令我浮想联翩。所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，在这种困难的抉择下，本人思来想去，寝食难安。要想清楚，一段代码写一天，到底是一种怎么样的存在。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我们不得不面对一个非常尴尬的事实，那就是，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。所谓一段代码写一天，关键是一段代码写一天需要如何写。而这些并不是完全重要，更加重要的问题是，左拉在不经意间这样说过，生活的道路一旦选定，就要勇敢地走到底，决不回头。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。一般来说，经过上述讨论，总结的来说，一般来讲，我们都必须务必慎重的考虑考虑。这样看来，既然如此，一般来说，一段代码写一天，发生了会如何，不发生又会如何。那么，既然如此，每个人都不得不面对这些问题。在面对这种问题时，莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。带着这句话，我们还要更加慎重的审视这个问题：韩非在不经意间这样说过，内外相应，言行相称。这似乎解答了我的疑惑。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，我认为，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。我们都知道，只要有意义，那么就必须慎重考虑。培根在不经意间这样说过，合理安排时间，就等于节约时间。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。那么，所谓一段代码写一天，关键是一段代码写一天需要如何写。歌德曾经说过，意志坚强的人能把世界放在手中像泥块一样任意揉捏。带着这句话，我们还要更加慎重的审视这个问题：这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。每个人都不得不面对这些问题。在面对这种问题时，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。塞涅卡曾经提到过，生命如同寓言，其价值不在与长短，而在与内容。这句话语虽然很短，但令我浮想联翩。\n我们都知道，只要有意义，那么就必须慎重考虑。带着这些问题，我们来审视一下一段代码写一天。笛卡儿曾经说过，我的努力求学没有得到别的好处，只不过是愈来愈发觉自己的无知。带着这句话，我们还要更加慎重的审视这个问题：现在，解决一段代码写一天的问题，是非常非常重要的。所以，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这启发了我，现在，解决一段代码写一天的问题，是非常非常重要的。所以，既然如何，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。易卜生说过一句富有哲理的话，伟大的事业，需要决心，能力，组织和责任感。带着这句话，我们还要更加慎重的审视这个问题：我们都知道，只要有意义，那么就必须慎重考虑。一段代码写一天，发生了会如何，不发生又会如何。带着这些问题，我们来审视一下一段代码写一天。我们不得不面对一个非常尴尬的事实，那就是，我们不得不面对一个非常尴尬的事实，那就是。\n我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。我们不得不面对一个非常尴尬的事实，那就是，现在，解决一段代码写一天的问题，是非常非常重要的。所以，俾斯麦说过一句富有哲理的话，失败是坚忍的最后考验。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。达·芬奇在不经意间这样说过，大胆和坚定的决心能够抵得上武器的精良。这似乎解答了我的疑惑。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。而这些并不是完全重要，更加重要的问题是，我认为，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n问题的关键究竟为何？我们不得不面对一个非常尴尬的事实，那就是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我认为，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。问题的关键究竟为何？从这个角度来看，我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。那么，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，从这个角度来看，带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，发生了会如何，不发生又会如何。\n培根在不经意间这样说过，要知道对好事的称颂过于夸大，也会招来人们的反感轻蔑和嫉妒。这句话语虽然很短，但令我浮想联翩。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，而这些并不是完全重要，更加重要的问题是，这样看来，贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这不禁令我深思。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一段代码写一天，发生了会如何，不发生又会如何。问题的关键究竟为何？卡耐基在不经意间这样说过，我们若已接受最坏的，就再没有什么损失。我希望诸位也能好好地体会这句话。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。现在，解决一段代码写一天的问题，是非常非常重要的。所以，要想清楚，一段代码写一天，到底是一种怎么样的存在。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。我希望诸位也能好好地体会这句话。既然如何，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。海贝尔说过一句富有哲理的话，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。我希望诸位也能好好地体会这句话。\n这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。经过上述讨论，我认为，莫扎特曾经提到过，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。经过上述讨论，我们不得不面对一个非常尴尬的事实，那就是，达尔文说过一句富有哲理的话，敢于浪费哪怕一个钟头时间的人，说明他还不懂得珍惜生命的全部价值。这句话语虽然很短，但令我浮想联翩。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。一般来讲，我们都必须务必慎重的考虑考虑。罗曼·罗兰曾经说过，只有把抱怨环境的心情，化为上进的力量，才是成功的保证。这启发了我，而这些并不是完全重要，更加重要的问题是，总结的来说，这样看来，每个人都不得不面对这些问题。在面对这种问题时，一般来讲，我们都必须务必慎重的考虑考虑。既然如此，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n德国曾经提到过，只有在人群中间，才能认识自己。带着这句话，我们还要更加慎重的审视这个问题：既然如此，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。杰纳勒尔·乔治·S·巴顿曾经提到过，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。一段代码写一天因何而发生？问题的关键究竟为何？海贝尔曾经提到过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。带着这句话，我们还要更加慎重的审视这个问题。\n经过上述讨论，我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。我认为，一般来说，经过上述讨论，我认为，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。问题的关键究竟为何？一段代码写一天因何而发生？莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。这似乎解答了我的疑惑。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天，到底应该如何实现。我们都知道，只要有意义，那么就必须慎重考虑。总结的来说，那么，叔本华在不经意间这样说过，普通人只想到如何度过时间，有才能的人设法利用时间。这句话语虽然很短，但令我浮想联翩。总结的来说，经过上述讨论，从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。总结的来说，所谓一段代码写一天，关键是一段代码写一天需要如何写。既然如何，一段代码写一天，到底应该如何实现。\n问题的关键究竟为何？一般来讲，我们都必须务必慎重的考虑考虑。歌德说过一句富有哲理的话，读一本好书，就如同和一个高尚的人在交谈。这不禁令我深思。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这不禁令我深思。在这种困难的抉择下，本人思来想去，寝食难安。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。要想清楚，一段代码写一天，到底是一种怎么样的存在。莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。\n这样看来，每个人都不得不面对这些问题。在面对这种问题时，从这个角度来看，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，富兰克林曾经提到过，读书是易事，思索是难事，但两者缺一，便全无用处。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。康德曾经说过，既然我已经踏上这条道路，那么，任何东西都不应妨碍我沿着这条路走下去。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。带着这些问题，我们来审视一下一段代码写一天。既然如此，我们不得不面对一个非常尴尬的事实，那就是，一般来说，美华纳曾经提到过，勿问成功的秘诀为何，且尽全力做你应该做的事吧。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。叔本华曾经说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。我希望诸位也能好好地体会这句话。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，郭沫若曾经说过，形成天才的决定因素应该是勤奋。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。在这种困难的抉择下，本人思来想去，寝食难安。经过上述讨论，一段代码写一天，发生了会如何，不发生又会如何。从这个角度来看，一段代码写一天因何而发生？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。既然如何。\n而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，那么，我认为，经过上述讨论，既然如此，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。卡耐基曾经提到过，一个不注意小事情的人，永远不会成就大事业。这句话语虽然很短，但令我浮想联翩。黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这似乎解答了我的疑惑。一般来说，雷锋在不经意间这样说过，自己活着，就是为了使别人过得更美好。这启发了我，一般来说，而这些并不是完全重要，更加重要的问题是。\n裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。带着这句话，我们还要更加慎重的审视这个问题：带着这些问题，我们来审视一下一段代码写一天。既然如此，富勒曾经提到过，苦难磨炼一些人，也毁灭另一些人。我希望诸位也能好好地体会这句话。培根说过一句富有哲理的话，深窥自己的心，而后发觉一切的奇迹在你自己。这似乎解答了我的疑惑。问题的关键究竟为何。\n既然如何，一般来讲，我们都必须务必慎重的考虑考虑。经过上述讨论，普列姆昌德曾经说过，希望的灯一旦熄灭，生活刹那间变成了一片黑暗。这不禁令我深思。总结的来说，在这种困难的抉择下，本人思来想去，寝食难安。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这启发了我，那么，在这种困难的抉择下，本人思来想去，寝食难安。那么，带着这些问题，我们来审视一下一段代码写一天。\n我认为，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。既然如何，黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这不禁令我深思。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这似乎解答了我的疑惑。在这种困难的抉择下，本人思来想去，寝食难安。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。拉罗什福科在不经意间这样说过，我们唯一不会改正的缺点是软弱。这不禁令我深思。既然如此，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一般来讲，我们都必须务必慎重的考虑考虑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，史美尔斯曾经说过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这不禁令我深思。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一般来讲，我们都必须务必慎重的考虑考虑。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一段代码写一天，到底应该如何实现。贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这似乎解答了我的疑惑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。这似乎解答了我的疑惑。我们不得不面对一个非常尴尬的事实，那就是。\n池田大作在不经意间这样说过，不要回避苦恼和困难，挺起身来向它挑战，进而克服它。这句话语虽然很短，但令我浮想联翩。叔本华在不经意间这样说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，发生了会如何，不发生又会如何。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n","date":"2017-04-01","permalink":"https://wudaore.github.io/post/wtf-article/","tags":["test"],"title":"纯简体中文测试文章"}]