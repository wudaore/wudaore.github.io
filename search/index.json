[{"content":"目录 基本原理 逻辑回归的损失函数 逻辑回归的优化 随机梯度上升 分类评估方法 sklearn实现 ps. 1. 基本原理 虽然叫回归但是其实解决的是二分类问题.逻辑回归的输入值为线性回归的输出值，即\n之所以能解决二分类问题，是因为逻辑回归将输入赋予sigmod函数，并设置阈值，可以达到分类的效果\nsigmod函数：\n逻辑回归的工作流程如下：\n2. 对数似然函数 区别于线性回归的平方损失函数，逻辑回归使用对数似然函数.\n也可以这么写（综合完整损失函数.）\n为什么要使用这个损失函数呢？试想：当样本值（y）为1而线性回归预估值比较小（比如h(θ)接近0）时，通过对数似然函数进行计算，会得到一个很大的值，给予模型最大的惩罚力度；当样本值（y）为1而线性回归预估值比较大（比如h(θ)接近1）时，计算得出0，不给模型惩罚.就很巧妙.\n3. 逻辑回归的优化 根据上面的综合完整损失函数，可以推导出损失函数的优化是这样的：要使求和的值最小，就要提升原本是类别1的概率，降低原本是类别0的概率.\n同样可以使用梯度下降（上升）进行优化.\n需要注意的是 《实战》中并没有具体推导对数似然函数的优化过程.以下是推导过程：\n于是，《实战》中梯度上升的代码就能被解释了.\n# 梯度上升迭代参数\rdef gradAscent(dataMatIn, classLabels):\rdataMatrix = mat(dataMatIn) #convert to NumPy matrix\rlabelMat = mat(classLabels).transpose() #转置.\rm,n = shape(dataMatrix)\ralpha = 0.001\rmaxCycles = 500\rweights = ones((n,1))\rfor k in range(maxCycles): #maxCycles 迭代次数\rh = sigmoid(dataMatrix*weights) #matrix mult\rerror = (labelMat - h) #vector subtraction\rweights = weights + alpha * dataMatrix.transpose()* error # 上面推导出的结果\rreturn weights\r # 画出最优曲线\rdef plotBestFit(weights):\rimport matplotlib.pyplot as plt\rdataMat,labelMat=loadDataSet()\rdataArr = array(dataMat)\rn = shape(dataArr)[0] xcord1 = []; ycord1 = []\rxcord2 = []; ycord2 = []\rfor i in range(n):\rif int(labelMat[i])== 1:\rxcord1.append(dataArr[i,1])\rycord1.append(dataArr[i,2])\relse:\rxcord2.append(dataArr[i,1])\rycord2.append(dataArr[i,2])\rfig = plt.figure()\rax = fig.add_subplot(111)\rax.scatter(xcord1, ycord1, s=30, c='blue', marker='s')\rax.scatter(xcord2, ycord2, s=30, c='green')\rx = arange(-3.0, 3.0, 0.1)\r# 这里的Y其实是X2. 须知y=xw，x0=0，而根据sigmod函数图像，当xw=0时可以分割两类曲线.因此0=xw，可以推导出x1和x2（这里的Y）的关系\ry = (-weights[0]-weights[1]*x)/weights[2]\rax.plot(x, y)\rplt.xlabel('X1'); plt.ylabel('X2');\rplt.show()\r 4. 随机的梯度上升  gradAscent中dataMatrix * weights是矩阵相乘，实际进行了300次，即需要遍历整个数据集，在数据集很大时效率不好.需要进行优化.\n使用随机梯度上升，每次只计算一个数据的梯度而非整个数据集，可以达到更快的速度.\n\r# 随机梯度上升迭代参数\rdef stocGradAscent0(dataMatrix, classLabels):\rm,n = shape(dataMatrix)\ralpha = 0.01\rweights = ones(n) #initialize to all ones\rfor i in range(m):\rh = sigmoid(sum(dataMatrix[i]*weights))\rerror = classLabels[i] - h\rweights = weights + alpha * error * dataMatrix[i]\rreturn weights\r 当存在非线性可分的点时，会导致出现较大的波动。另外，需要加快收敛速度.故而引入改进的随机梯度上升.\n\rdef stocGradAscent1(dataMatrix, classLabels, numIter=150):\rm,n = shape(dataMatrix)\rweights = ones(n) #initialize to all ones\rfor j in range(numIter):\rdataIndex = range(m)\rfor i in range(m):\ralpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant\rh = sigmoid(sum(dataMatrix[randIndex]*weights))\rerror = classLabels[randIndex] - h\rweights = weights + alpha * error * dataMatrix[randIndex]\rdel(dataIndex[randIndex])\rreturn weights\r alpha = 4/(1.0+j+i)+0.0001 是为了让α随迭代逐步减小到逼近0.（但是并不是严格单调递减）\n另外，区别于上个算法，这个算法使用随机的样本来更新权重系数，这样做的好处是能能够减少周期波动.\n5.分类评估方法  混淆矩阵：\n其中TP和TN是正确预测的.\n准确率(accuracy)：含义为所有样本中预测正确的占比 (TP+TN) / (TP+TN+FP+FN)\n精确率(precision)：含义为预测为正例的样本中真实为正的占比，代表查的准不准 (TP) / (TP+FP)\n召回率(recall)：含义为真实为正例中预测为正的占比，表示对正样本的区分能力，查的全不全 (TP) / (TP+FN)\nF1-score：含义为模型的稳健性 (2TP) / (2TP+FN+FP) = 2precisionrecall / recall+precision\n这些评估方法在sklearn中的实现如下图所示：\n真阳率TPR，即召回率：含义为检测出来的真阳性样本数除以所有真实阳性样本数 (TP) / (TP+FN)\n假阳率FPR：含义为检测出来的假阳性样本数除以所有真实阴性样本数 (FP) / (TN+FP)\nROC曲线：x轴为FPR，y轴为TPR.当预测所有样本为1时，曲线的结果为中间的虚线.\nAUC指标，即随机取一样本，正样本大于负样本的概率.范围在[0.5,1]之间.当为1时为完美分类器.这个指标一般用于不平衡二分类问题\n需要注意的是，在sklearn中使用AUC计算API时，必须0为反例1为正例（一般取样本多的为正例，比如本实验中，4代表恶性，样本多，取为1）.\nROC曲线的绘制过程如下：\n1.先将各个点的概率从高到低排序.\n2.挑选最高的，假设其为正例.如果其概率大于阈值，那么就是真正例，否则就是假正例.每次挑选，都要就是那个FPR和TPR\n3.迭代直到所有点都求出TPR和FPR.在轴上描点，结果就是ROC曲线.如下图所示，取阈值为0.75\nAUC其实就是ROC曲线的积分，也即面积.其表示分对的概率\n6. sklearn实现  sklearn-linearRegression\n下面是该api的一些常见参数\n其中penalty 有L1和L2，详见线性回归的博客；C代表正则化力度，C越大，惩罚越大；solver代表使用的梯度下降算法.\nimport pandas as pd\rimport numpy as np\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import roc_auc_score\r# 1.获取数据\r# 2.基本数据处理\r# 2.1 缺失值处理\r# 2.2 确定特征值,目标值\r# 2.3 分割数据\r# 3.特征工程(标准化)\r# 4.机器学习(逻辑回归)\r# 5.模型评估\r# 1.获取数据\rnames = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\r'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\r'Normal Nucleoli', 'Mitoses', 'Class']\rdata = pd.read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\u0026quot;,names=names)\r# 2.基本数据处理\r# 2.1 缺失值处理\rdata = data.replace(to_replace=\u0026quot;?\u0026quot;, value=np.nan)\rdata = data.dropna()\rx = data.iloc[:, 1:-1]\ry = data[\u0026quot;Class\u0026quot;]\r# 2.3 分割数据\rx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2, test_size=0.2)\r# 3.特征工程(标准化)\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(逻辑回归)\restimator = LogisticRegression()\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 基本评估\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是：\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是：\\n\u0026quot;, score)\r# 5.2 其他评估\rret = classification_report(y_test, y_pre, labels=(2,4), target_names=(\u0026quot;良性\u0026quot;, \u0026quot;恶性\u0026quot;))\rprint(ret)\r# 不平衡二分类问题评估方法\ry_test = np.where(y_test\u0026gt;3, 1, 0)\rroc_auc_score(y_true=y_test, y_score=y_pre)\r 7.ps.  numpy中mat.getA 将矩阵转换为ndarray\nones((n, 1))，n代表几行，1代表每行有一个数.\n关于loc和iloc\n前者是根据标签索引进行查找，后者是根据整数索引进行查找.\n使用案例：\ndf.loc([['a','j'], ['name','score']]) #定位行索引值为a，j和列索引值为name和score的dataframe\rdf.iloc([[0,9],[0,2]]) #定位第0,9行，第0,2列的dataframe\r# 切片用法，loc前后都闭合，而iloc前闭后开\rdf.loc(['a':'j', 'name':'score']) #定位行索引值为a到j和列索引值为name到score的dataframe\rdf.iloc([0:9,0:2]) #定位第0到8行，第0到1列的dataframe\r drop的用法\n像本案例的情况 还是使用replace更好\n","date":"2022-06-03","permalink":"https://wudaore.github.io/post/logistic-regression/","tags":["机器学习","algo"],"title":"逻辑回归学习"},{"content":"目录 基本原理 线性回归的损失优化 局部加权线性回归 正则化线性模型 逐步向前回归 sklearn用法 总结 ps. 1. 基本原理 1.1 分类和回归 区别在于分类问题是定性的，输出离散值（如+1，-1）.而回归问题是定量的，输出连续值（如预测明天的气温为36.5度）\n1.2 基本原理 只有在线性可分的情况下才能使用.大概就是求一组参数w使得y能拟合wx\n2. 线性回归的损失优化 2.1 损失\u0026mdash;最小二乘法 最小，即最小化.二乘，即真实值-预测值的平法.普通最小二乘法就是要最小化这个平方值.\n由此，可以得到模型的损失函数\n2.2 优化\u0026mdash;正规方程 将上式写作矩阵形式.因为要求差值（j(θ)）的极值，所以两边对θ求导=0，解得θ的值\n最后解得w(即θ)的最优解\n特点：一蹴而就，一下子就能算出来，但是只适合样本和特征比较少的情况.\n2.3 优化\u0026mdash;梯度下降（更多用到） 梯度下降的公式如下：\n原理就是逐步降低梯度，更新数据，如此循环，知道梯度无限趋近于0，找到一定区间内的极小值.梯度下降无法保证找到最小值.\n其中参数α为步长，步长太大容易导致跳过极值点，太小导致计算缓慢.\n2.4 各种梯度下降算法 fg，全梯度下降 计算所有样本的误差平均值作为目标函数. 时间长，内存消耗大（一般不用）\nsag，随机平均梯度下降给每个样本都维持一个平均值.后期计算的时候参考这个平均值. 初期不佳，优化慢，因为该算法将初始梯度设为1，每轮梯度更新都结合上一轮.（首选）\nsg，随机梯度下降每次只选择一个样本 . 能快速将平均损失函数降到很低，但是必须注意步长，且无法代表整体样本（一般不用）\nmini-batch，小批量梯度下降选择一部分样本. 介于SG和FG之间（次选）\n2.5 正规方程和梯度下降两者对比 3. 局部加权线性回归LWLR  线性回归容易出现欠拟合.所以可以引入一些误差来降低均方误差.\n对待求点附近每个点加权（即赋予核），对于更近的点，将赋予更高的权重.\n对于权重W，《实战》中使用的是高斯核.显然，越近的点权重越大.\n公式中还有一个人为规定的参数k.k越大，越多的数据被用于训练.\n\rdef lwlr(testPoint,xArr,yArr,k=1.0):\rxMat = mat(xArr); yMat = mat(yArr).T\rm = shape(xMat)[0]\rweights = mat(eye((m)))\rfor j in range(m):\rdiffMat = testPoint - xMat[j,:]\rweights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))\rxTx = xMat.T * (weights * xMat)\rif linalg.det(xTx) == 0.0: #需要检测矩阵是否可逆.\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = xTx.I * (xMat.T * (weights * yMat))\rreturn testPoint * ws\rdef lwlrTest(testArr,xArr,yArr,k=1.0): #对于每个点，都要遍历xarr计算权重\rm = shape(testArr)[0]\ryHat = zeros(m)\rfor i in range(m):\ryHat[i] = lwlr(testArr[i],xArr,yArr,k)\rreturn yHat\r 4.正则化线性模型（岭回归和lasso回归）  4.0 欠拟合和过拟合. 欠：在训练集和测试集上表现都不包\n解决方法：\n1.添加其他特征\n2.添加多项式特征.\n过：在训练集上好 测试集上不好.\n解决方法：\n1.重新清洗数据\n2.增大训练量\n3.正则化 即尽量减少高次项的影响.分为L1正则化（直接使w的一些值为0，如LASSO回归）和L2正则化(使得一些高次项的w值接近0，削弱某个特征的影响，如ridge岭回归)\n4.减少特征维度，防止维度灾难\n4.1 岭回归（推荐） 线性回归和局部加权的线性回归都存在求逆的问题.而当矩阵不可逆时（或者输入线性相关，约等于不可逆），可以引入岭回归.岭回归不仅解决矩阵不可逆的情况，还引入惩罚项，防止过拟合.\n相较于线性回归，岭回归引入了惩罚项如下：\n经过与线性回归相似的推导（求导，使极值为0）,得到岭回归的w最优解：\n岭回归的具体推导过程\n\rdef rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays\rreturn ((yArr-yHatArr)**2).sum()\rdef ridgeRegres(xMat,yMat,lam=0.2):\rxTx = xMat.T*xMat\rdenom = xTx + eye(shape(xMat)[1])*lam\rif linalg.det(denom) == 0.0:\rprint(\u0026quot;This matrix is singular, cannot do inverse\u0026quot;)\rreturn\rws = denom.I * (xMat.T*yMat)\rreturn ws\rdef ridgeTest(xArr,yArr):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #to eliminate X0 take mean off of Y\r#regularize X's\rxMeans = mean(xMat,0) #calc mean then subtract it off\rxVar = var(xMat,0) #calc variance of Xi then divide by it\rxMat = (xMat - xMeans)/xVar\rnumTestPts = 30\rwMat = zeros((numTestPts,shape(xMat)[1]))\rfor i in range(numTestPts):\rws = ridgeRegres(xMat,yMat,exp(i-10))\rwMat[i,:]=ws.T\rreturn wMat\r 4.2 lasso 相较于岭回归，lasso只是更改了惩罚项.\n岭回归通过增加平方项，并结合约束条件来约束高次项w的值（L2正则），顾其图线会是圆润的.而lasso回归通过增加绝对值项，并结合约束条件使得一些w项为0，表现在图线上就是不可导点，是尖锐的（在添加约束条件\u0026ndash;wk 的绝对值求和小于λ\u0026ndash;后，当λ足够小，会使一些系数被迫降到0.）.\n通过这种方法，能够自动进行特征选择.\nlasso可以更好的帮助分析数据（庞大特征数量下的特征选择），但是却大大增加了计算量且不太稳定.所以提出既能分析数据又能减少计算量的方法\u0026ndash;逐步向前回归\n4.3 Elastic Net 弹性网络 弹性网络通过混合比参数r控制岭回归和lasso回归.\n一般情况下优先使用岭回归，岭回归不适合的话才考虑弹性网络，最后才是lasso\n4.4 Early Stopping 当错误率达到阈值时，停止迭代.\n5.逐步向前回归  逐步向前回归的原理大概是:标准化数据后，经过若干次迭代，每次迭代改变一个系数得到新的w（分为增大和减小）.如新的w误差更小，则取新的w.\n算法共有两个可以调节的参数，即迭代次数和迭代步长.\ndef stageWise(xArr,yArr,eps=0.01,numIt=100):\rxMat = mat(xArr); yMat=mat(yArr).T\ryMean = mean(yMat,0)\ryMat = yMat - yMean #can also regularize ys but will get smaller coef\rxMat = regularize(xMat)\rm,n=shape(xMat)\rreturnMat = zeros((numIt,n)) #testing code remove\rws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()\rfor i in range(numIt):\rprint(ws.T)\rlowestError = inf; for j in range(n):\rfor sign in [-1,1]:\rwsTest = ws.copy()\rwsTest[j] += eps*sign\ryTest = xMat*wsTest\rrssE = rssError(yMat.A,yTest.A)\rif rssE \u0026lt; lowestError:\rlowestError = rssE\rwsMax = wsTest\rws = wsMax.copy()\rreturnMat[i,:]=ws.T\rreturn returnMat\r 6.sklearn用法  6.1 线性回归 可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n正规方程：\n梯度下降：\n具体案例使用如下（使用正规方程和梯度下降）：\n预测波士顿房价\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据\r2.数据基本处理\r2.1 数据集划分\r3.特征工程 --标准化\r4.机器学习(线性回归)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_boston\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, RidgeCV\rfrom sklearn.metrics import mean_squared_error\rdef linear_model1():\r\u0026quot;\u0026quot;\u0026quot;\r正规方程\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(线性回归)\restimator = LinearRegression()\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rdef linear_model2():\r\u0026quot;\u0026quot;\u0026quot;\r梯度下降法\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\restimator = SGDRegressor(max_iter=1000, learning_rate=\u0026quot;constant\u0026quot;, eta0=0.001)\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\rif __name__ == '__main__':\rlinear_model1()\rlinear_model2()\rlinear_model3()\r 6.2 岭回归 岭回归是线性回归的正则化版本.在sklearn中，使用SGDRegressor，设置参数penalty=l2，也可以实现岭回归，但是只能实现普通的梯度下降.而使用 Ridge可以实现随机平均梯度下降SAG\n对于参数alpha，根据上面岭回归的公式，alpha越大，正则化力度越大，权重系数就越小.\n对于参数normalize，如果为true，就可以不必进行上面的标准化.\n可以在sklearn官网中查看模型的参数和一些返回值.点击查看\n另外，该博客对参数和返回值做了一些中文解释，方便阅读和理解.点击跳转\n\rdef linear_model3():\r\u0026quot;\u0026quot;\u0026quot;\r岭回归\r:return: None\r\u0026quot;\u0026quot;\u0026quot;\r# 1.获取数据\rboston = load_boston()\r# 2.数据基本处理\r# 2.1 数据集划分\rx_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\r# 3.特征工程 --标准化\rtransfer = StandardScaler()\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习\r# estimator = Ridge()\restimator = RidgeCV(alphas=(0.001, 0.1, 1, 10, 100))\restimator.fit(x_train, y_train)\rprint(\u0026quot;这个模型的偏置是:\\n\u0026quot;, estimator.intercept_)\r# 5.模型评估\r# 5.1 预测值和准确率\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rscore = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, score)\r# 5.2 均方误差\rret = mean_squared_error(y_test, y_pre)\rprint(\u0026quot;均方误差是:\\n\u0026quot;, ret)\r 6.3 lasso 官网\n注释参数博客\n7. 总结  从本文总结的算法来看，主要经过以下流程：线性回归(容易欠拟合)\u0026ndash;\u0026gt;加权线性回归(容易过拟合且无法处理不可逆矩阵)\u0026ndash;\u0026gt;岭回归\u0026ndash;(不好分析数据)\u0026ndash;\u0026gt;lasso(计算量大)\u0026ndash;\u0026gt;逐步向前回归.\n通过加权线性回归我们知道，越小的核越能拟合训练数据.当然，也越容易过拟合.而lasso和逐步向前回归允许我们参考输入向量中每一维数据的作用大小，从而可以对数据进行缩减.\n然而，上述算法终究只适合线性的数据.对于非线性的数据，需要使用其他算法.\n8.ps.  1.numpy.matrix numpy.matrix表示矩阵，使用numpy.matrix.A方法可以像数组一样根据下标取出值，不能直接用下标.\n官方文档\n2.numpy.eye() 生成对角阵.\n具体链接\n3.矩阵求导 在推导正规方程时使用了矩阵的求导公式，点击跳转\n","date":"2022-06-02","permalink":"https://wudaore.github.io/post/linear-regression/","tags":["机器学习","algo"],"title":"线性回归学习"},{"content":"Introduction 基本原理 各种距离 距离调参 KNN优化-KD树 数据处理（归一化和标准化） 实战\u0026ndash;鸢尾花数据集 总结 1. 基本原理 非常朴素的原理。根据距离最近的K个点来判断目标点的类别.\n2. 各种距离 标准距离\n汉明距离\n杰卡德距离\n马氏距离\n余弦距离\n3. 距离调参 p=1 曼哈顿距离 2 欧式距离 无穷 切比雪夫距离 三者都是闵式距离.\n4.KNN优化-KD树 4.1 KD树基本原理 根据KNN的基本原理可知，当需要预测一个点时，需要计算训练集中每个店到它的距离.当数据集很大时，对于N个样本，D个特征的数据集，算法的时间复杂度到达O(D*N^2)\nKD树的基本原理是通过树来分割数据，假设A和B的距离很远，B和C的距离很近，那么A和C的距离一定很远.通过这种方法，可以跳过很多距离远的点，减少计算成本.\n优化后算法的复杂度为O(DNlog(N))\n4.2 KD树的实现 假设有数据点{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}\n要根据X和Y的方差计算第一次分割的平面.X的方差更大所以选择首先分割X轴，选择其中间的(7,2),切割\n第二次根据Y进行分割平面.\n左：(2,3),(4,7),(5,4) \u0026ndash;\u0026gt;3,4,7\n右：(8,1),(9,6) \u0026ndash;\u0026gt;1,6\n对点(5,4)和(9,6)进行切割\n第三次数据集中的点已经很少了，直接进行切割.得到结果如下：\n4.3 KD树的最近邻查找 以上述数据集为例，现在要查找点(2.1,3.1)则根据x-y-x的顺序遍历构造好的KD树，发现经过以下路径：\n\u0026lt;(7,2),(5,4),(2,3)\u0026gt; 反向遍历该路径.\n先假设(2,3)为最近点，计算与其欧式距离为0.141.以(2.1,3.1)为圆心，以0.141这个距离为半径画圆，发现不能与Y=4相交.因为(5,4)点是根据Y分割的，所以没必要到该点的右平面去查找.\n同样，圆不能与x=7相，所以也没必要到(7,2)的右平面查找.可知最近点为(2,3)\n而以点(2,4.5)为例，得到查找路径 \u0026lt;(7,2),(5,4),(4,7)\u0026gt; .先假设(4,7)为最优解，并按照上述步骤画圆.(4,7)和(5,4)出队列后发现，该圆能将(5,4)包括进去.意味着此时的最优点已经发生了变化，半径也变成了3.04.所以需要到(5,4)的左平面查找.此时的查找队列为\u0026lt;(7,2),(2,3)\u0026gt;\n继续回溯队列到(2,3)，此时发现该点更近，更新为最优解.距离更新为1.5\n最后回溯至(7,2)发现圆无法包括，不需要找(7,2)的右平面.查找完毕.\n以上只是举了例子，更详细的原理：https://www.bilibili.com/video/BV19f4y1R7vh?spm_id_from=333.337.search-card.all.click\n5.数据处理 5.1 归一化 将数据映射到[0,1]区间内.公式如下图：\nsklearn提供了对应的API接口.具体实现代码如下：\n# # 1.归一化\r# # 1.1 实例化一个转换器\rtransfer = MinMaxScaler(feature_range=(0, 1))\r# # 1.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过归一化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 5.1 标准化 异常点对归一化的影响特别大，只适合传统精确小数，鲁棒性较差.\n标准化：通过转换将数据转换为均值为0，标准差为1的范围内，公式如下：\nsklearn提供了对应的API接口.具体实现代码如下：\n# 2.标准化\r# 2.1 实例化一个转换器\rtransfer = StandardScaler()\r# 2.2 调用fit_transfrom方法\rminmax_data = transfer.fit_transform(data[['milage', 'Liters', 'Consumtime']])\rprint(\u0026quot;经过标准化处理之后的数据为:\\n\u0026quot;, minmax_data)\r 6.实战\u0026ndash;鸢尾花数据集(交叉验证，网格搜索) KNeighborsClassifier 可以使用algorithm参数指定使用的算法，包括ball树，暴力和kd树.具体见sklearn官网https://scikit-learn.org.cn/view/85.html\n交叉验证：将训练集分为训练集和验证集.交叉验证并不能提高准确率 但是能使模型更加可信 网格搜索：即将超参数(需要手动指定的参数)通过字典的形式传入，进行最优选择\nGridSearchCV的参数如下：https://scikit-learn.org.cn/view/655.html\n# coding:utf-8\r\u0026quot;\u0026quot;\u0026quot;\r1.获取数据集\r2.数据基本处理\r3.特征工程\r4.机器学习(模型训练)\r5.模型评估\r\u0026quot;\u0026quot;\u0026quot;\rfrom sklearn.datasets import load_iris\rfrom sklearn.model_selection import train_test_split, GridSearchCV\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.neighbors import KNeighborsClassifier\r# 1.获取数据集\riris = load_iris()\r# 2.数据基本处理\r# 2.1 数据分割\rx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\r# 3.特征工程\r# 3.1 实例化一个转换器\rtransfer = StandardScaler()\r# 3.2 调用fit_transform方法\rx_train = transfer.fit_transform(x_train)\rx_test = transfer.fit_transform(x_test)\r# 4.机器学习(模型训练)\r# 4.1 实例化一个估计器\restimator = KNeighborsClassifier()\r# 4.2 调用交叉验证网格搜索模型\rparam_grid = {\u0026quot;n_neighbors\u0026quot;: [1, 3, 5, 7, 9]}\restimator = GridSearchCV(estimator, param_grid=param_grid, cv=10, n_jobs=-1)\r# 4.3 模型训练\restimator.fit(x_train, y_train)\r# 5.模型评估\r# 5.1 输出预测值\ry_pre = estimator.predict(x_test)\rprint(\u0026quot;预测值是:\\n\u0026quot;, y_pre)\rprint(\u0026quot;预测值和真实值对比:\\n\u0026quot;, y_pre == y_test)\r# 5.2 输出准确率\rret = estimator.score(x_test, y_test)\rprint(\u0026quot;准确率是:\\n\u0026quot;, ret)\r# 5.3 其他评价指标\rprint(\u0026quot;最好的模型：\\n\u0026quot;, estimator.best_estimator_)\rprint(\u0026quot;最好的结果:\\n\u0026quot;, estimator.best_score_)\rprint(\u0026quot;整体模型结果:\\n\u0026quot;, estimator.cv_results_)\r 7.总结  k近邻的优点如下：\n1.简单有效\n2.重新训练代价低\n3.适合大样本自动分类\n该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n4.适合类域交叉样本\nKNN方法主要靠周围有限的邻近的样本,而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合\n缺点如下：\n1.惰性学习\nKNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多\n2.类别评分非规格化\n不像一些通过概率评分的分类\n3.输出的可解释性不强\n例如决策树的输出可解释性就较强\n4.不擅长不均衡样本\n当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。\n5.计算量太大\n","date":"2022-05-29","permalink":"https://wudaore.github.io/post/knn/","tags":["机器学习","algo"],"title":"KNN学习"},{"content":"Introduction 对matplotlib的简易学习\nimport matplotlib.pyplot as plt import random  matplotlib图像的绘制 # 1. 创建画布 # figsize 画布大小 dpi 像素 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 3.图像展示 plt.show()  # help(plt.figure)  图像保存 # 1. 创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图像绘制 x = [1,2,3,4,5,6] y = [3,6,3,5,3,10] plt.plot(x, y) # 2.1 图像保存 plt.savefig(\u0026quot;./data/test.png\u0026quot;) # 3.图像展示 plt.show() # 图像保存一定要放到show前面 # # 2.1 图像保存 # plt.savefig(\u0026quot;./data/test.png\u0026quot;)  案例：显示温度变化状况 图像基本绘制功能演示 # 解决中文显示不正常 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False # 0.生成数据 x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.图形绘制 plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] plt.yticks(y_ticks[::5]) plt.xticks(x[::5], x_ticks_labels[::5]) # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 # 第一个参数数字的功能大概是横坐标的实际个数 # 2.2 添加网格 plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # 2.3 添加描述 plt.xlabel(\u0026quot;时间\u0026quot;) plt.ylabel(\u0026quot;温度\u0026quot;) plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # 图例对应plt.label plt.legend(loc='best') # 3.图像展示 plt.show()  多个坐标系显示图像 # 0.生成数据 plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签 plt.rcParams['axes.unicode_minus']=False x = range(60) y_beijing = [random.uniform(10, 15) for i in x] y_shanghai = [random.uniform(15, 25) for i in x] y_tianjin = [random.uniform(10, 19) for i in x] y_zhejiang = [random.uniform(12, 25) for i in x] # 1.创建画布 # plt.figure(figsize=(20, 8), dpi=100) # 多个坐标系 使用subplots 参数：几行几列 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 16), dpi=100) # 2.图形绘制 # plt.plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) # plt.plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[0][0].plot(x, y_beijing, label=\u0026quot;北京\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[0][1].plot(x, y_shanghai, label=\u0026quot;上海\u0026quot;) axes[1][0].plot(x, y_tianjin, label=\u0026quot;天津\u0026quot;, color=\u0026quot;g\u0026quot;, linestyle=\u0026quot;-.\u0026quot;) axes[1][1].plot(x, y_zhejiang, label=\u0026quot;浙江\u0026quot;) # # 2.1 添加x,y轴刻度 y_ticks = range(40) x_ticks_labels = [\u0026quot;11点{}分\u0026quot;.format(i) for i in x] # plt.yticks(y_ticks[::5]) # plt.xticks(x[::5], x_ticks_labels[::5]) # # plt.xticks(x_ticks_labels[::5]) # 必须最开始传递进去的是数字 axes[0][0].set_xticks(x[::5]) axes[0][0].set_yticks(y_ticks[::5]) axes[0][0].set_xticklabels(x_ticks_labels[::5]) axes[0][1].set_xticks(x[::5]) axes[0][1].set_yticks(y_ticks[::5]) axes[0][1].set_xticklabels(x_ticks_labels[::5]) # # 2.2 添加网格 # plt.grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][0].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) axes[0][1].grid(True, linestyle=\u0026quot;--\u0026quot;, alpha=0.7) # # 2.3 添加描述 # plt.xlabel(\u0026quot;时间\u0026quot;) # plt.ylabel(\u0026quot;温度\u0026quot;) # plt.title(\u0026quot;一小时温度变化图\u0026quot;, fontsize=20) axes[0][0].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][0].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][0].set_title(\u0026quot;北京一小时温度变化图\u0026quot;, fontsize=20) axes[0][1].set_xlabel(\u0026quot;时间\u0026quot;) axes[0][1].set_ylabel(\u0026quot;温度\u0026quot;) axes[0][1].set_title(\u0026quot;上海一小时温度变化图\u0026quot;, fontsize=20) # 2.4 显示图例 # plt.legend(loc=0) axes[0][0].legend(loc=0) axes[0][1].legend(loc=0) # 3.图像展示 plt.show()  plot绘制数学图像 import numpy as np  # 0.生成数据 x = np.linspace(-10, 10, 1000) y = x*x*x # 1.创建画布 plt.figure(figsize=(20, 8), dpi=100) # 2.绘制 plt.plot(x, y) plt.grid() # 3.显示 plt.show()    ","date":"2022-05-24","permalink":"https://wudaore.github.io/post/hello_matplotlib/","tags":["机器学习"],"title":"matplotlib基础"},{"content":"Introduction \u0026laquo;实战\u0026raquo;中的svm数学推导实在是有些晦涩难懂.\nbilibili视频教程：https://www.bilibili.com/video/BV1Hs411w7ci?spm_id_from=333.337.search-card.all.click\n本文主要总结一部分svm的数学推导.后续再补全以及学习SVM的使用\n在开始之前，需要了解一些基本的概念\n1. 基础概念 1.1 wTx=b 以二维空间为例子，wTx 是指向量x在向量W上的投影长度乘以向量W的长度.wTx=b几何上来说所有乘积为b的向量x构成的一个平面.且该平面显然垂直于向量x.\n当出现空间中的另一个点a，想要计算该点到上述超平面的距离，可先将该点与x向量与w向量的起始点连接，形成x,向量.根据公式\n进行计算.详细推导见博客https://blog.csdn.net/Zelf0914/article/details/95811715\n理解之后不难发现，参数b的作用是控制超平面前后移动\nsvm \u0026mdash;间隔 对偶 核技巧\n2.硬间隔SVM 即最大间隔分类器.适用于数据线性可分.\n核心就是要找出一个超平面，使其对于“最近的点”的距离最大.（数据集中有很多点，对当前点的距离大了会导致和其他点的距离变小，这个时候其他点就变成“最近的点”了.)硬间隔SVM就是要求出对于“最近的点”的距离的最大值，即\n其中yi对应书中的label，当wTx\u0026gt;0时yi=1，反之为-1\n而|wTx+b|等价于yi（wTx+b） （因为yi只有可能是正负1且wTx+b时yi为+1，反之为-1.这样的转换是等价的.）\n替换后提出1/||w||（因为min内计算的是点到直线的距离，和向量w无关.||w||表示n维向量的模，称为范数），令min[yi（wTx+b）]=1（wTx+b 本质是一个向量，对其进行缩放不影响，就比如二维空间中的直线，所以设为1其实是设了个最小值）\n此时等式就变成了求max（1/||w||），等价于求min（1/2 wTw）(加1/2是为了方便求最小值？)\n推导过程见下图\n式子的目标函数是二次函数，限制条件是N个线性不等式条件，所以这是凸二次优化问题，可以用拉格朗日乘子法和KKT条件进行求解。拉格朗日乘子法可以寻找多元函数在一组约束条件下的极值，将d个变量与k个约束的最优化问题转变成d+k个变量的无约束问题。\n2.1 拉格朗日乘子法 经过上述推导，问题就变成了带有不等式约束条件的最优化问题（约束条件为yi（wTx+b）\u0026gt;=1）.\n借助拉格朗日乘子 将带约束问题转换成无约束问题（考研数学做过类似的题型，也可参考博客https://blog.csdn.net/lijil168/article/details/69395023），\n同时，1/2wTw 又等价于maxλL(w，b，λ)，故而可以如下进行转换，如下图：\n该问题的目标函数是二次的，约束又是线性的，故而满足强对偶关系.故而进行以下转换：\nL先对b求偏导，令结果=0，并带回L原式：\n继续对w求偏导.因为wTw=wwT，有以下转换：\n此时的w已经是最优解（？）\n最终得到如下转换\n2.2 KKT条件 满足强对偶关系（如上述，目标函数是二次且约束是线性）的充要条件是满足KKt条件.\nKKT条件：\n可知w和b都是关于x和y的组合.同时，λi只在支持向量上有意义，其他时候都是0\n支持向量即分类结果中平行于分类平面的过最近点的两个向量（不知道这个理解对不对），只有在支持向量上，wwT+b的绝对值才等于1，λi才有意义.\n软间隔SVM soft 指允许一点点误差.只需要在原目标函数后加一个误差函数loss即可：min（1/2 wTw）+loss\nloss如果使用yi(wwT+b)的点的个数的话会导致结果离散，不好求导.所以一般使用距离。即loss = max{0, 1-yi(wwT+b)},这就是hinge loss\n引入ξ表示1-yi(wwT+b)，即误差的距离.调整约束条件和目标函数，的软间隔svm模型：\n写在最后 非数学系，数学功底不好，做这个东西真的困难.还是了解一下，不做深入证明（如强弱对偶性的证明，太头疼），更偏向代码和运用一些.\n","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm1/","tags":["机器学习","algo"],"title":"svm学习(1)--基础概念和公式"},{"content":"Introduction SMO算法 即序列最小优化算法 对\u0026laquo;实战\u0026raquo;中的smoSimple函数尝试理解\n \rdef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\r\u0026quot;\u0026quot;\u0026quot;smoSimple\rArgs:\rdataMatIn 特征集合\rclassLabels 类别标签\rC 松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。\r控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。\r可以通过调节该参数达到不同的结果。\r其实就是上一篇博客中提到的loss值\rtoler 容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）\rmaxIter 退出前最大的循环次数\rReturns:\rb 模型的常量值\ralphas 拉格朗日乘子\r\u0026quot;\u0026quot;\u0026quot;\rdataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\rb = 0; m,n = shape(dataMatrix)\r# alpha 初始全为0\ralphas = mat(zeros((m,1)))\rprint(len(alphas))\rprint(len(dataMatrix))\riter = 0\rwhile (iter \u0026lt; maxIter):\r# 记录alpha是否已经优化\ralphaPairsChanged = 0\rfor i in range(m):\r# 预测的类别\r# 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*label[n]*x[n]\r# 理解了半天 被这个括号害死了\rfXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\r# 预测结果与真是结果比对 得到误差\rEi = fXi - float(labelMat[i])#if checks if an example violates KKT conditions\r# 如果误差大的话，就要进行优化.选择另一个向量j\rif ((labelMat[i]*Ei \u0026lt; -toler) and (alphas[i] \u0026lt; C)) or ((labelMat[i]*Ei \u0026gt; toler) and (alphas[i] \u0026gt; 0)):\rj = selectJrand(i,m)\rfXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\rEj = fXj - float(labelMat[j])\ralphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\r# L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句\r# labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。\rif (labelMat[i] != labelMat[j]):\rL = max(0, alphas[j] - alphas[i])\rH = min(C, C + alphas[j] - alphas[i])\relse:\rL = max(0, alphas[j] + alphas[i] - C)\rH = min(C, alphas[j] + alphas[i])\rif L==H: print (\u0026quot;L==H\u0026quot;); continue\r# eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程\r# 参考《统计学习方法》李航-P125~P128\u0026lt;序列最小最优化算法\u0026gt;\reta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\rif eta \u0026gt;= 0: print (\u0026quot;eta\u0026gt;=0\u0026quot;); continue\r# 计算出一个新的alphas[j]值\ralphas[j] -= labelMat[j]*(Ei - Ej)/eta\r# 并使用辅助函数，以及L和H对其进行调整\ralphas[j] = clipAlpha(alphas[j],H,L)\rif (abs(alphas[j] - alphaJold) \u0026lt; 0.00001): print(\u0026quot;j not moving enough\u0026quot;); continue\r# 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反\ralphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\r# 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。\r# w= Σ[1~n] ai*yi*xi =\u0026gt; b = yj- Σ[1~n] ai*yi(xi*xj)\r# 所以： b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)\r# 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍\rb1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\rb2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\rif (0 \u0026lt; alphas[i]) and (C \u0026gt; alphas[i]): b = b1\relif (0 \u0026lt; alphas[j]) and (C \u0026gt; alphas[j]): b = b2\relse:\rb = (b1 + b2)/2.0\ralphaPairsChanged += 1\rprint(\u0026quot;iter: %d i:%d, pairs changed %d\u0026quot; % (iter, i, alphaPairsChanged))\r# 在for循环外，检查alpha值是否做了更新，如果更新则将iter设为0后继续运行程序\r# 直到更新完毕后，iter次循环无变化，才退出循环。\rif (alphaPairsChanged == 0):\riter += 1\relse:\riter = 0\rprint(\u0026quot;iteration number: %d\u0026quot; % iter)\rreturn b, alphas\r ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm2/","tags":["机器学习","algo"],"title":"svm学习(2)--简易SMO算法代码理解"},{"content":"Introduction sklearn的SVM算法可以调整c值和核函数.本篇并未介绍核函数\n 导入数据和数据-标签的处理\nfrom sklearn.svm import SVC\rfrom sklearn import datasets\riris = datasets.load_iris()\rX = iris['data']\rX = iris['data'][:,(2,3)]\ry = iris['target']\r SVM训练.c值设置为无穷大代表几乎不容错 使用线性核函数\nsetosa_or_versicolor = (y==0)|(y==1)\rX = X[setosa_or_versicolor]\ry = y[setosa_or_versicolor]\rsvm_clf = SVC(kernel='linear', C=float('inf'))\rsvm_clf.fit(X,y)\rw = svm_clf.coef_\rb = svm_clf.intercept_\rw\r 使用matplotlib绘图\nimport numpy as np\rimport matplotlib\rimport os\rimport matplotlib.pyplot as plt\rimport math\r%matplotlib inline\rplt.rcParams['axes.labelsize']=14\rplt.rcParams['xtick.labelsize']=12\rplt.rcParams['ytick.labelsize']=12\rx0=np.linspace(0, 5.5, 200) pred_1=5*x0-20 pred_2=x0-1.8 pred_3=0.1*x0+0.5 def plot_svc_decision_boundary(svm_clf,xmin,xmax,sv=True):\rw= svm_clf.coef_[0]\rb=svm_clf.intercept_[0]\rprint(w)\rx0=np.linspace(xmin,xmax,200)\rdecision_boundary=-w[0]/w[1] *x0-b/w[1]\r#这里的margin算的并不是1/||w||,而是1/w[1]，是为了方便绘制支持向量\r#margin = 1 / np.sqrt(np.sum(svm_clf.coef_**2))\rmargin=1/w[1]\rprint(margin)\rgutter_up=decision_boundary+margin\rgutter_down=decision_boundary-margin\rif sv:\rsvs = svm_clf.support_vectors_\rplt.scatter(svs[:,0], svs[:,1], s=180, facecolors='#FFAAAA')\rplt.plot(x0, decision_boundary, 'k-', linewidth=2)\rplt.plot(x0, gutter_up, 'k-', linewidth=2)\rplt.plot(x0, gutter_down, 'k-', linewidth=2)\rplt.figure(figsize=(14,4))\rplt.subplot(121)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.plot(x0,pred_1,'g--',linewidth=2)\rplt.plot(x0,pred_2,'m-',linewidth=2)\rplt.plot(x0,pred_3,'r-',linewidth=2)\rplt.axis([0,5.5,0,2])\rplt.subplot(122)\rplot_svc_decision_boundary(svm_clf, 0, 5.5)\rplt.plot(X[:,0][y==1],X[:,1][y==1],'bs')\rplt.plot(X[:,0][y==0],X[:,1][y==0],'ys')\rplt.axis([0,5.5,0,2])\r 结果如下： ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/svm3/","tags":["机器学习","algo"],"title":"svm学习(3)--SVM实现鸢尾花数据分类"},{"content":"Introduction 系统的学习Linux对日常学习和之后的工作都有帮助\n本篇博客记录学习ubuntu文件系统时遇到的问题和需要记录的点\n1.ubuntu 常见目录 ","date":"2022-05-13","permalink":"https://wudaore.github.io/post/ubuntu-filesys/","tags":["linux"],"title":"ubuntu学习(1)--文件系统"},{"content":"Introduction 本文主要介绍如何在windows下使用hugo搭建自己的博客并通过github挂载，同时绑定自己的域名\nbilibili视频教程：https://www.bilibili.com/video/BV13c411h7k7?spm_id_from=333.999.0.0\n本文主要总结一些踩的坑以及功能拓展\nstep1 安装配置hugo hugo中文文档 ：https://www.gohugo.org/\nhugo编译文件下载地址：https://github.com/gohugoio/hugo/releases\n解压后放入bin文件夹内，将bin添加到系统变量，如下图所示\n打开cmd输入hugo version，若能显示版本，则配置成功\nstep2 创建博客 2.1创建项目文件 在cmd中输入以下指令创建项目文件.\nhugo new site yourname.com  2.2选择博客样式 在https://themes.gohugo.io/中选择下载博客的样式.本博客使用fuji模板，https://themes.gohugo.io/themes/hugo-theme-fuji/\n样式下载方法：直接git clone到项目文件夹下的themes中（ssh），或下载压缩包解压到themes中（http），如下图所示\n下载后需要在项目文件夹下的config.toml中修改样式名.同时可以修改博客名等字段\n注意项目名为应该与模板中的文件夹名相同，如下图所示，项目名就应该是hermit.\n进入hermit（以hermit模板为例）下的ExampleSite文件夹中，复制content和config.toml粘贴到项目文件夹xxx.com下.其中content为博客模板，是markdown文件，config.toml是配置项.\n导入完毕后在项目文件夹xxx.com下的cmd中执行命令.\nhugo  即可编译项目.\nstep3 通过github挂载 3.1建立个人仓库 注册github账号后点击头像-Your Repositories即可新建/管理仓库\n个人仓库名必须是github用户名+github.io，否则会报错.readme文档暂时不添加，防止冲突\n作者使用的是master分支而非main，这需要进入setting-branch中进行配置.还需要使用git push origin -d main 删除main分支\n建立仓库后通过git-scm.com下载最新版git，同样需要将git的bin文件加入系统变量\n3.2本地代码同步到git 首先在项目文件夹.com下的cmd中输入hugo编译项目.编译完成后出现public文件夹，即编译后的文件.\n进入public，右键Git Bash Here进入Git命令行.相继输入\ngit init git add -A git commit -m\u0026quot;init\u0026quot; git remote add origin https://github.com/你的用户名/你的用户名.github.io.git git push -f origin master  如果是第一次使用git，还需要进行验证（如果是http方式则会跳出窗口，如果是ssh方式则需要设置密钥公钥，详见博客https://www.xuanfengge.com/using-ssh-key-link-github-photo-tour.html）\npush完毕后打开github仓库，就能发现public中的内容已经被存到仓库中了\n此时浏览器访问用户名.github.io即可进入博客.\nstep4 关联到域名 4.1域名解析 本文使用的是腾讯云.进入控制台选择域名解析，添加以下配置：\n4.2github中的配置 进入项目仓库-settings-page，在Custom domain中配置自己的域名，等待解析完成即可.此页面也可配置默认分支为master，就不需要每次都修改.\n设置完毕后就可以通过自己的域名访问博客.\nstep5 新增，删除或者修改博客 所有的博客都保存在content/posts中（当然也可改名为blogs等，无伤大雅）.修改完毕后需要回退到项目文件夹.com下，cmd执行hugo命令重新编译，并进入public文件夹输入以下代码上传\ngit add -A git commit -m\u0026quot;edit\u0026quot; git push  step6 补充 有些博客模板在本地运行没有问题，但是绑定域名后出错，需要仔细筛选.\nHugo官网：https://gohugo.io/content-management 要学会查找官方文档，解决开发中遇到的问题.\n","date":"2022-05-12","permalink":"https://wudaore.github.io/post/creating-a-boke/","tags":["test","others"],"title":"windows下使用hugo搭建博客"},{"content":"\r\rThis post is for in-post APlayer test, above is previous post-player.\nMusic files are all downloaded from Free Music Archive.\nSingle file \r\rMultiple files You can open the playlist to check other musics.\n\r\rSpaces between multiple items can be omited.\n","date":"2021-01-10","permalink":"https://wudaore.github.io/post/aplayer-test/","tags":["test","aplayer"],"title":"In-post APlayer Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot; /\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item   First Sub-item Second Sub-item  Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-05-11","permalink":"https://wudaore.github.io/post/markdown-syntax/","tags":["markdown","css","html","themes"],"title":"Markdown Syntax"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }}\r{{ partial \u0026quot;math.html\u0026quot; . }}\r{{ end }}\r  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files.  Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $ \\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887… $\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2020-03-08","permalink":"https://wudaore.github.io/post/math-typesetting/","tags":null,"title":"Math Typesetting"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09","permalink":"https://wudaore.github.io/post/placeholder-text/","tags":["markdown","text"],"title":"Placeholder Text"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n:s ee_no_evil:[Remove the space] 🙈 :h ear_no_evil:[Remove the space] 🙉 :s peak_no_evil:[Remove the space] 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji {\rfont-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;\r}\r","date":"2019-03-05","permalink":"https://wudaore.github.io/post/emoji-support/","tags":["emoji"],"title":"Emoji Support"},{"content":"The following is part of the CJK text, this page is for test use only.\nCJK Radicals Supplement ⺀ ⺁ ⺂ ⺃ ⺄ ⺅ ⺆ ⺇ ⺈ ⺉ ⺊ ⺋ ⺌ ⺍ ⺎ ⺏ ⺐ ⺑ ⺒ ⺓ ⺔ ⺕ ⺖ ⺗ ⺘ ⺙ ⺛ ⺜ ⺝ ⺞ ⺟ ⺠ ⺡ ⺢ ⺣ ⺤ ⺥ ⺦ ⺧ ⺨ ⺩ ⺪ ⺫ ⺬ ⺭ ⺮ ⺯ ⺰ ⺱ ⺲ ⺳ ⺴ ⺵ ⺶ ⺷ ⺸ ⺹ ⺺ ⺻ ⺼ ⺽ ⺾ ⺿ ⻀ ⻁ ⻂ ⻃ ⻄ ⻅ ⻆ ⻇ ⻈ ⻉ ⻊ ⻋ ⻌ ⻍ ⻎ ⻏ ⻐ ⻑ ⻒ ⻓ ⻔ ⻕ ⻖ ⻗ ⻘ ⻙ ⻚ ⻛ ⻜ ⻝ ⻞ ⻟ ⻠ ⻡ ⻢ ⻣ ⻤ ⻥ ⻦ ⻧ ⻨ ⻩ ⻪ ⻫ ⻬ ⻭ ⻮ ⻯ ⻰ ⻱ ⻲ ⻳\nKangxi Radicals ⼀ ⼁ ⼂ ⼃ ⼄ ⼅ ⼆ ⼇ ⼈ ⼉ ⼊ ⼋ ⼌ ⼍ ⼎ ⼏ ⼐ ⼑ ⼒ ⼓ ⼔ ⼕ ⼖ ⼗ ⼘ ⼙ ⼚ ⼛ ⼜ ⼝ ⼞ ⼟ ⼠ ⼡ ⼢ ⼣ ⼤ ⼥ ⼦ ⼧ ⼨ ⼩ ⼪ ⼫ ⼬ ⼭ ⼮ ⼯ ⼰ ⼱ ⼲ ⼳ ⼴ ⼵ ⼶ ⼷ ⼸ ⼹ ⼺ ⼻ ⼼ ⼽ ⼾ ⼿ ⽀ ⽁ ⽂ ⽃ ⽄ ⽅ ⽆ ⽇ ⽈ ⽉ ⽊ ⽋ ⽌ ⽍ ⽎ ⽏ ⽐ ⽑ ⽒ ⽓ ⽔ ⽕ ⽖ ⽗ ⽘ ⽙ ⽚ ⽛ ⽜ ⽝ ⽞ ⽟ ⽠ ⽡ ⽢ ⽣ ⽤ ⽥ ⽦ ⽧ ⽨ ⽩ ⽪ ⽫ ⽬ ⽭ ⽮ ⽯ ⽰ ⽱ ⽲ ⽳ ⽴ ⽵ ⽶ ⽷ ⽸ ⽹ ⽺ ⽻ ⽼ ⽽ ⽾ ⽿ \u0026hellip;\nCJK Symbols and Punctuation 、 。 〃 〄 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〓 〔 〕 〖 〗 〘 〙 〚 〛 〜 〝 〞 〟 〠 〡 〢 〣 〤 〥 〦 〧 〨 〩 〪 〫 〬 〭 〮 〯 〰 〱 〲 〳 〴 〵 〶 〷 〸 〹 〺 〻 〼 〽 〾 〿\nHiragana ぁ あ ぃ い ぅ う ぇ え ぉ お か が き ぎ く ぐ け げ こ ご さ ざ し じ す ず せ ぜ そ ぞ た だ ち ぢ っ つ づ て で と ど な に ぬ ね の は ば ぱ ひ び ぴ ふ ぶ ぷ へ べ ぺ ほ ぼ ぽ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ ゎ わ ゐ ゑ を ん ゔ ゕ ゖ ゙ ゚ ゛ ゜ ゝ ゞ ゟ\nKatakana ゠ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ ガ キ ギ ク グ ケ ゲ コ ゴ サ ザ シ ジ ス ズ セ ゼ ソ ゾ タ ダ チ ヂ ッ ツ ヅ テ デ ト ド ナ ニ ヌ ネ ノ ハ バ パ ヒ ビ ピ フ ブ プ ヘ ベ ペ ホ ボ ポ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ヮ ワ ヰ ヱ ヲ ン ヴ ヵ ヶ ヷ ヸ ヹ ヺ ・ ー ヽ ヾ ヿ\nBopomofo ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ ㄚ ㄛ ㄜ ㄝ ㄞ ㄟ ㄠ ㄡ ㄢ ㄣ ㄤ ㄥ ㄦ ㄧ ㄨ ㄩ ㄪ ㄫ ㄬ\nHangul Compatibility Jamo ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄸ ㄹ ㄺ ㄻ ㄼ ㄽ ㄾ ㄿ ㅀ ㅁ ㅂ ㅃ ㅄ ㅅ ㅆ ㅇ ㅈ ㅉ ㅊ ㅋ ㅌ ㅍ ㅎ ㅏ ㅐ ㅑ ㅒ ㅓ ㅔ ㅕ ㅖ ㅗ ㅘ ㅙ ㅚ ㅛ ㅜ ㅝ ㅞ ㅟ ㅠ ㅡ ㅢ ㅣ ㅤ ㅥ ㅦ ㅧ ㅨ ㅩ ㅪ ㅫ ㅬ ㅭ ㅮ ㅯ ㅰ ㅱ ㅲ ㅳ ㅴ ㅵ ㅶ ㅷ ㅸ ㅹ ㅺ ㅻ ㅼ ㅽ ㅾ ㅿ ㆀ ㆁ ㆂ ㆃ ㆄ ㆅ ㆆ ㆇ ㆈ ㆉ ㆊ ㆋ ㆌ ㆍ ㆎ\nKanbun ㆐ ㆑ ㆒ ㆓ ㆔ ㆕ ㆖ ㆗ ㆘ ㆙ ㆚ ㆛ ㆜ ㆝ ㆞ ㆟\nBopomofo Extended ㆠ ㆡ ㆢ ㆣ ㆤ ㆥ ㆦ ㆧ ㆨ ㆩ ㆪ ㆫ ㆬ ㆭ ㆮ ㆯ ㆰ ㆱ ㆲ ㆳ ㆴ ㆵ ㆶ ㆷ\nKatakana Phonetic Extensions ㇰ ㇱ ㇲ ㇳ ㇴ ㇵ ㇶ ㇷ ㇸ ㇹ ㇺ ㇻ ㇼ ㇽ ㇾ ㇿ\nEnclosed CJK Letters and Months ㈀ ㈁ ㈂ ㈃ ㈄ ㈅ ㈆ ㈇ ㈈ ㈉ ㈊ ㈋ ㈌ ㈍ ㈎ ㈏ ㈐ ㈑ ㈒ ㈓ ㈔ ㈕ ㈖ ㈗ ㈘ ㈙ ㈚ ㈛ ㈜ ㈠ ㈡ ㈢ ㈣ ㈤ ㈥ ㈦ ㈧ ㈨ ㈩ ㈪ ㈫ ㈬ ㈭ ㈮ ㈯ ㈰ ㈱ ㈲ ㈳ ㈴ ㈵ ㈶ ㈷ ㈸ ㈹ ㈺ ㈻ ㈼ ㈽ ㈾ ㈿ ㉀ ㉁ ㉂ ㉃ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㉠ ㉡ ㉢ ㉣ ㉤ ㉥ ㉦ ㉧ ㉨ ㉩ ㉪ ㉫ ㉬ ㉭ ㉮ ㉯ ㉰ ㉱ ㉲ ㉳ ㉴ ㉵ ㉶ ㉷ ㉸ ㉹ ㉺ ㉻ ㉿ ㊀ ㊁ ㊂ ㊃ ㊄ ㊅ ㊆ ㊇ ㊈ ㊉ ㊊ ㊋ ㊌ ㊍ ㊎ ㊏ ㊐ ㊑ ㊒ \u0026hellip;\nCJK Compatibility ㌀ ㌁ ㌂ ㌃ ㌄ ㌅ ㌆ ㌇ ㌈ ㌉ ㌊ ㌋ ㌌ ㌍ ㌎ ㌏ ㌐ ㌑ ㌒ ㌓ ㌔ ㌕ ㌖ ㌗ ㌘ ㌙ ㌚ ㌛ ㌜ ㌝ ㌞ ㌟ ㌠ ㌡ ㌢ ㌣ ㌤ ㌥ ㌦ ㌧ ㌨ ㌩ ㌪ ㌫ ㌬ ㌭ ㌮ ㌯ ㌰ ㌱ ㌲ ㌳ ㌴ ㌵ ㌶ ㌷ ㌸ ㌹ ㌺ ㌻ ㌼ ㌽ ㌾ ㌿ ㍀ ㍁ ㍂ ㍃ ㍄ ㍅ ㍆ ㍇ ㍈ ㍉ ㍊ ㍋ ㍌ ㍍ ㍎ ㍏ ㍐ ㍑ ㍒ ㍓ ㍔ ㍕ ㍖ ㍗ ㍘ ㍙ ㍚ ㍛ ㍜ ㍝ ㍞ ㍟ ㍠ ㍡ ㍢ ㍣ ㍤ ㍥ ㍦ ㍧ ㍨ ㍩ ㍪ ㍫ ㍬ ㍭ ㍮ ㍯ ㍰ ㍱ ㍲ ㍳ ㍴ ㍵ ㍶ ㍻ ㍼ ㍽ ㍾ ㍿ ㎀ ㎁ ㎂ ㎃ \u0026hellip;\nCJK Unified Ideographs Extension A 㐀 㐁 㐂 㐃 㐄 㐅 㐆 㐇 㐈 㐉 㐊 㐋 㐌 㐍 㐎 㐏 㐐 㐑 㐒 㐓 㐔 㐕 㐖 㐗 㐘 㐙 㐚 㐛 㐜 㐝 㐞 㐟 㐠 㐡 㐢 㐣 㐤 㐥 㐦 㐧 㐨 㐩 㐪 㐫 㐬 㐭 㐮 㐯 㐰 㐱 㐲 㐳 㐴 㐵 㐶 㐷 㐸 㐹 㐺 㐻 㐼 㐽 㐾 㐿 㑀 㑁 㑂 㑃 㑄 㑅 㑆 㑇 㑈 㑉 㑊 㑋 㑌 㑍 㑎 㑏 㑐 㑑 㑒 㑓 㑔 㑕 㑖 㑗 㑘 㑙 㑚 㑛 㑜 㑝 㑞 㑟 㑠 㑡 㑢 㑣 㑤 㑥 㑦 㑧 㑨 㑩 㑪 㑫 㑬 㑭 㑮 㑯 㑰 㑱 㑲 㑳 㑴 㑵 㑶 㑷 㑸 㑹 㑺 㑻 㑼 㑽 㑾 㑿 \u0026hellip;\nCJK Unified Ideographs 一 丁 丂 七 丄 丅 丆 万 丈 三 上 下 丌 不 与 丏 丐 丑 丒 专 且 丕 世 丗 丘 丙 业 丛 东 丝 丞 丟 丠 両 丢 丣 两 严 並 丧 丨 丩 个 丫 丬 中 丮 丯 丰 丱 串 丳 临 丵 丶 丷 丸 丹 为 主 丼 丽 举 丿 乀 乁 乂 乃 乄 久 乆 乇 么 义 乊 之 乌 乍 乎 乏 乐 乑 乒 乓 乔 乕 乖 乗 乘 乙 乚 乛 乜 九 乞 也 习 乡 乢 乣 乤 乥 书 乧 乨 乩 乪 乫 乬 乭 乮 乯 买 乱 乲 乳 乴 乵 乶 乷 乸 乹 乺 乻 乼 乽 乾 乿 \u0026hellip;\nHangul Syllables 가 각 갂 갃 간 갅 갆 갇 갈 갉 갊 갋 갌 갍 갎 갏 감 갑 값 갓 갔 강 갖 갗 갘 같 갚 갛 개 객 갞 갟 갠 갡 갢 갣 갤 갥 갦 갧 갨 갩 갪 갫 갬 갭 갮 갯 갰 갱 갲 갳 갴 갵 갶 갷 갸 갹 갺 갻 갼 갽 갾 갿 걀 걁 걂 걃 걄 걅 걆 걇 걈 걉 걊 걋 걌 걍 걎 걏 걐 걑 걒 걓 걔 걕 걖 걗 걘 걙 걚 걛 걜 걝 걞 걟 걠 걡 걢 걣 걤 걥 걦 걧 걨 걩 걪 걫 걬 걭 걮 걯 거 걱 걲 걳 건 걵 걶 걷 걸 걹 걺 걻 걼 걽 걾 걿 \u0026hellip;\nCJK Compatibility Ideographs 豈 更 車 賈 滑 串 句 龜 龜 契 金 喇 奈 懶 癩 羅 蘿 螺 裸 邏 樂 洛 烙 珞 落 酪 駱 亂 卵 欄 爛 蘭 鸞 嵐 濫 藍 襤 拉 臘 蠟 廊 朗 浪 狼 郎 來 冷 勞 擄 櫓 爐 盧 老 蘆 虜 路 露 魯 鷺 碌 祿 綠 菉 錄 鹿 論 壟 弄 籠 聾 牢 磊 賂 雷 壘 屢 樓 淚 漏 累 縷 陋 勒 肋 凜 凌 稜 綾 菱 陵 讀 拏 樂 諾 丹 寧 怒 率 異 北 磻 便 復 不 泌 數 索 參 塞 省 葉 說 殺 辰 沈 拾 若 掠 略 亮 兩 凉 梁 糧 良 諒 量 勵 \u0026hellip;\nCJK Compatibility Forms ︰ ︱ ︲ ︳ ︴ ︵ ︶ ︷ ︸ ︹ ︺ ︻ ︼ ︽ ︾ ︿ ﹀ ﹁ ﹂ ﹃ ﹄ ﹅ ﹆ ﹉ ﹊ ﹋ ﹌ ﹍ ﹎ ﹏\n","date":"2018-03-09","permalink":"https://wudaore.github.io/post/cjk-unicode-test/","tags":["test","cjk"],"title":"CJK Unicode Test"},{"content":"本文内容无实际意义，由狗屁不通文章生成器自动生成，不代表作者本人观点。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，到底应该如何实现。马克思在不经意间这样说过，一切节省，归根到底都归结为时间的节省。这不禁令我深思。马克思曾经说过，一切节省，归根到底都归结为时间的节省。\n带着这句话，我们还要更加慎重的审视这个问题：对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如何，我们不得不面对一个非常尴尬的事实，那就是，亚伯拉罕·林肯曾经提到过，你活了多少岁不算什么，重要的是你是如何度过这些岁月的。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我认为，在这种困难的抉择下，本人思来想去，寝食难安。所谓一段代码写一天，关键是一段代码写一天需要如何写。裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。总结的来说，带着这些问题，我们来审视一下一段代码写一天。西班牙曾经说过，自知之明是最难得的知识。这不禁令我深思。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。贝多芬在不经意间这样说过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。\n每个人都不得不面对这些问题。在面对这种问题时，歌德曾经提到过，读一本好书，就如同和一个高尚的人在交谈。这似乎解答了我的疑惑。歌德在不经意间这样说过，读一本好书，就如同和一个高尚的人在交谈。我希望诸位也能好好地体会这句话。从这个角度来看，一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。现在，解决一段代码写一天的问题，是非常非常重要的。所以，马克思曾经提到过，一切节省，归根到底都归结为时间的节省。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。我希望诸位也能好好地体会这句话。既然如此，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。白哲特曾经说过，坚强的信念能赢得强者的心，并使他们变得更坚强。带着这句话，我们还要更加慎重的审视这个问题：富勒在不经意间这样说过，苦难磨炼一些人，也毁灭另一些人。带着这句话，我们还要更加慎重的审视这个问题：这样看来，一般来讲，我们都必须务必慎重的考虑考虑。从这个角度来看，从这个角度来看，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。而这些并不是完全重要，更加重要的问题是。\n带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。经过上述讨论，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。佚名曾经提到过，感激每一个新的挑战，因为它会锻造你的意志和品格。这句话语虽然很短，但令我浮想联翩。现在，解决一段代码写一天的问题，是非常非常重要的。所以，每个人都不得不面对这些问题。在面对这种问题时，我们都知道，只要有意义，那么就必须慎重考虑。经过上述讨论。\n了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一般来说，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。笛卡儿在不经意间这样说过，阅读一切好书如同和过去最杰出的人谈话。我希望诸位也能好好地体会这句话。在这种困难的抉择下，本人思来想去，寝食难安。问题的关键究竟为何？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。莎士比亚曾经说过，抛弃时间的人，时间也抛弃他。我希望诸位也能好好地体会这句话。\n笛卡儿说过一句富有哲理的话，阅读一切好书如同和过去最杰出的人谈话。这句话语虽然很短，但令我浮想联翩。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这启发了我，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。一段代码写一天因何而发生？一段代码写一天因何而发生？我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。邓拓曾经说过，越是没有本领的就越加自命不凡。这启发了我，从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说。\n对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。总结的来说，从这个角度来看，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。德谟克利特说过一句富有哲理的话，节制使快乐增加并使享受加强。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们都知道，只要有意义，那么就必须慎重考虑。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。史美尔斯曾经提到过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这句话语虽然很短，但令我浮想联翩。一般来讲，我们都必须务必慎重的考虑考虑。既然如此，我们都知道，只要有意义，那么就必须慎重考虑。这样看来，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。德国曾经提到过，只有在人群中间，才能认识自己。我希望诸位也能好好地体会这句话。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，从这个角度来看，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。莎士比亚在不经意间这样说过，意志命运往往背道而驰，决心到最后会全部推倒。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。那么，一段代码写一天，到底应该如何实现。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天，发生了会如何，不发生又会如何。在这种困难的抉择下，本人思来想去，寝食难安。塞涅卡在不经意间这样说过，生命如同寓言，其价值不在与长短，而在与内容。这不禁令我深思。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。所谓一段代码写一天，关键是一段代码写一天需要如何写。我认为，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。在这种困难的抉择下，本人思来想去，寝食难安。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，到底应该如何实现。一段代码写一天，发生了会如何，不发生又会如何。既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。总结的来说，那么，带着这些问题，我们来审视一下一段代码写一天。要想清楚，一段代码写一天，到底是一种怎么样的存在。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天，发生了会如何，不发生又会如何。西班牙曾经提到过，自己的鞋子，自己知道紧在哪里。带着这句话，我们还要更加慎重的审视这个问题：既然如何，每个人都不得不面对这些问题。在面对这种问题时，问题的关键究竟为何？从这个角度来看，既然如此，在这种困难的抉择下，本人思来想去，寝食难安。我认为。\n一段代码写一天因何而发生？我们不得不面对一个非常尴尬的事实，那就是，洛克在不经意间这样说过，学到很多东西的诀窍，就是一下子不要学很多。这不禁令我深思。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。一般来说，而这些并不是完全重要，更加重要的问题是，问题的关键究竟为何？而这些并不是完全重要，更加重要的问题是，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一段代码写一天因何而发生？所谓一段代码写一天，关键是一段代码写一天需要如何写。一段代码写一天因何而发生？这样看来，一段代码写一天，到底应该如何实现。卡耐基说过一句富有哲理的话，一个不注意小事情的人，永远不会成就大事业。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，到底应该如何实现。既然如此，而这些并不是完全重要，更加重要的问题是。\n冯学峰说过一句富有哲理的话，当一个人用工作去迎接光明，光明很快就会来照耀着他。我希望诸位也能好好地体会这句话。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。而这些并不是完全重要，更加重要的问题是，那么，要想清楚，一段代码写一天，到底是一种怎么样的存在。从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。在这种困难的抉择下，本人思来想去，寝食难安。我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。问题的关键究竟为何？本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。鲁巴金曾经提到过，读书是在别人思想的帮助下，建立起自己的思想。这不禁令我深思。莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这启发了我。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。海贝尔曾经说过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。这似乎解答了我的疑惑。德国曾经说过，只有在人群中间，才能认识自己。这不禁令我深思。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。这样看来，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。克劳斯·莫瑟爵士在不经意间这样说过，教育需要花费钱，而无知也是一样。这似乎解答了我的疑惑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。一般来讲，我们都必须务必慎重的考虑考虑。一般来讲，我们都必须务必慎重的考虑考虑。一段代码写一天因何而发生？对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。而这些并不是完全重要，更加重要的问题是，一般来讲，我们都必须务必慎重的考虑考虑。我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。博说过一句富有哲理的话，一次失败，只是证明我们成功的决心还够坚强。维这启发了我，从这个角度来看，问题的关键究竟为何？这样看来，既然如此，所谓一段代码写一天，关键是一段代码写一天需要如何写。可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。总结的来说，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。经过上述讨论，史美尔斯说过一句富有哲理的话，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。带着这句话，我们还要更加慎重的审视这个问题：在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。冯学峰曾经说过，当一个人用工作去迎接光明，光明很快就会来照耀着他。这句话语虽然很短，但令我浮想联翩。\n所谓一段代码写一天，关键是一段代码写一天需要如何写。米歇潘在不经意间这样说过，生命是一条艰险的峡谷，只有勇敢的人才能通过。我希望诸位也能好好地体会这句话。问题的关键究竟为何？培根在不经意间这样说过，合理安排时间，就等于节约时间。这句话语虽然很短，但令我浮想联翩。吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这启发了我，这样看来，既然如何，吉格·金克拉说过一句富有哲理的话，如果你能做梦，你就能实现它。这句话语虽然很短，但令我浮想联翩。所谓一段代码写一天，关键是一段代码写一天需要如何写。我们不得不面对一个非常尴尬的事实，那就是，在这种困难的抉择下，本人思来想去，寝食难安。要想清楚，一段代码写一天，到底是一种怎么样的存在。了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。我们不得不面对一个非常尴尬的事实，那就是，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。所谓一段代码写一天，关键是一段代码写一天需要如何写。而这些并不是完全重要，更加重要的问题是，左拉在不经意间这样说过，生活的道路一旦选定，就要勇敢地走到底，决不回头。这似乎解答了我的疑惑。一般来讲，我们都必须务必慎重的考虑考虑。一般来说，经过上述讨论，总结的来说，一般来讲，我们都必须务必慎重的考虑考虑。这样看来，既然如此，一般来说，一段代码写一天，发生了会如何，不发生又会如何。那么，既然如此，每个人都不得不面对这些问题。在面对这种问题时，莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。带着这句话，我们还要更加慎重的审视这个问题：韩非在不经意间这样说过，内外相应，言行相称。这似乎解答了我的疑惑。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。我们不得不面对一个非常尴尬的事实，那就是，总结的来说，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，我认为，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。我们都知道，只要有意义，那么就必须慎重考虑。培根在不经意间这样说过，合理安排时间，就等于节约时间。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。从这个角度来看，一般来讲，我们都必须务必慎重的考虑考虑。那么，所谓一段代码写一天，关键是一段代码写一天需要如何写。歌德曾经说过，意志坚强的人能把世界放在手中像泥块一样任意揉捏。带着这句话，我们还要更加慎重的审视这个问题：这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。每个人都不得不面对这些问题。在面对这种问题时，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。塞涅卡曾经提到过，生命如同寓言，其价值不在与长短，而在与内容。这句话语虽然很短，但令我浮想联翩。\n我们都知道，只要有意义，那么就必须慎重考虑。带着这些问题，我们来审视一下一段代码写一天。笛卡儿曾经说过，我的努力求学没有得到别的好处，只不过是愈来愈发觉自己的无知。带着这句话，我们还要更加慎重的审视这个问题：现在，解决一段代码写一天的问题，是非常非常重要的。所以，本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这启发了我，现在，解决一段代码写一天的问题，是非常非常重要的。所以，既然如何，我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。易卜生说过一句富有哲理的话，伟大的事业，需要决心，能力，组织和责任感。带着这句话，我们还要更加慎重的审视这个问题：我们都知道，只要有意义，那么就必须慎重考虑。一段代码写一天，发生了会如何，不发生又会如何。带着这些问题，我们来审视一下一段代码写一天。我们不得不面对一个非常尴尬的事实，那就是，我们不得不面对一个非常尴尬的事实，那就是。\n我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，在这种困难的抉择下，本人思来想去，寝食难安。在这种困难的抉择下，本人思来想去，寝食难安。我们不得不面对一个非常尴尬的事实，那就是，现在，解决一段代码写一天的问题，是非常非常重要的。所以，俾斯麦说过一句富有哲理的话，失败是坚忍的最后考验。带着这句话，我们还要更加慎重的审视这个问题：可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。达·芬奇在不经意间这样说过，大胆和坚定的决心能够抵得上武器的精良。这似乎解答了我的疑惑。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。而这些并不是完全重要，更加重要的问题是，我认为，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n问题的关键究竟为何？我们不得不面对一个非常尴尬的事实，那就是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。我认为，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。带着这些问题，我们来审视一下一段代码写一天。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。问题的关键究竟为何？从这个角度来看，我们都知道，只要有意义，那么就必须慎重考虑。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。那么，这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，从这个角度来看，带着这些问题，我们来审视一下一段代码写一天。一段代码写一天，发生了会如何，不发生又会如何。\n培根在不经意间这样说过，要知道对好事的称颂过于夸大，也会招来人们的反感轻蔑和嫉妒。这句话语虽然很短，但令我浮想联翩。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。既然如何，而这些并不是完全重要，更加重要的问题是，这样看来，贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这不禁令我深思。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一段代码写一天，发生了会如何，不发生又会如何。问题的关键究竟为何？卡耐基在不经意间这样说过，我们若已接受最坏的，就再没有什么损失。我希望诸位也能好好地体会这句话。一段代码写一天的发生，到底需要如何做到，不一段代码写一天的发生，又会如何产生。现在，解决一段代码写一天的问题，是非常非常重要的。所以，要想清楚，一段代码写一天，到底是一种怎么样的存在。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。我希望诸位也能好好地体会这句话。既然如何，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。海贝尔说过一句富有哲理的话，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。我希望诸位也能好好地体会这句话。\n这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。经过上述讨论，我认为，莫扎特曾经提到过，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。经过上述讨论，我们不得不面对一个非常尴尬的事实，那就是，达尔文说过一句富有哲理的话，敢于浪费哪怕一个钟头时间的人，说明他还不懂得珍惜生命的全部价值。这句话语虽然很短，但令我浮想联翩。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。一般来讲，我们都必须务必慎重的考虑考虑。罗曼·罗兰曾经说过，只有把抱怨环境的心情，化为上进的力量，才是成功的保证。这启发了我，而这些并不是完全重要，更加重要的问题是，总结的来说，这样看来，每个人都不得不面对这些问题。在面对这种问题时，一般来讲，我们都必须务必慎重的考虑考虑。既然如此，总结的来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n德国曾经提到过，只有在人群中间，才能认识自己。带着这句话，我们还要更加慎重的审视这个问题：既然如此，了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。杰纳勒尔·乔治·S·巴顿曾经提到过，接受挑战，就可以享受胜利的喜悦。这不禁令我深思。一段代码写一天因何而发生？问题的关键究竟为何？海贝尔曾经提到过，人生就是学校。在那里，与其说好的教师是幸福，不如说好的教师是不幸。带着这句话，我们还要更加慎重的审视这个问题。\n经过上述讨论，我们都知道，只要有意义，那么就必须慎重考虑。本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。我认为，一般来说，经过上述讨论，我认为，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。问题的关键究竟为何？一段代码写一天因何而发生？莎士比亚曾经提到过，本来无望的事，大胆尝试，往往能成功。这似乎解答了我的疑惑。这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。一段代码写一天，到底应该如何实现。我们都知道，只要有意义，那么就必须慎重考虑。总结的来说，那么，叔本华在不经意间这样说过，普通人只想到如何度过时间，有才能的人设法利用时间。这句话语虽然很短，但令我浮想联翩。总结的来说，经过上述讨论，从这个角度来看，一段代码写一天，发生了会如何，不发生又会如何。总结的来说，所谓一段代码写一天，关键是一段代码写一天需要如何写。既然如何，一段代码写一天，到底应该如何实现。\n问题的关键究竟为何？一般来讲，我们都必须务必慎重的考虑考虑。歌德说过一句富有哲理的话，读一本好书，就如同和一个高尚的人在交谈。这不禁令我深思。迈克尔·F·斯特利曾经提到过，最具挑战性的挑战莫过于提升自我。这不禁令我深思。在这种困难的抉择下，本人思来想去，寝食难安。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。要想清楚，一段代码写一天，到底是一种怎么样的存在。莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这句话语虽然很短，但令我浮想联翩。\n这样看来，每个人都不得不面对这些问题。在面对这种问题时，从这个角度来看，现在，解决一段代码写一天的问题，是非常非常重要的。所以，我们不得不面对一个非常尴尬的事实，那就是，富兰克林曾经提到过，读书是易事，思索是难事，但两者缺一，便全无用处。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。康德曾经说过，既然我已经踏上这条道路，那么，任何东西都不应妨碍我沿着这条路走下去。我希望诸位也能好好地体会这句话。一段代码写一天，到底应该如何实现。而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。带着这些问题，我们来审视一下一段代码写一天。既然如此，我们不得不面对一个非常尴尬的事实，那就是，一般来说，美华纳曾经提到过，勿问成功的秘诀为何，且尽全力做你应该做的事吧。这句话语虽然很短，但令我浮想联翩。我们都知道，只要有意义，那么就必须慎重考虑。叔本华曾经说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。我希望诸位也能好好地体会这句话。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，郭沫若曾经说过，形成天才的决定因素应该是勤奋。我希望诸位也能好好地体会这句话。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。一般来讲，我们都必须务必慎重的考虑考虑。在这种困难的抉择下，本人思来想去，寝食难安。经过上述讨论，一段代码写一天，发生了会如何，不发生又会如何。从这个角度来看，一段代码写一天因何而发生？了解清楚一段代码写一天到底是一种怎么样的存在，是解决一切问题的关键。既然如何。\n而这些并不是完全重要，更加重要的问题是，而这些并不是完全重要，更加重要的问题是，既然如何，要想清楚，一段代码写一天，到底是一种怎么样的存在。那么，那么，我认为，经过上述讨论，既然如此，对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。卡耐基曾经提到过，一个不注意小事情的人，永远不会成就大事业。这句话语虽然很短，但令我浮想联翩。黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这似乎解答了我的疑惑。一般来说，雷锋在不经意间这样说过，自己活着，就是为了使别人过得更美好。这启发了我，一般来说，而这些并不是完全重要，更加重要的问题是。\n裴斯泰洛齐在不经意间这样说过，今天应做的事没有做，明天再早也是耽误了。这句话语虽然很短，但令我浮想联翩。爱迪生曾经说过，失败也是我需要的，它和成功对我一样有价值。带着这句话，我们还要更加慎重的审视这个问题：带着这些问题，我们来审视一下一段代码写一天。既然如此，富勒曾经提到过，苦难磨炼一些人，也毁灭另一些人。我希望诸位也能好好地体会这句话。培根说过一句富有哲理的话，深窥自己的心，而后发觉一切的奇迹在你自己。这似乎解答了我的疑惑。问题的关键究竟为何。\n既然如何，一般来讲，我们都必须务必慎重的考虑考虑。经过上述讨论，普列姆昌德曾经说过，希望的灯一旦熄灭，生活刹那间变成了一片黑暗。这不禁令我深思。总结的来说，在这种困难的抉择下，本人思来想去，寝食难安。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这启发了我，那么，在这种困难的抉择下，本人思来想去，寝食难安。那么，带着这些问题，我们来审视一下一段代码写一天。\n我认为，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。生活中，若一段代码写一天出现了，我们就不得不考虑它出现了的事实。既然如何，黑塞曾经提到过，有勇气承担命运这才是英雄好汉。这不禁令我深思。杰纳勒尔·乔治·S·巴顿说过一句富有哲理的话，接受挑战，就可以享受胜利的喜悦。这似乎解答了我的疑惑。在这种困难的抉择下，本人思来想去，寝食难安。\n可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。拉罗什福科在不经意间这样说过，我们唯一不会改正的缺点是软弱。这不禁令我深思。既然如此，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。一般来讲，我们都必须务必慎重的考虑考虑。对我个人而言，一段代码写一天不仅仅是一个重大的事件，还可能会改变我的人生。既然如此，史美尔斯曾经说过，书籍把我们引入最美好的社会，使我们认识各个时代的伟大智者。这不禁令我深思。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一般来讲，我们都必须务必慎重的考虑考虑。现在，解决一段代码写一天的问题，是非常非常重要的。所以，一段代码写一天，到底应该如何实现。贝多芬曾经提到过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。这似乎解答了我的疑惑。阿卜·日·法拉兹曾经说过，学问是异常珍贵的东西，从任何源泉吸收都不可耻。这似乎解答了我的疑惑。我们不得不面对一个非常尴尬的事实，那就是。\n池田大作在不经意间这样说过，不要回避苦恼和困难，挺起身来向它挑战，进而克服它。这句话语虽然很短，但令我浮想联翩。叔本华在不经意间这样说过，意志是一个强壮的盲人，倚靠在明眼的跛子肩上。带着这句话，我们还要更加慎重的审视这个问题：一段代码写一天，发生了会如何，不发生又会如何。就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，就我个人来说，一段代码写一天对我的意义，不能不说非常重大。一般来说，可是，即使是这样，一段代码写一天的出现仍然代表了一定的意义。\n","date":"2017-04-01","permalink":"https://wudaore.github.io/post/wtf-article/","tags":["test","wtf"],"title":"纯简体中文测试文章"}]