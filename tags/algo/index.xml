<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>algo on wudao的博客</title>
    <link>https://wudaore.github.io/tags/algo/</link>
    <description>Recent content in algo on wudao的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 22 Sep 2022 00:00:00 +0800</lastBuildDate><atom:link href="https://wudaore.github.io/tags/algo/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>李宏毅-深度学习课程学习笔记(3)</title>
      <link>https://wudaore.github.io/post/deeplearning-lhy3/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/deeplearning-lhy3/</guid>
      <description>0. 序 继续李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现. 1. receptive field 做图像识别时，让Neuron输入整个图片未免有些现实.给每个Neuron设置一个receptive fiel</description>
    </item>
    
    <item>
      <title>李宏毅-深度学习课程学习笔记(2)</title>
      <link>https://wudaore.github.io/post/deeplearning-lhy2/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/deeplearning-lhy2/</guid>
      <description>0. 序 继续李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现. 1. 通过正则化解决过拟合 1.1 正则化的基本原理（以岭回归为例） 正则化，岭回归等内容在之前的博客已经提及，原理不加赘述.</description>
    </item>
    
    <item>
      <title>李宏毅-深度学习课程学习笔记(1)</title>
      <link>https://wudaore.github.io/post/deeplearning-lhy1/</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/deeplearning-lhy1/</guid>
      <description>0. 序 开始李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现. 1. pytorch中dataset的使用 要使用自己的数据集，首先要导入pytorch对应的库 from torch.utils.data import DataLoader,Dataset DataLoa</description>
    </item>
    
    <item>
      <title>CART决策树</title>
      <link>https://wudaore.github.io/post/cart/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/cart/</guid>
      <description>目录 CART树基本原理 CART建树 CART剪枝 CART回归树 CART模型树 总结 ps. 1. CART树基本原理 和ID3的切割特征所有取值相比，CART树使用了二元切割法.即，大于给定值的走左子树，小于的则走右子</description>
    </item>
    
    <item>
      <title>特征降维-奇异值分解SVD</title>
      <link>https://wudaore.github.io/post/svd/</link>
      <pubDate>Sat, 25 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/svd/</guid>
      <description>目录 SVD基本原理 SVD数学推导 相似度 SVD《实战》代码 总结 ps. 1. SVD基本原理 SVD就是将原始的数据集矩阵Data分解成三个矩阵U、Σ 和VT. 1.1 矩阵的拉伸和旋转 在了解SVD之前 首先要知道矩阵的拉伸和旋</description>
    </item>
    
    <item>
      <title>特征降维-主成分分析PCA</title>
      <link>https://wudaore.github.io/post/pca/</link>
      <pubDate>Fri, 24 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/pca/</guid>
      <description>目录 PCA基本原理 PCA数学推导 PCA代码实现 PCA接口实现 总结 ps. 1. PCA基本原理 太高维度的数据处理时会大大增加计算难度和时间.所以必要时需要对特征进行降维. PCA，主成分分析 是一种数据降维的方法. 首</description>
    </item>
    
    <item>
      <title>FP-growth算法</title>
      <link>https://wudaore.github.io/post/fp-growth/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/fp-growth/</guid>
      <description>目录 FP树基本原理 FP树源码 总结 ps. 1. FP树基本原理 在寻找频繁项集时，Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁.在数据量大时运行会很慢. 而FP-groth算法通过构造F</description>
    </item>
    
    <item>
      <title>Apriori算法实现关联分析</title>
      <link>https://wudaore.github.io/post/apriori/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/apriori/</guid>
      <description>目录 Apriori关联分析基本原理 关联分析《实战》源码 关联规则基本原理 关联规则《实战》源码 总结 ps. 1. Apriori关联分析基本原理 频繁项集是指那些经常出现在一起的物品集合.使用频繁项集和关联规则，商家可</description>
    </item>
    
    <item>
      <title>k均值聚类算法</title>
      <link>https://wudaore.github.io/post/k-means/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/k-means/</guid>
      <description>目录 基本原理 《实战》源码 模型评估 算法优化 特征降维 API接口实现 案例-消费预测 总结 ps. 1. 基本原理 原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点</description>
    </item>
    
    <item>
      <title>集成学习</title>
      <link>https://wudaore.github.io/post/ensemble-learning/</link>
      <pubDate>Wed, 08 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/ensemble-learning/</guid>
      <description>目录 基本原理 bagging boosting 总结 ps. 1. 基本原理 即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测. 对于欠拟合问题，使用b</description>
    </item>
    
    <item>
      <title>决策树学习</title>
      <link>https://wudaore.github.io/post/decision-tree/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/decision-tree/</guid>
      <description>目录 基本原理 度量方法 常用的剪枝方法 特征工程-特征提取 决策树的实现 决策树绘制 总结 ps. 1. 基本原理 决策树是一种树形结构，每个内部节点代表一个属性上的判断.每个叶节点代表一种分类结果.它的本质就是基于数据，通过</description>
    </item>
    
    <item>
      <title>逻辑回归学习</title>
      <link>https://wudaore.github.io/post/logistic-regression/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/logistic-regression/</guid>
      <description>目录 基本原理 逻辑回归的损失函数 逻辑回归的优化 随机梯度上升 分类评估方法 sklearn实现 总结 ps. 1. 基本原理 虽然叫回归但是其实解决的是二分类问题.逻辑回归的输入值为线性回归的输出值，即 之所以能解决二分类问题</description>
    </item>
    
    <item>
      <title>线性回归学习</title>
      <link>https://wudaore.github.io/post/linear-regression/</link>
      <pubDate>Thu, 02 Jun 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/linear-regression/</guid>
      <description>目录 基本原理 线性回归的损失优化 局部加权线性回归 正则化线性模型 逐步向前回归 sklearn用法 总结 ps. 1. 基本原理 1.1 分类和回归 区别在于分类问题是定性的，输出离散值（如+1，-1）.而回归问题是定量的，输出连续</description>
    </item>
    
    <item>
      <title>KNN学习</title>
      <link>https://wudaore.github.io/post/knn/</link>
      <pubDate>Sun, 29 May 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/knn/</guid>
      <description>Introduction 基本原理 各种距离 距离调参 KNN优化-KD树 数据处理（归一化和标准化） 实战&amp;ndash;鸢尾花数据集 总结 1. 基本原理 非常朴素的原理。根据距离最近的K个点来判断目标点的类别. 2. 各种距离 标准距离 汉明距离 杰卡</description>
    </item>
    
    <item>
      <title>svm学习(1)--基础概念和公式</title>
      <link>https://wudaore.github.io/post/svm1/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/svm1/</guid>
      <description>Introduction &amp;laquo;实战&amp;raquo;中的svm数学推导实在是有些晦涩难懂. bilibili视频教程：https://www.bilibili.com/video/BV1Hs411w7ci?spm_id_</description>
    </item>
    
    <item>
      <title>svm学习(2)--简易SMO算法代码理解</title>
      <link>https://wudaore.github.io/post/svm2/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/svm2/</guid>
      <description>Introduction SMO算法 即序列最小优化算法 对&amp;laquo;实战&amp;raquo;中的smoSimple函数尝试理解 def smoSimple(dataMatIn, classLabels, C, toler, maxIter): &amp;quot;&amp;quot;&amp;quot;smoSimple Args: dataMatIn 特征集合 classLabels 类别标签 C 松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。 控制</description>
    </item>
    
    <item>
      <title>svm学习(3)--SVM实现鸢尾花数据分类</title>
      <link>https://wudaore.github.io/post/svm3/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0800</pubDate>
      
      <guid>https://wudaore.github.io/post/svm3/</guid>
      <description>Introduction sklearn的SVM算法可以调整c值和核函数.本篇并未介绍核函数 导入数据和数据-标签的处理 from sklearn.svm import SVC from sklearn import datasets iris = datasets.load_iris() X = iris[&#39;data&#39;] X = iris[&#39;data&#39;][:,(2,3)] y = iris[&#39;target&#39;] SVM训练.c值设置为无穷大代表几乎不容错 使用线性核函数 setosa_or_versicolor = (y==0)|(y==1) X =</description>
    </item>
    
  </channel>
</rss>
