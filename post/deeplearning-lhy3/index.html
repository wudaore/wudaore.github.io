<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.98.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>李宏毅-深度学习课程学习笔记(3) - wudao的博客</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="深度学习, algo, pytorch" />


<meta property="og:title" content="李宏毅-深度学习课程学习笔记(3)" />
<meta name="twitter:title" content="李宏毅-深度学习课程学习笔记(3)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wudaore.github.io/post/deeplearning-lhy3/" /><meta property="og:description" content="0. 序 继续李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现. 1. receptive field 做图像识别时，让Neuron输入整个图片未免有些现实.给每个Neuron设置一个receptive fiel" />
<meta name="twitter:description" content="0. 序 继续李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现. 1. receptive field 做图像识别时，让Neuron输入整个图片未免有些现实.给每个Neuron设置一个receptive fiel" /><meta property="og:image" content="https://wudaore.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://wudaore.github.io/img/og.png" /><meta property="article:published_time" content="2022-09-22T00:00:00+08:00" /><meta property="article:modified_time" content="2022-09-22T00:00:00+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://wudaore.github.io/assets/css/fuji.min.css" />








</head>

<body
  data-theme="auto"
  data-theme-auto='true'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://wudaore.github.io/">wudao的博客</a>
            
            <span class="title-sub">钝学累功,不妨精熟</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://wudaore.github.io/post/deeplearning-lhy3/">李宏毅-深度学习课程学习笔记(3)</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-09-22</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;1642 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>&nbsp;<a href="/tags/algo">algo</a>&nbsp;<a href="/tags/pytorch">pytorch</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h2 id="0-序aa">0. 序<a></a></h2>
<p>继续李宏毅-深度学习课程的学习，并结合课程对暑假做过的项目进行深度的理解和尝试复现.</p>
<h2 id="1-receptive-fieldaa">1. receptive field<a></a></h2>
<p>做图像识别时，让Neuron输入整个图片未免有些现实.给每个Neuron设置一个receptive field，将receptive field中的内容作为Neuron的输入，每个Neuron只需要关注其receptive field中的内容即可.</p>
<p>不同Neuron的receptive field可以重叠，而同一个receptive field会有一组Neuron去守备.</p>
<h3 id="11-kernel-size">1.1 Kernel size</h3>
<p>receptive field的长乘宽就是Kernel size.一般较小（3*3）.</p>
<h3 id="12-stride">1.2 stride</h3>
<p>stride：receptive field移动的范围.超出影像范围的叫padding</p>
<p>Receptive Field 和 paramters sharing都限制了模型的弹性，增加了 model bias.两者相加就是convolutional Neural（卷积神经），运用在newwork上就是CNN.CNN在影像上有比较好的表现.</p>
<h2 id="2-cnn">2. CNN</h2>
<h3 id="21-convolution-layer">2.1 convolution layer</h3>
<p>convolution layer中有很多filter ，每个filter（举例）的大小是3*\3*Channel(视彩色或者黑白而定).每个filter的目的都是抓取一个Pattern</p>
<p>具体做法是将每一个Filter和某个receptive field相乘，不断移动.做完后会得到一个Feature map.</p>
<p>Filter的高度等于Channel数.当吧上一次的结果作为输入再次代入时，上一次有多少层，这一次的Filter高度就是多少.</p>
<h3 id="22-池化-pooling">2.2 池化 Pooling</h3>
<p>池化非是一个Layer，因为并没有做参数的相乘.它更像是如Relu一般的激活函数.</p>
<p>以max-pooling为例，在一个n*n的空间内只取最大值.</p>
<p>池化后可以经过Flatten使数据扁平化 再经过Fully 卷积神经 和softmax等.</p>
<p><img class="img-zoomable" src="/LHY/15.png" alt="png" />
</p>
<h3 id="23-image-transformation">2.3 image transformation</h3>
<p>要将图片缩放&ndash;平移&ndash;旋转。可以将图像数字化后乘以一个矩阵，再加上一个偏移量矩阵来实现.</p>
<h2 id="3-self-attention">3. self-Attention</h2>
<p>有时模型的输入会是不定长的一组向量（如声音信号，社交网络）.</p>
<p>输出可能有以下三种情况：</p>
<p>1.每个输入向量对应一个输出，输入和输出的长度一致（如词性标注）&ndash;即Sequence Labeling</p>
<p>2.一整个sequence只需要输出一个（如情感分析）</p>
<p>3.机器来决定输出多少个label（seq2seq，如语音辨识）</p>
<p><strong>self-Attention</strong>会让输入input考虑一整个sequence后再得出结果，而非考虑一个非常小的window窗口.且可调用多次&ndash;即交替使用全连接层和self-Attention，如下图所示</p>
<p><img class="img-zoomable" src="/LHY/16.png" alt="png" />
</p>
<h2 id="31-dot-product">3.1 dot-product</h2>
<p>由于需要知道sequence中的向量和输入之间的关联值α，所以需要一个模组来进行计算.</p>
<p>如dot-product做法是将输入的两个向量分别乘以不同的两个矩阵之后按比例相乘相加得到α</p>
<p>具体做法如下：</p>
<p><strong>简单来说</strong>，就是先将输入序列a乘以不同的矩阵，得到参数q,k,b.</p>
<p><img class="img-zoomable" src="/LHY/18.png" alt="png" />
</p>
<p>接着每个q都会和k去计算inner-product，得到attention分数（也即矩阵q和k相乘）之后经过softmax.</p>
<p><img class="img-zoomable" src="/LHY/19.png" alt="png" />
</p>
<p>最后将attantion矩阵乘以v矩阵，得到b</p>
<h2 id="32-变形--multi-head-self-attention">3.2 变形&ndash;multi-head self-attention</h2>
<p>如果有不同的相关性，就需要多个head.其实做法就是将原先的一个qkv变成head个qkv</p>
<p><img class="img-zoomable" src="/LHY/20.png" alt="png" />
</p>
<h2 id="33-positional-encoding">3.3 positional encoding</h2>
<p>self-Attention并没有考虑位置.但是，在做例如词性标注的任务时，位置的咨询是很重要的.这时需要通过positional encoding将它代入.</p>
<p>具体做法如下：</p>
<p>1.为每个位置设置一个vactor ei</p>
<p>2.将ei加到ai上面</p>
<h2 id="34-与cnn的差异与关联">3.4 与CNN的差异与关联</h2>
<p>CNN可看做一个简化版的self-Attention.CNN只考虑receptive中的咨询而attention考虑一整个咨询.</p>
<h2 id="35-与rnn的比较">3.5 与RNN的比较</h2>
<p>attention和RNN都考虑了整个input sequence但是attention的每一个input都考虑了整个input sequence而RNN只考虑前面最近的那个（当然也可以做双向的）.</p>
<p>另外，RNN无法平行处理所有输出.运算效率偏低.</p>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#0-序aa">0. 序<a></a></a></li>
    <li><a href="#1-receptive-fieldaa">1. receptive field<a></a></a>
      <ul>
        <li><a href="#11-kernel-size">1.1 Kernel size</a></li>
        <li><a href="#12-stride">1.2 stride</a></li>
      </ul>
    </li>
    <li><a href="#2-cnn">2. CNN</a>
      <ul>
        <li><a href="#21-convolution-layer">2.1 convolution layer</a></li>
        <li><a href="#22-池化-pooling">2.2 池化 Pooling</a></li>
        <li><a href="#23-image-transformation">2.3 image transformation</a></li>
      </ul>
    </li>
    <li><a href="#3-self-attention">3. self-Attention</a></li>
    <li><a href="#31-dot-product">3.1 dot-product</a></li>
    <li><a href="#32-变形--multi-head-self-attention">3.2 变形&ndash;multi-head self-attention</a></li>
    <li><a href="#33-positional-encoding">3.3 positional encoding</a></li>
    <li><a href="#34-与cnn的差异与关联">3.4 与CNN的差异与关联</a></li>
    <li><a href="#35-与rnn的比较">3.5 与RNN的比较</a></li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#0-序aa">0. 序<a></a></a></li>
    <li><a href="#1-receptive-fieldaa">1. receptive field<a></a></a>
      <ul>
        <li><a href="#11-kernel-size">1.1 Kernel size</a></li>
        <li><a href="#12-stride">1.2 stride</a></li>
      </ul>
    </li>
    <li><a href="#2-cnn">2. CNN</a>
      <ul>
        <li><a href="#21-convolution-layer">2.1 convolution layer</a></li>
        <li><a href="#22-池化-pooling">2.2 池化 Pooling</a></li>
        <li><a href="#23-image-transformation">2.3 image transformation</a></li>
      </ul>
    </li>
    <li><a href="#3-self-attention">3. self-Attention</a></li>
    <li><a href="#31-dot-product">3.1 dot-product</a></li>
    <li><a href="#32-变形--multi-head-self-attention">3.2 变形&ndash;multi-head self-attention</a></li>
    <li><a href="#33-positional-encoding">3.3 positional encoding</a></li>
    <li><a href="#34-与cnn的差异与关联">3.4 与CNN的差异与关联</a></li>
    <li><a href="#35-与rnn的比较">3.5 与RNN的比较</a></li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2022
                <a href="https://wudaore.github.io/">DSRKafuU</a>
                 | <a href="https://github.com/dsrkafuu/hugo-theme-fuji">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>



</body>

</html>
