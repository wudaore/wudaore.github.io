<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.98.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>k均值聚类算法 - wudao的博客</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="机器学习, algo" />


<meta property="og:title" content="k均值聚类算法" />
<meta name="twitter:title" content="k均值聚类算法" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wudaore.github.io/post/k-means/" /><meta property="og:description" content="目录 基本原理 《实战》源码 模型评估 算法优化 特征降维 API接口实现 案例-消费预测 总结 ps. 1. 基本原理 原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点" />
<meta name="twitter:description" content="目录 基本原理 《实战》源码 模型评估 算法优化 特征降维 API接口实现 案例-消费预测 总结 ps. 1. 基本原理 原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点" /><meta property="og:image" content="https://wudaore.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://wudaore.github.io/img/og.png" /><meta property="article:published_time" content="2022-06-20T00:00:00+08:00" /><meta property="article:modified_time" content="2022-06-20T00:00:00+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://wudaore.github.io/assets/css/fuji.min.css" />








</head>

<body
  data-theme="auto"
  data-theme-auto='true'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://wudaore.github.io/">wudao的博客</a>
            
            <span class="title-sub">钝学累功,不妨精熟</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://wudaore.github.io/post/k-means/">k均值聚类算法</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-06-20</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;3962 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>&nbsp;<a href="/tags/algo">algo</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h1 id="目录">目录</h1>
<h4 id="基本原理1"><a href="#1">基本原理</a></h4>
<h4 id="实战源码2"><a href="#2">《实战》源码</a></h4>
<h4 id="模型评估3"><a href="#3">模型评估</a></h4>
<h4 id="算法优化4"><a href="#4">算法优化</a></h4>
<h4 id="特征降维5"><a href="#5">特征降维</a></h4>
<h4 id="api接口实现sk"><a href="#sk">API接口实现</a></h4>
<h4 id="案例-消费预测case"><a href="#case">案例-消费预测</a></h4>
<h4 id="总结conclusion"><a href="#conclusion">总结</a></h4>
<h4 id="psfinal"><a href="#final">ps.</a></h4>
<h2 id="1-基本原理a-id1a">1. 基本原理<a id="1"></a></h2>
<p>原理比较简单.首先随机设置k个簇的中心点，遍历每个点计算距离，将较近的点归于一个簇中.完毕后，更新簇的中心点为簇的均值点，重新遍历</p>
<h2 id="2-实战源码a-id2a">2. 《实战》源码<a id="2"></a></h2>
<h3 id="21-普通k均值聚类">2.1 普通k均值聚类</h3>
<pre><code># 计算欧式距离
def distEclud(vecA, vecB): 
    return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)

# 随机形成质心
def randCent(dataSet, k):
    n = shape(dataSet)[1]
    centroids = mat(zeros((k,n)))#create centroid mat
    for j in range(n):#create random cluster centers, within bounds of each dimension
        minJ = min(dataSet[:,j])
        rangeJ = float(max(dataSet[:,j]) - minJ)
        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) # 通过最小值+（最大值和最小值的）差值*一个0-1的数的方式，随机生成一个区间内的质心
    return centroids

# k均值算法
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))# 簇结果分配矩阵.第一列记录簇索引值，第二列记录误差
    centroids = createCent(dataSet, k)
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m):# 对于每一行（每一个数据），计算距离， 直到不变为止
            minDist = inf; minIndex = -1
            for j in range(k):
                distJI = distMeas(centroids[j,:],dataSet[i,:])
                if distJI &lt; minDist:
                    minDist = distJI; minIndex = j
            if clusterAssment[i,0] != minIndex: clusterChanged = True
            clusterAssment[i,:] = minIndex,minDist**2
        for cent in range(k):#recalculate centroids
            ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]# 得到这个簇的所有点
            centroids[cent,:] = mean(ptsInClust, axis=0) # 将质心重置为均值
    return centroids, clusterAssment

</code></pre>
<p>返回值中centroids代表质心，clusterAssment用于记录簇的聚类结果和误差</p>
<h3 id="22-二分k均值聚类">2.2 二分k均值聚类</h3>
<p>普通k均值算法只能达到局部最优解.初始点的选择对算法的影响很大.无法达到全局最优.此时就需要进行后处理.</p>
<p>聚类算法的指标是SSE（误差平方和），也即clusterAssment第二列之和，越小 代表数据点越接近质心，聚类效果好.后处理中降低SSE的方法，可以将最大的类再进行二分的聚类，然后为了保持簇的总数不变，需要合并两个使SSE增幅最小的簇的质心.对于高维数据并不好用.</p>
<p>SSE在k-means中的计算如下：</p>
<p><img class="img-zoomable" src="/kmeans/2.png" alt="png" />
</p>
<p>引入二分k均值聚类.初始将所有数据看做一个簇，然后循环.对于每个簇，先计算总误差， 然后将簇二分k均值后计算误差.最后选择使误差最小的簇进行划分.当簇的数量到达k时，退出循环.</p>
<pre><code>
def biKmeans(dataSet, k, distMeas=distEclud):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))# 记录最佳聚类的类别和距离，随迭代不断更新
    centroid0 = mean(dataSet, axis=0).tolist()[0]
    centList =[centroid0] #create a list with one centroid
    print(centList)
    for j in range(m):#calc initial Error
        clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2
    while (len(centList) &lt; k):
        lowestSSE = inf
        for i in range(len(centList)):
            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]# 获得簇i的所有数据点
            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)# splitClustAss:簇分配矩阵，第一列索引第二列偏差；centroidMat：距离矩阵
            sseSplit = sum(splitClustAss[:,1])# 计算分割后的误差平方和（该公式上面已经提及）
            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])# 计算未分割的误差平方和
            print (&quot;sseSplit, and notSplit: &quot;,sseSplit,sseNotSplit)
            if (sseSplit + sseNotSplit) &lt; lowestSSE:
                bestCentToSplit = i
                bestNewCents = centroidMat
                bestClustAss = splitClustAss.copy()
                lowestSSE = sseSplit + sseNotSplit
        print(nonzero(bestClustAss[:,0].A == 1))
        bestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)# 在二分的结果中找出类别“1”，将其替换到新的聚类类序号（如第三 类，第四类）
        bestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit# 在二分的结果中找出类别“0”，将其替换为最优分割点的类别
        print ('the bestCentToSplit is: ',bestCentToSplit)
        print ('the len of bestClustAss is: ', len(bestClustAss))
        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]# 将质心替换为最优聚类时的质心
        centList.append(bestNewCents[1,:].tolist()[0])
        clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss# 重新分配簇和sse
    return mat(centList), clusterAssment

</code></pre>
<h3 id="23-二分k均值聚类处理实际数据">2.3 二分k均值聚类处理实际数据</h3>
<p>《实战》中使用雅虎数据集测试算法.这里就不陈列获取数据的代码了，直接上画图的代码</p>
<pre><code>def clusterClubs(numClust=5):
    datList = []
    for line in open('places.txt').readlines():
        lineArr = line.split('\t')
        datList.append([float(lineArr[4]), float(lineArr[3])])
    datMat = mat(datList)
    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)
    fig = plt.figure()
    rect=[0.1,0.1,0.8,0.8]# 该列表的作用：第一个左右移动，第二个上下移动，第三个左右缩放，第四个上下缩放
    scatterMarkers=['s', 'o', '^', '8', 'p', \
                    'd', 'v', 'h', '&gt;', '&lt;']
    axprops = dict(xticks=[], yticks=[])
    ax0=fig.add_axes(rect, label='ax0', **axprops)
    imgP = plt.imread('Portland.png')
    ax0.imshow(imgP)
    ax1=fig.add_axes(rect, label='ax1', frameon=False)
    for i in range(numClust):
        ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]
        markerStyle = scatterMarkers[i % len(scatterMarkers)]
        ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)
    ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=300)
    plt.show()

</code></pre>
<h2 id="3-模型评估a-id3a">3. 模型评估<a id="3"></a></h2>
<h3 id="31-确定k的值-肘方法elbow-method">3.1 确定K的值-肘方法（Elbow method）</h3>
<p>对于每个K的取值，一次计算其到簇中心的平方和.随着簇的增加，平方和会逐渐减少.</p>
<p>下降过程中忽然出现拐点（下降速度变慢），被认为是最佳K值</p>
<p><img class="img-zoomable" src="/kmeans/3.png" alt="png" />
</p>
<h3 id="32-聚类的评估-轮廓系数法silhouette-coefficient-sc系数">3.2 聚类的评估-轮廓系数法（Silhouette Coefficient， SC系数）</h3>
<p>结合聚类的凝聚度和分离度，对聚类效果进行评估</p>
<p>目的是实现内部距离最小化，外部距离最大化</p>
<p><img class="img-zoomable" src="/kmeans/4.png" alt="png" />
</p>
<p>范围在-1-1之间，越接近1效果越好</p>
<h3 id="33-聚类的评估-ch系数法calinski-harabasz-">3.3 聚类的评估-CH系数法（Calinski-harabasz ）</h3>
<p>CH系数追求用尽量少的类别聚类尽量多的样本，并获取较好的效果</p>
<p>计算方式如下图</p>
<p><img class="img-zoomable" src="/kmeans/5.png" alt="png" />
</p>
<p>s值越大聚类效果越好</p>
<h2 id="4算法优化a-id4a">4.算法优化<a id="4"></a></h2>
<p>首先对传统kmeans进行总结：</p>
<p>优点如下：</p>
<p><strong>1.原理简单实现容易</strong></p>
<p><strong>2.空间复杂度为O(N)，时间复杂度为O(IKN)， k为簇数，I为迭代次数</strong></p>
<p>缺点如下：</p>
<p><strong>聚类效果依赖K的选择和初始点的选择</strong></p>
<p><strong>中心店易偏移，对利群点，噪声敏感</strong></p>
<p><strong>很难发现大小差别很大的簇进行增量计算</strong></p>
<p><strong>只能得到局部最优解</strong></p>
<p>除了二分kmeans，还有其他优化算法</p>
<h3 id="41-使用canopy算法确定初始点">4.1 使用canopy算法确定初始点</h3>
<p>canopy的原理如下：选择随机初始点，以该点为圆心，不同的值（T1，T2）为半径画同心圆.然后不断寻找圆以外的点为圆心 ，不同的值为半径画同心圆，直到所有点都能包含在一个圆中.这么做可以使初始点尽可能的远.</p>
<p>canopy的优点如下：</p>
<p><strong>kmeans的抗干扰能比比较弱，使用canopy可以直接去掉较小数据点的簇，有利于抗干扰</strong></p>
<p><strong>canopy选出的中心点更精确</strong></p>
<p>canopy的缺点如下：</p>
<p><strong>算法中T1，T2的确立，仍然会落入局部最优的问题</strong></p>
<h3 id="42-使用kmeans算法确定初始点">4.2 使用kmeans++算法确定初始点</h3>
<p>K-Means++算法使用距离平方求解，尽可能保证下一个质心到当前质心距离最远.</p>
<p>K-Means++算法的初始化过程如下所示：</p>
<p>从输入的数据点集合中随机选择一个点作为第一个聚类中心</p>
<p>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x).根据D（x）计算距离.距离计算公式如下:</p>
<p><img class="img-zoomable" src="/kmeans/6.png" alt="png" />
</p>
<p><strong>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大（并非绝对，而是将距离映射到一个概率.<a href="https://blog.csdn.net/yujianhuakaila/article/details/52947408" target="_blank">详情参考博客</a>）</strong></p>
<p>重复2和3直到k个聚类中心被选出来</p>
<p>利用这k个初始的聚类中心来运行标准的k-means算法</p>
<p>相比于kmeans，kmeans++的主要区别就在第三步.kmeans更新中心点为簇的均值点，而++更新中心为距离相对较远的点.</p>
<h3 id="43-使用二分kmeans算法设置阈值进行划分">4.3 使用二分kmeans算法设置阈值进行划分</h3>
<p>当SSE达到阈值时或k值达到要求时，停止划分</p>
<p>《实战》源码中已经提及和代码原理注释，不过多赘述</p>
<h3 id="44-使用kmedoids算法选取质心">4.4 使用kmedoids算法选取质心</h3>
<p>与kmeans的区别在于，kmeans选取新的簇中心使用平均值，而kmedoids算法比较麻烦，需要计算除原中心点外的其他点成为中心点时，代价函数（如距离和）的值.选取最小值以成为新的中心点</p>
<p>这么做，相较于kmeans，提高了抗噪声能力.但是样本多的时候运行非常慢.</p>
<h3 id="45-其他">4.5 其他</h3>
<p><strong>Kernel kmeans</strong>通过将每一个样本投射到高维空间（SVM核函数），再将处理好的数据使用普通kmeans进行聚类</p>
<p><strong>ISODATA</strong>类似于《实战》中的后处理方法，类别的数目会随着聚类的进行而改变.合并（样本数少，两个类距离太近）和分裂（类内方差过大）</p>
<p><strong>Minibatch</strong> 适用于大批量数据（10000+）.原理是从大批量数据中每次选取一批进行聚类，进行多次.是大数据集的分批聚类</p>
<h2 id="5-特征降维a-id5a">5. 特征降维<a id="5"></a></h2>
<p>包括特征选择和主成分分析（可以理解成一种特征提取的方式）.</p>
<h3 id="51-特征选择">5.1 特征选择</h3>
<h4 id="511-filter过滤式">5.1.1 Filter(过滤式)</h4>
<p>主要探究特征本身，特征与特征之间和目标值之间的关联</p>
<h5 id="5111-低方差特征过滤">5.1.1.1 低方差特征过滤</h5>
<p>两列数据方差小，代表某个特征大部分的值比较接近.需要过滤.</p>
<p><a href="#lowfc">API接口</a></p>
<h5 id="5112-相关系数">5.1.1.2 相关系数</h5>
<p>包括皮尔逊相关系数和斯皮尔曼相关系数</p>
<p><strong>皮尔逊相关系数</strong></p>
<p>非常麻烦的式子，计算公式如下图：</p>
<p><img class="img-zoomable" src="/kmeans/8.png" alt="png" />
</p>
<p>其中x和y为两列，即两个特征.n为个数</p>
<p>相关系数r的范围是-1到1.当|r|&lt;0.4为低度相关,当0.4&lt;|r|&lt;0.7为显著性相关,当0.7&lt;|r|&lt;1为高度线性相关</p>
<p><a href="#pearsonr">API接口</a></p>
<p><strong>斯皮尔曼相关系数</strong></p>
<p>计算公式如下，其中d<sub>i</sub>为二列成队变量的等级差值：</p>
<p><img class="img-zoomable" src="/kmeans/10.png" alt="png" />
</p>
<p><a href="#pearsonr">API接口</a></p>
<h4 id="512-embedded嵌入式">5.1.2 Embedded（嵌入式）</h4>
<p>算法自动选择特征（特征与目标值之间的关联）</p>
<p><a href="https://blog.daorenwu.com/post/decision-tree/#2-%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95a-id2a" target="_blank">决策树，包括信息熵，信息增益,点击跳转</a></p>
<p><a href="https://blog.daorenwu.com/post/linear-regression/#4%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8Classo%E5%9B%9E%E5%BD%92-a-id4a" target="_blank">正则化，包括L1，L2，线性回归中有提到,点击跳转</a></p>
<p>深度学习（卷积）</p>
<h3 id="52-主成分分析pca">5.2 主成分分析（PCA）</h3>
<p>高维信息转低维.此过程中可能舍弃原有数据，创建新的变量.应用在回归分析或聚类分析中.</p>
<p>与特征选择的区别是特征选择删除特征，没有产生新特征.而PCA产生新特征</p>
<p><img class="img-zoomable" src="/kmeans/11.png" alt="png" />
</p>
<p><a href="#PCA">API接口</a></p>
<h2 id="6-api接口实现a-idska">6. API接口实现<a id="sk"></a></h2>
<h3 id="61-kmeans">6.1 kmeans</h3>
<p>接口非常简单</p>
<p><img class="img-zoomable" src="/kmeans/1.png" alt="png" />
</p>
<p>由于聚类算法不同于分类 ，fit和predict可以一步实现</p>
<p>打分标准和分类（监督学习）也不一样，通过CH方法进行评估.</p>
<pre><code>calinski_harabaz_score(X, y_pre)
</code></pre>
<p>来实现.分数越高，分类效果越好.</p>
<h3 id="62-低方差特征过滤a-idlowfca">6.2 低方差特征过滤<a id="lowfc"></a></h3>
<p><img class="img-zoomable" src="/kmeans/7.png" alt="png" />
</p>
<p>默认0.0的参数是方差，必须设置，比较基本不会有完全相同的两个特征</p>
<pre><code>def var_thr():
    &quot;&quot;&quot;
    特征选择：低方差特征过滤
    :return:
    &quot;&quot;&quot;
    data = pd.read_csv(&quot;./data/factor_returns.csv&quot;)
    print(data)

    transfer = VarianceThreshold(threshold=10)
    trans_data = transfer.fit_transform(data.iloc[:, 1:10])

    print(&quot;之前数据的形状：\n&quot;,data.iloc[:, 1:10].shape)
    print(&quot;之后数据的形状：\n&quot;,trans_data.shape)

    print(trans_data)
</code></pre>
<h3 id="63-皮尔逊-斯皮尔曼相关系数a-idpearsonra">6.3 皮尔逊, 斯皮尔曼相关系数<a id="pearsonr"></a></h3>
<pre><code>from scipy.stats import pearsonr, spearmanr

# 皮尔逊演示
x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]

ret = pearsonr(x1, x2)
print(&quot;这两列数据的皮尔逊相关系数为：\n&quot;, ret)


ret = spearmanr(x1, x2)
print(&quot;这两列数据的斯皮尔曼相关系数为：\n&quot;, ret)

</code></pre>
<p><img class="img-zoomable" src="/kmeans/9.png" alt="png" />
</p>
<p>皮尔逊计算得出两个值.第一个值越大越相关.第二个值越小越相关.当样本数大于500时，第二个数更实用.斯皮尔曼的结果与其类似</p>
<h3 id="64-主成分分析a-idpcaa">6.4 主成分分析<a id="PCA"></a></h3>
<pre><code>from sklearn.decomposition import PCA

data = [[2, 8, 4, 5],
        [6, 3, 0, 8],
        [5, 4, 9, 1]]

# 1.保留到多少维度
# transfer = PCA(n_components=2)
# trans_data = transfer.fit_transform(data)
# print(trans_data)

# 2.保留信息的百分比
transfer = PCA(n_components=0.95)
transfer_data = transfer.fit_transform(data)
print(transfer_data)
</code></pre>
<h2 id="7-案例-消费预测a-idcasea">7. 案例-消费预测<a id="case"></a></h2>
<p>现有四张表：order表，product表，order_product表，aisle表.首先需要将多表合并，然后通过特征降维获取输入数据，机器学习后得出模型</p>
<pre><code># 1.获取数据
# 2.数据基本处理
# 2.1 合并表格
# 2.2 交叉表合并
# 2.3 数据截取
# 3.特征工程 — pca
# 4.机器学习（k-means）
# 5.模型评估

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 1.获取数据
order_product = pd.read_csv(&quot;./data/instacart/order_products__prior.csv&quot;)
products = pd.read_csv(&quot;./data/instacart/products.csv&quot;)
orders = pd.read_csv(&quot;./data/instacart/orders.csv&quot;)
aisles = pd.read_csv(&quot;./data/instacart/aisles.csv&quot;)

# 2.数据基本处理
# 2.1 合并表格
table1 = pd.merge(order_product, products, on=[&quot;product_id&quot;, &quot;product_id&quot;])
table2 = pd.merge(table1, orders, on=[&quot;order_id&quot;, &quot;order_id&quot;])
table = pd.merge(table2, aisles, on=[&quot;aisle_id&quot;, &quot;aisle_id&quot;])
# 2.2 交叉表合并
data = pd.crosstab(table[&quot;user_id&quot;], table[&quot;aisle&quot;])
# 2.3 数据截取
new_data = data[:1000]
# 3.特征工程 — pca
transfer = PCA(n_components=0.9)
trans_data = transfer.fit_transform(new_data)
# 4.机器学习（k-means）
estimator = KMeans(n_clusters=5)
y_pre = estimator.fit_predict(trans_data)
# 5.模型评估
silhouette_score(trans_data, y_pre)


</code></pre>
<h2 id="8-总结a-idconclusiona">8. 总结<a id="conclusion"></a></h2>
<p>本博客介绍了k均值算法，并对其不足之处陈列了一系列改进算法.同时介绍了特征降维两种方法，即特征提取和主成分分析.最后，通过一个消费预测的实例测试了算法</p>
<h2 id="9-psa-idfinala">9. p.s.<a id="final"></a></h2>
<p><a href="https://zhuanlan.zhihu.com/p/132579724" target="_blank">pd.merge</a></p>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/aplayer/">aplayer</a>
            </span>
            
            <span>
                <a href="/tags/cjk/">cjk</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/emoji/">emoji</a>
            </span>
            
            <span>
                <a href="/tags/html/">html</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">Pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/themes/">themes</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/wtf/">wtf</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-基本原理a-id1a">1. 基本原理<a id="1"></a></a></li>
    <li><a href="#2-实战源码a-id2a">2. 《实战》源码<a id="2"></a></a>
      <ul>
        <li><a href="#21-普通k均值聚类">2.1 普通k均值聚类</a></li>
        <li><a href="#22-二分k均值聚类">2.2 二分k均值聚类</a></li>
        <li><a href="#23-二分k均值聚类处理实际数据">2.3 二分k均值聚类处理实际数据</a></li>
      </ul>
    </li>
    <li><a href="#3-模型评估a-id3a">3. 模型评估<a id="3"></a></a>
      <ul>
        <li><a href="#31-确定k的值-肘方法elbow-method">3.1 确定K的值-肘方法（Elbow method）</a></li>
        <li><a href="#32-聚类的评估-轮廓系数法silhouette-coefficient-sc系数">3.2 聚类的评估-轮廓系数法（Silhouette Coefficient， SC系数）</a></li>
        <li><a href="#33-聚类的评估-ch系数法calinski-harabasz-">3.3 聚类的评估-CH系数法（Calinski-harabasz ）</a></li>
      </ul>
    </li>
    <li><a href="#4算法优化a-id4a">4.算法优化<a id="4"></a></a>
      <ul>
        <li><a href="#41-使用canopy算法确定初始点">4.1 使用canopy算法确定初始点</a></li>
        <li><a href="#42-使用kmeans算法确定初始点">4.2 使用kmeans++算法确定初始点</a></li>
        <li><a href="#43-使用二分kmeans算法设置阈值进行划分">4.3 使用二分kmeans算法设置阈值进行划分</a></li>
        <li><a href="#44-使用kmedoids算法选取质心">4.4 使用kmedoids算法选取质心</a></li>
        <li><a href="#45-其他">4.5 其他</a></li>
      </ul>
    </li>
    <li><a href="#5-特征降维a-id5a">5. 特征降维<a id="5"></a></a>
      <ul>
        <li><a href="#51-特征选择">5.1 特征选择</a></li>
        <li><a href="#52-主成分分析pca">5.2 主成分分析（PCA）</a></li>
      </ul>
    </li>
    <li><a href="#6-api接口实现a-idska">6. API接口实现<a id="sk"></a></a>
      <ul>
        <li><a href="#61-kmeans">6.1 kmeans</a></li>
        <li><a href="#62-低方差特征过滤a-idlowfca">6.2 低方差特征过滤<a id="lowfc"></a></a></li>
        <li><a href="#63-皮尔逊-斯皮尔曼相关系数a-idpearsonra">6.3 皮尔逊, 斯皮尔曼相关系数<a id="pearsonr"></a></a></li>
        <li><a href="#64-主成分分析a-idpcaa">6.4 主成分分析<a id="PCA"></a></a></li>
      </ul>
    </li>
    <li><a href="#7-案例-消费预测a-idcasea">7. 案例-消费预测<a id="case"></a></a></li>
    <li><a href="#8-总结a-idconclusiona">8. 总结<a id="conclusion"></a></a></li>
    <li><a href="#9-psa-idfinala">9. p.s.<a id="final"></a></a></li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/aplayer/">aplayer</a>
            </span>
            
            <span>
                <a href="/tags/cjk/">cjk</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/emoji/">emoji</a>
            </span>
            
            <span>
                <a href="/tags/html/">html</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">Pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/themes/">themes</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/wtf/">wtf</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-基本原理a-id1a">1. 基本原理<a id="1"></a></a></li>
    <li><a href="#2-实战源码a-id2a">2. 《实战》源码<a id="2"></a></a>
      <ul>
        <li><a href="#21-普通k均值聚类">2.1 普通k均值聚类</a></li>
        <li><a href="#22-二分k均值聚类">2.2 二分k均值聚类</a></li>
        <li><a href="#23-二分k均值聚类处理实际数据">2.3 二分k均值聚类处理实际数据</a></li>
      </ul>
    </li>
    <li><a href="#3-模型评估a-id3a">3. 模型评估<a id="3"></a></a>
      <ul>
        <li><a href="#31-确定k的值-肘方法elbow-method">3.1 确定K的值-肘方法（Elbow method）</a></li>
        <li><a href="#32-聚类的评估-轮廓系数法silhouette-coefficient-sc系数">3.2 聚类的评估-轮廓系数法（Silhouette Coefficient， SC系数）</a></li>
        <li><a href="#33-聚类的评估-ch系数法calinski-harabasz-">3.3 聚类的评估-CH系数法（Calinski-harabasz ）</a></li>
      </ul>
    </li>
    <li><a href="#4算法优化a-id4a">4.算法优化<a id="4"></a></a>
      <ul>
        <li><a href="#41-使用canopy算法确定初始点">4.1 使用canopy算法确定初始点</a></li>
        <li><a href="#42-使用kmeans算法确定初始点">4.2 使用kmeans++算法确定初始点</a></li>
        <li><a href="#43-使用二分kmeans算法设置阈值进行划分">4.3 使用二分kmeans算法设置阈值进行划分</a></li>
        <li><a href="#44-使用kmedoids算法选取质心">4.4 使用kmedoids算法选取质心</a></li>
        <li><a href="#45-其他">4.5 其他</a></li>
      </ul>
    </li>
    <li><a href="#5-特征降维a-id5a">5. 特征降维<a id="5"></a></a>
      <ul>
        <li><a href="#51-特征选择">5.1 特征选择</a></li>
        <li><a href="#52-主成分分析pca">5.2 主成分分析（PCA）</a></li>
      </ul>
    </li>
    <li><a href="#6-api接口实现a-idska">6. API接口实现<a id="sk"></a></a>
      <ul>
        <li><a href="#61-kmeans">6.1 kmeans</a></li>
        <li><a href="#62-低方差特征过滤a-idlowfca">6.2 低方差特征过滤<a id="lowfc"></a></a></li>
        <li><a href="#63-皮尔逊-斯皮尔曼相关系数a-idpearsonra">6.3 皮尔逊, 斯皮尔曼相关系数<a id="pearsonr"></a></a></li>
        <li><a href="#64-主成分分析a-idpcaa">6.4 主成分分析<a id="PCA"></a></a></li>
      </ul>
    </li>
    <li><a href="#7-案例-消费预测a-idcasea">7. 案例-消费预测<a id="case"></a></a></li>
    <li><a href="#8-总结a-idconclusiona">8. 总结<a id="conclusion"></a></a></li>
    <li><a href="#9-psa-idfinala">9. p.s.<a id="final"></a></a></li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2022
                <a href="https://wudaore.github.io/">DSRKafuU</a>
                 | <a href="https://github.com/dsrkafuu/hugo-theme-fuji">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>



</body>

</html>
