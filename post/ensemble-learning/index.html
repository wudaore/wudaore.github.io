<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.98.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>集成学习 - wudao的博客</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="机器学习, algo" />


<meta property="og:title" content="集成学习" />
<meta name="twitter:title" content="集成学习" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wudaore.github.io/post/ensemble-learning/" /><meta property="og:description" content="目录 基本原理 bagging boosting 总结 ps. 1. 基本原理 即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测. 对于欠拟合问题，使用b" />
<meta name="twitter:description" content="目录 基本原理 bagging boosting 总结 ps. 1. 基本原理 即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测. 对于欠拟合问题，使用b" /><meta property="og:image" content="https://wudaore.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://wudaore.github.io/img/og.png" /><meta property="article:published_time" content="2022-06-08T00:00:00+08:00" /><meta property="article:modified_time" content="2022-06-08T00:00:00+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://wudaore.github.io/assets/css/fuji.min.css" />








</head>

<body
  data-theme="auto"
  data-theme-auto='true'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://wudaore.github.io/">wudao的博客</a>
            
            <span class="title-sub">钝学累功,不妨精熟</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://wudaore.github.io/post/ensemble-learning/">集成学习</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-06-08</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;2632 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>&nbsp;<a href="/tags/algo">algo</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h1 id="目录">目录</h1>
<h4 id="基本原理1"><a href="#1">基本原理</a></h4>
<h4 id="bagging2"><a href="#2">bagging</a></h4>
<h4 id="boosting3"><a href="#3">boosting</a></h4>
<h4 id="总结conclusion"><a href="#conclusion">总结</a></h4>
<h4 id="psfinal"><a href="#final">ps.</a></h4>
<h2 id="1-基本原理a-id1a">1. 基本原理<a id="1"></a></h2>
<p>即通过简历多个模型来解决单一预测问题.原理是生成多个分类器，各自独立学习得出预测结果，这些预测最后组成预测序列.因此优于任何一个单分类做出的预测.</p>
<p>对于欠拟合问题，使用boosting逐步增强学习；对于过拟合问题，使用bagging采样学习集成.</p>
<h2 id="2-bagginga-id2a">2. bagging<a id="2"></a></h2>
<h3 id="21-bagging-集成原理和过程">2.1 bagging 集成原理和过程</h3>
<p>1.<strong>采样</strong>.从所有版本中，采样一部分</p>
<p>2.<strong>学习</strong>.训练弱学习器</p>
<p>3.<strong>集成</strong>.使用平权投票</p>
<h3 id="22-随机森林">2.2 随机森林</h3>
<p>随机森林，即bagging+决策树.其实现流程如下：</p>
<p>1.随机选k条数据（一次随机选一个样本，有放回的抽样）</p>
<p>2.随机选m个特征（m远小于特征总数）</p>
<p>3.训练决策树（默认CART（基尼系数）决策树）</p>
<p>4.重复1-3步构造n棵弱决策树</p>
<p>5.平权投票集成所有决策树</p>
<p><strong>注</strong>：随机抽样是为了让每棵树的训练集不一样</p>
<p>抽样必须是有放回的，不然可能会导致每棵树训练出来的结果有很大的差异.而随机森林取决于每棵树的投票</p>
<h3 id="23-随机森林sklearn接口">2.3 随机森林sklearn接口</h3>
<p><img class="img-zoomable" src="/ensemble/1.png" alt="png" />
</p>
<p>在使用随机森林时，需要对树的个数，树的深度等超参数进行调整.泰坦尼克号样例代码如下（与决策树代码类似，改一下模型就可）：</p>
<pre><code># 4.机器学习（模型训练）
estimator = RandomForestClassifier()

param_grid = {&quot;n_estimators&quot;: [120,200,300,500,800,1200], &quot;max_depth&quot;: [5, 8, 15, 25, 30]}
estimator = GridSearchCV(estimator, param_grid=param_grid, cv=5)

estimator.fit(x_train, y_train)
print(estimator.score(x_train, y_train))
print(estimator.best_estimator_) # 打印最佳参数选择
</code></pre>
<h3 id="24-bagging总结">2.4 bagging总结</h3>
<p>bagging集成学习方法 = Bagging+决策树/逻辑回归深度学习&hellip;</p>
<p>经过上面方式组成的集成方法，可以提高一定的泛化准确率，并简单，方便，通用</p>
<h2 id="3-boostinga-id3a">3. boosting<a id="3"></a></h2>
<p>1.训练学习器</p>
<p>2.调整数据分布（对于分类争取的数据减少其权重，分类正确则增加其权重），将训练注意力集中在错误数据上</p>
<p>3.重复1,2.直到达到结束条件（《实战》中的结束条件为迭代次数到了或者再也没有错误分类了）</p>
<p>4.各个学习器加权投票，得出结果</p>
<h3 id="31-adaboost--实战源码">3.1 AdaBoost&ndash;《实战》源码</h3>
<p>AdaBoost将结合《实战》中的代码进行学习.AdaBoost即自适应Boost（adaptive Boost）.其基本流程如下：</p>
<p>1.初始化训练数据权重相等，训练第一个学习器</p>
<pre><code>    D = mat(ones((m,1))/m)   #init D to all equal
</code></pre>
<p>2.计算该学习器在训练数据中的错误率</p>
<p>3.计算该学习器的投票权重α<sub>t</sun>.α<sub>t</sun>的计算公式如下：</p>
<p><img class="img-zoomable" src="/ensemble/2.png" alt="png" />
</p>
<pre><code>alpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0
bestStump['alpha'] = alpha  
weakClassArr.append(bestStump)                  # 将最佳单层决策树存入数组中
</code></pre>
<p>4.根据投票权重，调整学习器的注意力到错误数据上，对训练数据重新赋权值.</p>
<p><img class="img-zoomable" src="/ensemble/3.png" alt="png" />
</p>
<pre><code>expon = multiply(-1*alpha*mat(classLabels).T,classEst) #  权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1
D = multiply(D,exp(expon))                              # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值
D = D/D.sum()
</code></pre>
<p>5.重复执行1-4步n次</p>
<p>6.对所有学习器投票计算权重</p>
<details>
<summary>训练模型代码</summary>
<pre><code>
def adaBoostTrainDS(dataArr,classLabels,numIt=40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat(ones((m,1))/m)   #init D to all equal
    aggClassEst = mat(zeros((m,1)))
    for i in range(numIt):
        bestStump,error,classEst = buildStump(dataArr,classLabels,D)# 最优单层决策树的基本信息（特征下标，阈值和符号），最小误差，分类结果
        #print('result = ', bestStump)
        #print(error)
        #print(classEst)
        #print("D:",D.T)
        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))# max(error,1e-16) 确保计算在没有错误时没有零溢出.calc alpha, throw in max(error,eps) to account for error=0
        bestStump['alpha'] = alpha  
        weakClassArr.append(bestStump)                  # 将最佳单层决策树存入数组中
        expon = multiply(-1*alpha*mat(classLabels).T,classEst) #  权重向量D会越来越混乱.很巧妙。当真实值（classLabel）=预测值（classEst）时，乘积为1.不等时，为-1
        D = multiply(D,exp(expon))                              # 计算下一轮迭代的D值.对于预测正确的特征减小D值，正确的特征增加D值
        D = D/D.sum()
        #calc training error of all classifiers, if this is 0 quit for loop early (use break)
        aggClassEst += alpha*classEst
        #print("aggClassEst: ",aggClassEst.T)
        # addErrors存放分类错误的特征.分错为1分对为0
        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))
        #print('aggerr', aggErrors)
        errorRate = aggErrors.sum()/m
        print("total error: ",errorRate)
        if errorRate == 0.0: break
    return weakClassArr,aggClassEst
</code></pre>
</details>
<details>
<summary>利用模型分类 代码</summary>
<pre><code>
def adaClassify(datToClass,classifierArr):
    # print(classifierArr)
    # 输入：由一个或多个待分类样例daToClass以及多个弱分类器组成的数组classifierArr
    # 输出：aggClassEst的符号，即aggClassEst大于0则返回+1，小于0则返回-1
    dataMatrix = mat(datToClass)
    m = shape(dataMatrix)[0]
    aggClassEst = mat(zeros((m,1)))
    for i in range(len(classifierArr)):
        classEst = stumpClassify(dataMatrix,classifierArr[0][i]['dim'],\
                                 classifierArr[0][i]['thresh'],\
                                 classifierArr[0][i]['ineq'])
        aggClassEst += classifierArr[0][i]['alpha']*classEst # 。输出的类别估计值乘上该单层决策树的alpha权重然后累加到aggClassEst上
        # print(aggClassEst)
    return sign(aggClassEst)
</code></pre>
</details>
<h3 id="32-bagging和boosting的区别">3.2 bagging和boosting的区别</h3>
<p><strong>1.数据方面</strong></p>
<p>Bagging：对数据进行采样训练</p>
<p>Boosting：根据前一轮学习结果调整数据重要性</p>
<p><strong>2.投票方面</strong></p>
<p>Bagging：所有学习器平权投票</p>
<p>Boosting：所有学习器加权投票</p>
<p><strong>3.学习顺序</strong></p>
<p>Bagging：Bagging是并行的，每个学习器没有依赖关系</p>
<p>Boosting：学习是串行，有先后顺序</p>
<p><strong>4.主要作用</strong></p>
<p>Bagging：解决过拟合（降低方差，提高泛化性能）</p>
<p>Boosting：解决欠拟合（提高训练精度，降低偏差）</p>
<h3 id="33-gbdt">3.3 GBDT</h3>
<p>GBDT = 梯度下降+Boosting+决策树</p>
<p>GBDT使用梯度下降法优化代价函数，使用一层决策树作为弱学习器，负梯度作为目标值，最后利用boosting思想进行集成</p>
<p><img class="img-zoomable" src="/ensemble/4.png" alt="png" />
</p>
<p><strong>GBDT举例</strong></p>
<table>
<thead>
<tr>
<th>编号</th>
<th>年龄</th>
<th>体重</th>
<th>身高</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>5</td>
<td>20</td>
<td>1.1</td>
</tr>
<tr>
<td>2</td>
<td>7</td>
<td>30</td>
<td>1.3</td>
</tr>
<tr>
<td>3</td>
<td>21</td>
<td>70</td>
<td>1.7</td>
</tr>
<tr>
<td>4</td>
<td>30</td>
<td>60</td>
<td>1.8</td>
</tr>
<tr>
<td>5</td>
<td>25</td>
<td>65</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>预测身高，这是一个回归问题.<strong>回归问题的损失函数可以使用平方误差（二分类使用log，多分类使用softmax）</strong></p>
<p><strong>第一步，计算损失函数，并求出第一个预测值.</strong></p>
<p><img class="img-zoomable" src="/ensemble/5.png" alt="png" />
</p>
<p><strong>第二步，使用误差值替换目标值.求解划分点，使得方差最小</strong></p>
<p><img class="img-zoomable" src="/ensemble/6.png" alt="png" />
</p>
<p><strong>第三步，通过划分点划分，求解得出h<sub>1</sub>x</strong></p>
<p>y<sup>,</sup>的求法类似于上面的h<sub>0</sub>x，即对损失函数求导使其等于0</p>
<p><strong>第四步，求解h<sub>2</sub>x</strong></p>
<p>h<sub>2</sub>x的求法也类似于h<sub>1</sub>x.首先求出误差值，使用误差值替换目标值.h<sub>2</sub>x的误差值等于上一轮的目标值-预测值.</p>
<p>最后经过一系列计算，得出最后的预测模型：</p>
<p><img class="img-zoomable" src="/ensemble/7.png" alt="png" />
</p>
<h3 id="34-gbdt和adaboost的区别">3.4 GBDT和AdaBoost的区别</h3>
<p>GBDT与Adboost最主要的区别在于两者如何识别模型的问题。</p>
<p><strong>Adaboost</strong>用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。</p>
<p><strong>GBDT</strong>通过负梯度来识别问题，通过计算负梯度来改进模型。</p>
<p>GBDT每一轮训练时所关注的重点是本轮产生结果的残差，下一轮以本轮残差作为输入，尽量去拟合这个残差，使下一轮输出的残差不断变小。所以GBDT可以做到每一轮一定向损失函数减小的梯度方向变化，而传统的boosting算法只能是尽量向梯度方向减小，这是GBDT与传统boosting算法最大的区别，这也是为什么GBDT相比传统boosting算法可以用更少的树个数与深度达到更好的效果。</p>
<p>和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足，而GBDT是通过算梯度来定位模型的不足。因此相比AdaBoost，GBDT可以使用更多种类的目标函数。</p>
<h3 id="35-xgboost简单了解">3.5 XGBoost（简单了解）</h3>
<p>基本原理：二阶泰勒展开，boosting，正则化，决策树</p>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/dsrkafuu" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/dsrkafuu" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/19767474" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/aplayer/">aplayer</a>
            </span>
            
            <span>
                <a href="/tags/cjk/">cjk</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/emoji/">emoji</a>
            </span>
            
            <span>
                <a href="/tags/html/">html</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/themes/">themes</a>
            </span>
            
            <span>
                <a href="/tags/wtf/">wtf</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-基本原理a-id1a">1. 基本原理<a id="1"></a></a></li>
    <li><a href="#2-bagginga-id2a">2. bagging<a id="2"></a></a>
      <ul>
        <li><a href="#21-bagging-集成原理和过程">2.1 bagging 集成原理和过程</a></li>
        <li><a href="#22-随机森林">2.2 随机森林</a></li>
        <li><a href="#23-随机森林sklearn接口">2.3 随机森林sklearn接口</a></li>
        <li><a href="#24-bagging总结">2.4 bagging总结</a></li>
      </ul>
    </li>
    <li><a href="#3-boostinga-id3a">3. boosting<a id="3"></a></a>
      <ul>
        <li><a href="#31-adaboost--实战源码">3.1 AdaBoost&ndash;《实战》源码</a></li>
        <li><a href="#32-bagging和boosting的区别">3.2 bagging和boosting的区别</a></li>
        <li><a href="#33-gbdt">3.3 GBDT</a></li>
        <li><a href="#34-gbdt和adaboost的区别">3.4 GBDT和AdaBoost的区别</a></li>
        <li><a href="#35-xgboost简单了解">3.5 XGBoost（简单了解）</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/dsrkafuu" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/dsrkafuu" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/19767474" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/aplayer/">aplayer</a>
            </span>
            
            <span>
                <a href="/tags/cjk/">cjk</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/emoji/">emoji</a>
            </span>
            
            <span>
                <a href="/tags/html/">html</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/themes/">themes</a>
            </span>
            
            <span>
                <a href="/tags/wtf/">wtf</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-基本原理a-id1a">1. 基本原理<a id="1"></a></a></li>
    <li><a href="#2-bagginga-id2a">2. bagging<a id="2"></a></a>
      <ul>
        <li><a href="#21-bagging-集成原理和过程">2.1 bagging 集成原理和过程</a></li>
        <li><a href="#22-随机森林">2.2 随机森林</a></li>
        <li><a href="#23-随机森林sklearn接口">2.3 随机森林sklearn接口</a></li>
        <li><a href="#24-bagging总结">2.4 bagging总结</a></li>
      </ul>
    </li>
    <li><a href="#3-boostinga-id3a">3. boosting<a id="3"></a></a>
      <ul>
        <li><a href="#31-adaboost--实战源码">3.1 AdaBoost&ndash;《实战》源码</a></li>
        <li><a href="#32-bagging和boosting的区别">3.2 bagging和boosting的区别</a></li>
        <li><a href="#33-gbdt">3.3 GBDT</a></li>
        <li><a href="#34-gbdt和adaboost的区别">3.4 GBDT和AdaBoost的区别</a></li>
        <li><a href="#35-xgboost简单了解">3.5 XGBoost（简单了解）</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2022
                <a href="https://wudaore.github.io/">DSRKafuU</a>
                 | <a href="https://github.com/dsrkafuu/hugo-theme-fuji">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>



</body>

</html>
