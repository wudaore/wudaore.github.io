<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.98.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>NLP--RNN - wudao的博客</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="深度学习, NLP" />


<meta property="og:title" content="NLP--RNN" />
<meta name="twitter:title" content="NLP--RNN" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wudaore.github.io/post/nlp-rnn/" /><meta property="og:description" content="目录 1. 序 NLP&ndash;RNN的学习 在普通的神经网络中，信息是单向传递的.这么做虽然让网络更容易学习，但是在很多的现实任务中，网络的输出不仅依赖于当前的输入，也依赖于过去的输出.此外，普通的神经网" />
<meta name="twitter:description" content="目录 1. 序 NLP&ndash;RNN的学习 在普通的神经网络中，信息是单向传递的.这么做虽然让网络更容易学习，但是在很多的现实任务中，网络的输出不仅依赖于当前的输入，也依赖于过去的输出.此外，普通的神经网" /><meta property="og:image" content="https://wudaore.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://wudaore.github.io/img/og.png" /><meta property="article:published_time" content="2022-07-20T00:00:00+08:00" /><meta property="article:modified_time" content="2022-07-20T00:00:00+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://wudaore.github.io/assets/css/fuji.min.css" />








</head>

<body
  data-theme="auto"
  data-theme-auto='true'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://wudaore.github.io/">wudao的博客</a>
            
            <span class="title-sub">钝学累功,不妨精熟</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://wudaore.github.io/post/nlp-rnn/">NLP--RNN</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-07-20</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;4638 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>&nbsp;<a href="/tags/nlp">NLP</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h1 id="目录">目录</h1>
<h2 id="1-序a-id1a">1. 序<a id="1"></a></h2>
<p>NLP&ndash;RNN的学习</p>
<p>在普通的神经网络中，信息是单向传递的.这么做虽然让网络更容易学习，但是在很多的现实任务中，<strong>网络的输出不仅依赖于当前的输入，也依赖于过去的输出</strong>.此外，普通的神经网络要求输入和输出的维度是固定的，这样就难以处理诸如视频，语音，文本这样的<strong>数据长度不固定的时序数据.</strong></p>
<p>循环神经网络(RNN)是一类具有<strong>短期</strong>记忆能力的神经网络.神经元不仅可以接受其他神经元的信息，也可以接受自己的信息.换句话说，<strong>神经元的输出可以作为下一个时间的输入</strong></p>
<p>普通神经网络结构图</p>
<p><img class="img-zoomable" src="/RNN/1.png" alt="png" />
</p>
<p>循环神经网络结构图</p>
<p><img class="img-zoomable" src="/RNN/2.png" alt="png" />
</p>
<p>通过图我们可以看到，相较于传统神经网络，RNN多了一个循环圈.将RNN在时间点上展开，可以得到：</p>
<p><img class="img-zoomable" src="/RNN/3.png" alt="png" />
</p>
<h2 id="2-反向传播a-id2a">2. 反向传播<a id="2"></a></h2>
<p>前面的学习中已经初步了解了反向传播.接下来通过吴恩达的专项课程来进一步理解其中的数学原理.</p>
<h3 id="21-正向传播">2.1 正向传播</h3>
<p>就简单的神经网络而言，其实<code>h = self.activate(input_data · self.W + self.b) </code>就是一个正向传播的过程.</p>
<p>也即，输入x经过权重w和偏置b后通过激活函数得到结果h</p>
<h3 id="22-非输出层的反向传播">2.2 非输出层的反向传播</h3>
<p>我们的最终目的是算出误差loss并更新权重w.这需要我们计算得出loss(后面统称l)对传播过程中各个参数的偏导.</p>
<p><a href="https://zhuanlan.zhihu.com/p/115571464" target="_blank">参考博客</a></p>
<p>有如下神经网络：</p>
<p><img class="img-zoomable" src="/RNN/4.png" alt="png" />
</p>
<p>反向传播从l开始.先求l对a的偏导：</p>
<p><img class="img-zoomable" src="/RNN/5.png" alt="png" />
</p>
<p>再通过a，求l到h的偏导</p>
<p><img class="img-zoomable" src="/RNN/6.png" alt="png" />
</p>
<p>最后求出l对w和b的偏导</p>
<p><img class="img-zoomable" src="/RNN/7.png" alt="png" />
</p>
<p>由此，反向传播的表达如下：</p>
<p><img class="img-zoomable" src="/RNN/8.png" alt="png" />
</p>
<p><strong>注意</strong>，&quot;*&ldquo;代表element-wise乘积，即元素对应相乘;&rdquo;.&ldquo;代表矩阵相乘</p>
<p>代码如下：</p>
<pre><code>class Layer:
    '''中间层类'''
    self.W  # (input_dim, output_dim)
    self.b  # (1, output_dim)
    self.activate(a) = sigmoid(a)/tanh(a)/ReLU(a)/Softmax(a)

    def forward(self, input_data):       # input_data: (1, input_dim)
       '''单个样本的前向传播'''
       input_data · self.W + self.b = a  # a: (1, output_dim)
       h = self.activate(a)              # h: (1, output_dim)
       return h

    def backward(input_grad):
       '''单个样本的反向传播'''
       a_grad = input_grad * activate’(a)  # (1, output_dim)
       b_grad = a_grad                     # (1, output_dim)
       W_grad = (input_data.T) · a_grad    # (input_dim, output_dim)

       self.b -= learning_rate * b_grad 
       self.W -= learning_rate * W_grad

</code></pre>
<h3 id="23-输出层的反向传播">2.3 输出层的反向传播</h3>
<p>输出层的反向传播略有不同，因为若使用softmax等激活函数，a和h的数量不嫩一一对应，element-wise乘积无法生效.</p>
<p>这时就需要l对h的偏导乘以h到a的向量梯度(雅克比矩阵)</p>
<p>但实际上，经过看上去复杂的计算后输出层l对a的偏导就等于<code>h-y</code>(具体过程看博客，其实我也不是很懂)</p>
<p>那么，输出层的反向传播表达如下图：</p>
<p><img class="img-zoomable" src="/RNN/9.png" alt="png" />
</p>
<p>代码如下：</p>
<pre><code># * 表示element-wise乘积，· 表示矩阵乘积

class Output_layer(Layer):
    '''属性和forward方法继承Layer类'''

    def backward(input_grad):
       '''输出层backward方法'''
       '''单个样本的反向传播'''
       a_grad = input_grad                 # (1, output_dim)
       b_grad = a_grad                     # (1, output_dim)
       W_grad = (input_data.T) · a_grad    # (input_dim, output_dim)

       self.b -= learning_rate * b_grad 
       self.W -= learning_rate * W_grad

       return a_grad · (self.W).T          # (1, input_dim)

</code></pre>
<h3 id="24-改进batch批量计算">2.4 改进：batch批量计算</h3>
<p>除非用随机梯度下降，否则每次用以训练的样本都是整个batch计算的，损失函数l则是整个batch中样本得到损失的均值。</p>
<p>在计算中会以向量化的方式增加运算效率，用batch_size表示批的规模，代码可更改为：</p>
<pre><code># * 表示element-wise乘积，· 表示矩阵乘积

class Layer:
    '''中间层类'''

    def forward(self, input_data):       # input_data: (batch_size, input_dim)
       '''batch_size个样本的前向传播'''
       input_data · self.W + self.b = a  # a: (1, output_dim)
       h = self.activate(a)              # h: (1, output_dim)
       return h

    def backward(input_grad):             # input_grad: (batch_size, output_dim)
       '''batch_size个样本的反向传播'''
       a_grad = input_grad * activate’(a) # (batch_size, output_dim)

       b_grad = a_grad.mean(axis=0)       # (1, output_dim)
       W_grad = (a_grad.reshape(batch_size,1,output_dim) 
                    * input_data.reshape(batch_size,input_dim,1)).mean(axis=0)
       # (input_dim, output_dim) 
       
       self.b -= lr * b_grad
       self.W -= lr * W_grad

       return a_grad · (self.W).T         # output_grad: (batch_size, input_dim)


class Output_layer(Layer):
    '''输出层类：属性和forward方法继承Layer类'''

    def backward(input_grad):             # input_grad: (batch_size, output_dim)
       '''输出层backward方法'''
       '''batch_size个样本的反向传播'''
       a_grad = input_grad                # (batch_size, output_dim)

       b_grad = a_grad.mean(axis=0)       # (1, output_dim)
       W_grad = (a_grad.reshape(batch_size,1,output_dim) 
                    * input_data.reshape(batch_size,input_dim,1)).mean(axis=0)
       # (input_dim, output_dim) 

       self.b -= learning_rate * b_grad 
       self.W -= learning_rate * W_grad

       return a_grad · (self.W).T          # output_grad: (batch_size, input_dim)

</code></pre>
<p><strong>梯度对中间结果a和h上均不用求均值，在w和b上需要</strong></p>
<h2 id="3-rnn的不同表示和功能a-id3a">3. RNN的不同表示和功能<a id="3"></a></h2>
<p><img class="img-zoomable" src="/RNN/10.png" alt="png" />
</p>
<ol>
<li>one to one</li>
</ol>
<p>固定长度的输入输出(图像分类)</p>
<ol start="2">
<li>one to many</li>
</ol>
<p>序列输出(图像转文字)</p>
<ol start="3">
<li>many to one</li>
</ol>
<p>数列输入(文字分类)</p>
<ol start="4">
<li>many to many</li>
</ol>
<p>序列输入和输出</p>
<pre><code>4.1 异步

    文本翻译

4.2 同步

    根据视频的每一帧给视频分类
</code></pre>
<h2 id="4-lstma-id4a">4. LSTM<a id="4"></a></h2>
<p>在进行一些预测时，可能决定性的因素在时间序列上排在很前面，这时RNN就不会有非常好的效果.因为它是短期记忆的.</p>
<p>为了解决这个问题就需要LSTM(Lone Short-Term Memory)&ndash;一种可以学习长期依赖的特殊类型RNN</p>
<h3 id="41-基本原理">4.1 基本原理</h3>
<p><img class="img-zoomable" src="/RNN/11.png" alt="png" />
</p>
<p>其中∅代表sigmod函数.</p>
<p>LSTM的核心是单元(细胞)中的状态，也即上面那条线.</p>
<p><img class="img-zoomable" src="/RNN/12.png" alt="png" />
</p>
<p>但是光靠那条线不能实现信息的增删.此时需要通过sigmod和点乘来实现一个门，控制信息通过与否.</p>
<p>上图中，C保存的是记忆</p>
<h3 id="42-遗忘门">4.2 遗忘门</h3>
<p><img class="img-zoomable" src="/RNN/13.png" alt="png" />
</p>
<p>即，将上一层的输出h<sub>t-1</sub>和输入x<sub>t</sub>做矩阵concat操作(<strong>[]代表concat操作，也即矩阵的拼接</strong>)后，乘以权值加上偏置，通过sigmod函数之后乘以上一次的记忆C<sub>t-1</sub></p>
<p>sigmod取值在0到1之间.1代表全部通过，0代表不给通过.与C<sub>t-1</sub>相乘后可以决定保留记忆的程度.</p>
<h3 id="43-输入门">4.3 输入门</h3>
<p><img class="img-zoomable" src="/RNN/14.png" alt="png" />
</p>
<p>i<sub>t</sub>代表就当前更新而言，有多少比例的信息会更新，C<sub>t</sub>代表要更新的信息.</p>
<p>通过遗忘门和输入门，就可以更新记忆(细胞状态)了.具体来说就是旧的细胞状态和遗忘门相乘，加上数入门和tanh相乘的结果.</p>
<p>举个例子：&ldquo;我昨天吃了苹果，我今天吃了菠萝&quot;在这个句子中，通过遗忘门遗忘苹果，通过输入门逐步更新主语为菠萝.</p>
<p><img class="img-zoomable" src="/RNN/15.png" alt="png" />
</p>
<h3 id="44-输出门">4.4 输出门</h3>
<p>作用是输出当前时刻的结果和h<sub>t</sub>，这两者其实是一样的，也即hidden_state，隐藏状态.</p>
<p><img class="img-zoomable" src="/RNN/16.png" alt="png" />
</p>
<h3 id="45-api">4.5 API</h3>
<p><img class="img-zoomable" src="/RNN/19.png" alt="png" />
</p>
<p>实例化LSTM对象后，不仅要传入输入，还要传入上一次的隐藏状态h和前一次的memory C</p>
<p>LSTM的默认输出格式如下：</p>
<p><img class="img-zoomable" src="/RNN/20.png" alt="png" />
</p>
<p>这是默认的输出.也即batch_first为False的情况.要想输出的batch在seq_len前面，需要使batch_first为True.</p>
<p><strong>numdirection</strong>：当为双向LSTM时为2，否则为1.</p>
<p>对于输入值，格式为[<strong>batch, seq_len</strong>].经过word-emdbedding后，格式变成[<strong>batch, seq_len, n</strong>],n为word-emdbedding-dim</p>
<p>[<strong>seq_len, batch, n</strong>]，也即默认输入input的格式.</p>
<p>输入经过LSTM后，因为LSTM有hidden_size个单元.输入经过处理后由每个单元输出.所以输出的格式为[<strong>seq_len, batch, hidden_size*num_direction</strong>]</p>
<p>h和C的格式都是[<strong>num_layer*num_direction, batch, hidden_size</strong>].因为LSTM有hidden_size个单元，所以h(或者C)的长度也是hidden_size.每次有batch条数据，又有num_layer*num_direction个LSTM单元.所以得出这样的h和C的格式.</p>
<p><strong>output其实是将每一时间步上的结果在seq_len这一维度进行了拼接；h则是将不同层的隐藏状态在第个维度上进行了拼接</strong></p>
<p>代码：</p>
<pre><code>
import torch
import torch.nn as nn

batch_size = 10
seq_len = 20
num_embeddings = 100
embedding_dim = 30

input = torch.ones([batch_size,seq_len],dtype=torch.long)

#embedding
embedding = nn.Embedding(100,30)
input_embeded = embedding(input) #[batch_size,seq_len,embedding_dim] #[10,20,30]

#LSTM
lstm = nn.LSTM(input_size=embedding_dim,hidden_size=8,num_layers=1,batch_first=True,bidirectional=True)
# gru = nn.GRU(input_size=embedding_dim,hidden_size=8,num_layers=2,batch_first=True,bidirectional=False)

output,(h_n,c_n) = lstm(input_embeded)

print('*****')
print(output.size())  #[batch_Size,seq_len,8]
print(h_n.size())    #[2,batch_size,hidden_size]
print(c_n.size())

#获取最后一个时间步骤的输出：
#最上一层的输出：
# last_output = output[:,-1,:]
#
# #最上层的h_n:
# h_n_last = h_n[-1,:,:]
# print(last_output.eq(h_n_last))


#双向的LSTM中，最上层的正向的最后一个输出
o1 = output[:,-1,:8] #正向的最后一个输出
o2 = output[:,0,8:]  #反向最后一个输出

#双向LSTM中，最上层正向的h_n
h1 = h_n[-2,:,:] #最上层正向
h2 = h_n[-1,:,:] #最上层反向
print(o1.eq(h1))
print(o2.eq(h2))


</code></pre>
<h2 id="5-双向lstma-id5a">5. 双向LSTM<a id="5"></a></h2>
<p>在进行预测时，有时会需要从后面预测全面.这时就需要双向LSTM.</p>
<p>双向LSTM既有正向的LSTM又有反向的LSTM.两者的输出concat以后再代入一个LSTM（也可以是全连接层.接口中只提供前面的两个方向相反的LSTM）中进行计算.这样，输出就既有正向也有反向的记忆了.</p>
<p><img class="img-zoomable" src="/RNN/18.png" alt="png" />
</p>
<h2 id="6-grua-id6a">6. GRU<a id="6"></a></h2>
<p>GRU是LSTM的变形版本.将输入门和遗忘门组合成了更新门.同时合并了单元状态(<strong>C</strong>)和隐藏状态(<strong>h</strong>)并进行了一系列其他改进.模型比LSTM简化.</p>
<p><img class="img-zoomable" src="/RNN/17.png" alt="png" />
</p>
<h2 id="7-使用lstmgru改进文本情感分类模型a-id7a">7. 使用LSTM/GRU改进文本情感分类模型<a id="7"></a></h2>
<h3 id="71-注意事项">7.1 注意事项</h3>
<ol>
<li>
<p>第一次调用前要初始化隐藏状态，不然就会默认生成全为0的隐藏状态.</p>
</li>
<li>
<p>使用单向LSTM时一般使用output的最后一维结果(也就是h<sub>n</sub>)作为文本处理的结果.因为它也包含了前面所有的隐藏状态h.默认情况下，表现为output[-1::]</p>
</li>
<li>
<p>使用双向LSTM时一般使用<strong>每个方向</strong>output的最后一维结果.<code>torch.cat([h_n[-2,:,:], h_n[-1,:,:]], dim=-1)</code>.当然也可以用output但是比较麻烦.最后的形状为[batch_size, hidden_size*2]</p>
</li>
<li>
<p>如果要交换output中batch_size和seq_len的位置，不能用view等变形函数，而需要用permute交换0，1轴.<a href="https://blog.csdn.net/qian_5557/article/details/88644570?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-88644570-blog-120973298.pc_relevant_multi_platform_whitelistv3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-88644570-blog-120973298.pc_relevant_multi_platform_whitelistv3&amp;utm_relevant_index=1" target="_blank">两者区别</a></p>
</li>
</ol>
<h3 id="72-梯度消失和梯度爆炸">7.2 梯度消失和梯度爆炸</h3>
<p><strong>梯度消失</strong></p>
<p>有如下简单的神经网络：</p>
<p><img class="img-zoomable" src="/RNN/21.png" alt="png" />
</p>
<p>展开来就是这样的</p>
<p><img class="img-zoomable" src="/RNN/23.png" alt="png" />
</p>
<p>令y<sub>i</sub>=σ(z<sub>i</sub>) = σ(w<sub>i</sub>*x<sub>i</sub>+b<sub>i</sub>)</p>
<p>求输出C对参数b1的偏导.等于：</p>
<p><img class="img-zoomable" src="/RNN/22.png" alt="png" />
</p>
<p>σ是激活函数sigmod，其导数图像为：</p>
<p><img class="img-zoomable" src="/RNN/24.png" alt="png" />
</p>
<p>由图像可见，这个值最大为0.25</p>
<p>反向传播时，当层数足够多时，梯度一直乘以0.25，呈指数倍无限下降，直至出现消失的情况.</p>
<p><strong>梯度爆炸</strong></p>
<p>同上图所示求输出C对参数b1的偏导.当权重w很大时，结果会呈指数增加，导致爆炸.</p>
<p><strong>解决方法</strong></p>
<ol>
<li>
<p>选择relu等梯度大部分落在常数上的激活函数</p>
</li>
<li>
<p>改进梯度优化算法.如使用adam算法</p>
</li>
<li>
<p>使用<strong>batch normalization</strong></p>
</li>
</ol>
<h3 id="73-一些api">7.3 一些API</h3>
<ol>
<li>nn.BatchNormld</li>
</ol>
<p>批规范化.在每个batch训练的过程中归一化数据，以达到加速训练的目的.</p>
<p>以sigmod为例，当梯度接近0时更新幅度小，更新会很慢.使用归一化后会尽量将梯度拉到[0,1]的范围内，加速更新.</p>
<p>batchNormld一般在激活函数之后使用</p>
<p><img class="img-zoomable" src="/RNN/25.png" alt="png" />
</p>
<ol start="2">
<li>nn.Dropout</li>
</ol>
<p>对参数的随机失活.可以增加模型的稳健性，解决过拟合问题(增加泛化能力)，最后的模型可以理解为多个模型的组合结果，类似随机森林.</p>
<p><img class="img-zoomable" src="/RNN/26.png" alt="png" />
</p>
<h3 id="关于with-torchno_grad">关于<code>with torch.no_grad()</code>:</h3>
<p>with语句保证了不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。</p>
<h3 id="75-使用lstm优化模型">7.5 使用LSTM/优化模型</h3>
<p>区别于上次的只使用了word-embedding的模型，这次的模型只需要修改model.py并添加测试文件即可.</p>
<p>而使用了<code>with torch.no_grad</code>后，所有计算得出的tensor的requires_grad都自动设置为False，即使前面已经设置了requires_grad = True.</p>
<p>为什么要这样做？因为当测试的时候，直接导入模型进行训练就行，不需要再求导数反向传播，大大节省时间和内存</p>
<p><strong>model.py</strong></p>
<pre><code>&quot;&quot;&quot;构建模型&quot;&quot;&quot;
import torch.nn as nn
import config
import torch
import torch.nn.functional as F

class ImdbModel(nn.Module):
    def __init__(self):
        super(ImdbModel,self).__init__()
        self.embedding = nn.Embedding(num_embeddings=len(config.ws),embedding_dim=200,padding_idx=config.ws.PAD)
        self.lstm = nn.LSTM(input_size=200,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True,dropout=0.5)
        self.fc1 = nn.Linear(64*2,64)
        self.fc2 = nn.Linear(64,2)

    def forward(self, input):
        &quot;&quot;&quot;
        :param input:[batch_size,max_len]
        :return:
        &quot;&quot;&quot;
        input_embeded = self.embedding(input) #input embeded :[batch_size,max_len,200]

        output,(h_n,c_n) = self.lstm(input_embeded)  #  output：[batch_size, seq_len, 64*2];h_n :[batch_size,4,hidden_size]
        out = torch.cat([h_n[-1,:,:],h_n[-2,:,:]],dim=-1) #拼接正向最后一个输出和反向最后一个输出#out :[batch_size,hidden_size*2]

        #进行全连接
        out_fc1 = self.fc1(out)
        #进行relu
        out_fc1_relu = F.relu(out_fc1)

        #再经过一个全连接层全连接
        out_fc2= self.fc2(out_fc1_relu)  #out :[batch_size,2]
        return F.log_softmax(out_fc2,dim=-1)

</code></pre>
<p><strong>eval.py</strong></p>
<pre><code>&quot;&quot;&quot;进行模型的评估&quot;&quot;&quot;

import config
from model import  ImdbModel
from dataset import get_dataloader
import torch
import torch.nn.functional as F
import numpy as np


def eval():
    model = ImdbModel().to(config.device)
    model.load_state_dict(torch.load(&quot;./models/model.pkl&quot;))

    loss_list = []  # 保存每一次的损失
    acc_list = []
    test_dataloader = get_dataloader(train=False)
    with torch.no_grad():
        for idx,(input,target) in enumerate(test_dataloader):
            input = input.to(config.device)
            target = target.to(config.device)
            output = model(input)
            loss = F.nll_loss(output,target)
            loss_list.append(loss.item())

            #准确率
            pred = output.max(dim=-1)[-1]
            acc_list.append(pred.eq(target).cpu().float().mean())
    print(&quot;loss mean:{},acc mean:{}&quot;.format(np.mean(loss_list),np.mean(acc_list)))


if __name__ == '__main__':
    eval()

</code></pre>
<h3 id="76-模型优化方法">7.6 模型优化方法.</h3>
<ol>
<li>调参.</li>
</ol>
<p>包括增加embedding_dim的维度；增加LSTM的hidden_size=64,num_layers=2等等.</p>
<ol start="2">
<li>添加一个新的全连接层作为输出层，激活函数处理.</li>
</ol>
<pre><code>#再经过一个全连接层全连接
out_fc2= self.fc2(out_fc1_relu)  #out :[batch_size,2]
return F.log_softmax(out_fc2,dim=-1)

</code></pre>
<ol start="3">
<li>将双向LSTM的输出带入到一个新的LSTM中</li>
</ol>
<h2 id="8-总结a-id8a">8. 总结<a id="8"></a></h2>
<p>更深的理解了正向传播和反向传播，初步理解了RNN以及其变体LSTM。</p>
<p>对于LSTM，学习了其API的调用还有一些其他可以优化模型的API调用.</p>
<p>对梯度消失和梯度爆炸以及他们的解决方法进行了学习</p>
<p>最后，通过上述知识，改机了前面的文本情感分类模型</p>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">Pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#1-序a-id1a">1. 序<a id="1"></a></a></li>
    <li><a href="#2-反向传播a-id2a">2. 反向传播<a id="2"></a></a>
      <ul>
        <li><a href="#21-正向传播">2.1 正向传播</a></li>
        <li><a href="#22-非输出层的反向传播">2.2 非输出层的反向传播</a></li>
        <li><a href="#23-输出层的反向传播">2.3 输出层的反向传播</a></li>
        <li><a href="#24-改进batch批量计算">2.4 改进：batch批量计算</a></li>
      </ul>
    </li>
    <li><a href="#3-rnn的不同表示和功能a-id3a">3. RNN的不同表示和功能<a id="3"></a></a></li>
    <li><a href="#4-lstma-id4a">4. LSTM<a id="4"></a></a>
      <ul>
        <li><a href="#41-基本原理">4.1 基本原理</a></li>
        <li><a href="#42-遗忘门">4.2 遗忘门</a></li>
        <li><a href="#43-输入门">4.3 输入门</a></li>
        <li><a href="#44-输出门">4.4 输出门</a></li>
        <li><a href="#45-api">4.5 API</a></li>
      </ul>
    </li>
    <li><a href="#5-双向lstma-id5a">5. 双向LSTM<a id="5"></a></a></li>
    <li><a href="#6-grua-id6a">6. GRU<a id="6"></a></a></li>
    <li><a href="#7-使用lstmgru改进文本情感分类模型a-id7a">7. 使用LSTM/GRU改进文本情感分类模型<a id="7"></a></a>
      <ul>
        <li><a href="#71-注意事项">7.1 注意事项</a></li>
        <li><a href="#72-梯度消失和梯度爆炸">7.2 梯度消失和梯度爆炸</a></li>
        <li><a href="#73-一些api">7.3 一些API</a></li>
        <li><a href="#关于with-torchno_grad">关于<code>with torch.no_grad()</code>:</a></li>
        <li><a href="#75-使用lstm优化模型">7.5 使用LSTM/优化模型</a></li>
        <li><a href="#76-模型优化方法">7.6 模型优化方法.</a></li>
      </ul>
    </li>
    <li><a href="#8-总结a-id8a">8. 总结<a id="8"></a></a></li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">Pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#1-序a-id1a">1. 序<a id="1"></a></a></li>
    <li><a href="#2-反向传播a-id2a">2. 反向传播<a id="2"></a></a>
      <ul>
        <li><a href="#21-正向传播">2.1 正向传播</a></li>
        <li><a href="#22-非输出层的反向传播">2.2 非输出层的反向传播</a></li>
        <li><a href="#23-输出层的反向传播">2.3 输出层的反向传播</a></li>
        <li><a href="#24-改进batch批量计算">2.4 改进：batch批量计算</a></li>
      </ul>
    </li>
    <li><a href="#3-rnn的不同表示和功能a-id3a">3. RNN的不同表示和功能<a id="3"></a></a></li>
    <li><a href="#4-lstma-id4a">4. LSTM<a id="4"></a></a>
      <ul>
        <li><a href="#41-基本原理">4.1 基本原理</a></li>
        <li><a href="#42-遗忘门">4.2 遗忘门</a></li>
        <li><a href="#43-输入门">4.3 输入门</a></li>
        <li><a href="#44-输出门">4.4 输出门</a></li>
        <li><a href="#45-api">4.5 API</a></li>
      </ul>
    </li>
    <li><a href="#5-双向lstma-id5a">5. 双向LSTM<a id="5"></a></a></li>
    <li><a href="#6-grua-id6a">6. GRU<a id="6"></a></a></li>
    <li><a href="#7-使用lstmgru改进文本情感分类模型a-id7a">7. 使用LSTM/GRU改进文本情感分类模型<a id="7"></a></a>
      <ul>
        <li><a href="#71-注意事项">7.1 注意事项</a></li>
        <li><a href="#72-梯度消失和梯度爆炸">7.2 梯度消失和梯度爆炸</a></li>
        <li><a href="#73-一些api">7.3 一些API</a></li>
        <li><a href="#关于with-torchno_grad">关于<code>with torch.no_grad()</code>:</a></li>
        <li><a href="#75-使用lstm优化模型">7.5 使用LSTM/优化模型</a></li>
        <li><a href="#76-模型优化方法">7.6 模型优化方法.</a></li>
      </ul>
    </li>
    <li><a href="#8-总结a-id8a">8. 总结<a id="8"></a></a></li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2022
                <a href="https://wudaore.github.io/">DSRKafuU</a>
                 | <a href="https://github.com/dsrkafuu/hugo-theme-fuji">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>



</body>

</html>
