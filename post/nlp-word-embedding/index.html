<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.98.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>NLP--word embedding - wudao的博客</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="深度学习, NLP" />


<meta property="og:title" content="NLP--word embedding" />
<meta name="twitter:title" content="NLP--word embedding" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wudaore.github.io/post/nlp-word-embedding/" /><meta property="og:description" content="目录 序 NLP基础 文本情感分类 总结 1. 序 NLP&ndash;word embedding的学习 2. RNN-NLP基础 2.1 N-gram N表示能够用在一起的词语的数量.使用N-gram模型时，往往将N个词语当成一个单位使用 区别于传统分词， N-gram" />
<meta name="twitter:description" content="目录 序 NLP基础 文本情感分类 总结 1. 序 NLP&ndash;word embedding的学习 2. RNN-NLP基础 2.1 N-gram N表示能够用在一起的词语的数量.使用N-gram模型时，往往将N个词语当成一个单位使用 区别于传统分词， N-gram" /><meta property="og:image" content="https://wudaore.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://wudaore.github.io/img/og.png" /><meta property="article:published_time" content="2022-07-14T00:00:00+08:00" /><meta property="article:modified_time" content="2022-07-14T00:00:00+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://wudaore.github.io/assets/css/fuji.min.css" />








</head>

<body
  data-theme="auto"
  data-theme-auto='true'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://wudaore.github.io/">wudao的博客</a>
            
            <span class="title-sub">钝学累功,不妨精熟</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://wudaore.github.io/post/nlp-word-embedding/">NLP--word embedding</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-07-14</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;3061 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>&nbsp;<a href="/tags/nlp">NLP</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h1 id="目录">目录</h1>
<h4 id="序1"><a href="#1">序</a></h4>
<h4 id="nlp基础2"><a href="#2">NLP基础</a></h4>
<h4 id="文本情感分类3"><a href="#3">文本情感分类</a></h4>
<h4 id="总结conclusion"><a href="#conclusion">总结</a></h4>
<h2 id="1-序a-id1a">1. 序<a id="1"></a></h2>
<p>NLP&ndash;word embedding的学习</p>
<h2 id="2-rnn-nlp基础a-id2a">2. RNN-NLP基础<a id="2"></a></h2>
<h3 id="21-n-gram">2.1 N-gram</h3>
<p>N表示能够用在一起的词语的数量.使用N-gram模型时，往往将N个词语当成一个单位使用</p>
<p>区别于传统分词， N-gram考虑了词语间的顺序，比传统分词效果更好</p>
<h3 id="21-文本向量化">2.1 文本向量化</h3>
<p><strong>one-hot encoding</strong>：</p>
<p>在独热编码中，每一个token（词语）都被一个长度为N的向量表示，其中N代表词典长度.</p>
<p>不可避免的，这么做会导致矩阵<strong>稀疏</strong>，占用空间大</p>
<p><strong>word embedding</strong>：</p>
<p>区别于独热编码，word embedding的维度由人为规定而不取决于词典长度.矩阵中每个向量的值都是一个超参数，是随机生成的，之后会在学习过程中训练而获得.</p>
<p>在这过程中，模型会将token转换为数字，再将数字转换为向量.</p>
<p>距离:维度为4的token的word embedding过程：首先训练集的形状为[batch_size,N],然后经过转化，变为[batch_size,N,4]的形状.</p>
<p><img class="img-zoomable" src="/NLPB/17.png" alt="png" />
</p>
<p>因此，word embedding使用一个<strong>稠密</strong>矩阵来表示token.</p>
<p>pytorch的API如下：</p>
<p><img class="img-zoomable" src="/NLPB/18.png" alt="png" />
</p>
<h2 id="3-实战-文本情感分类a-id3a">3. 实战-文本情感分类<a id="3"></a></h2>
<h3 id="31-数据的预处理--数据类的定义">3.1 数据的预处理&ndash;数据类的定义</h3>
<p>训练和测试的文本，都放在文件夹aclImdb中.其中都包括pos和neg两个文件夹，分别表示消极文本和积极文本.</p>
<p>对于每个文本，下划线之前的是编号，下划线之后的是评分.大于5分被视为积极，小于5则被视为消极.</p>
<p>根据之前的学习我们知道，要导入数据集需要先定义一个数据类，继承于from torch.utils.data.Dataset.</p>
<p>这个类要复写三个方法.</p>
<p><strong>首先是__init__，初始化方法.可以在该方法中加入每个文件的路径.</strong></p>
<pre><code>    def __init__(self,train=True):
        # super(ImdbDataset,self).__init__()
        data_path = r&quot;D:\BaiduNetdiskDownload\阶段9-人工智能NLP项目\第四天\代码\data\aclImdb&quot;
        data_path += r&quot;\train&quot; if train else r&quot;\test&quot;
        self.total_path = []  #保存所有的文件路径
        for temp_path in [r&quot;\pos&quot;,r&quot;\neg&quot;]:
            cur_path = data_path + temp_path
            self.total_path += [os.path.join(cur_path,i) for i in os.listdir(cur_path) if i.endswith(&quot;.txt&quot;)]
</code></pre>
<p><strong>其次是__getitem__方法.用于通过下标取出对应的数据.本样例中，需要返回分词后的文本和标签.分词需要经过去除无必要字符等操作.分词和复写后如下：</strong></p>
<pre><code># 分词
&quot;&quot;&quot;
实现额外的方法
&quot;&quot;&quot;
import re

def tokenlize(sentence):
    &quot;&quot;&quot;
    进行文本分词
    :param sentence: str
    :return: [str,str,str]
    &quot;&quot;&quot;

    fileters = ['!', '&quot;', '#', '$', '%', '&amp;', '\(', '\)', '\*', '\+', ',', '-', '\.', '/', ':', ';', '&lt;', '=', '&gt;',
                '\?', '@', '\[', '\\', '\]', '^', '_', '`', '\{', '\|', '\}', '~', '\t', '\n', '\x97', '\x96', '”', '“', ]
    sentence = sentence.lower() #把大写转化为小写
    sentence = re.sub(&quot;&lt;br /&gt;&quot;,&quot; &quot;,sentence)
    # sentence = re.sub(&quot;I'm&quot;,&quot;I am&quot;,sentence)
    # sentence = re.sub(&quot;isn't&quot;,&quot;is not&quot;,sentence)
    # re.sub用竖线隔开代表同时替换
    sentence = re.sub(&quot;|&quot;.join(fileters),&quot; &quot;,sentence)
    result = [i for i in sentence.split(&quot; &quot;) if len(i)&gt;0]

    return result


</code></pre>
<pre><code>    # 复写
    def __getitem__(self, idx):
        file = self.total_path[idx]
        review = utils.tokenlize(open(file, encoding='utf-8').read()) #评论
        label = int(file.split(&quot;_&quot;)[-1].split(&quot;.&quot;)[0])
        label = 0 if label &lt;5 else 1
        return review,label
</code></pre>
<p><strong>最后是__len__方法，用于统计数据集的长度.复写后如下：</strong></p>
<pre><code>    def __len__(self):
        return len(self.total_path)
</code></pre>
<h3 id="32-数据的预处理--dataloader">3.2 数据的预处理&ndash;dataloader</h3>
<p>定义数据类后，需要再定义Dataloader对象来存储数据.</p>
<p>运行以下代码</p>
<pre><code>dataset = ImdbDataset(train)
batch_size = config.train_batch_size if train else config.test_batch_size
return DataLoader(dataset,batch_size=batch_size,shuffle=True)
</code></pre>
<p>会发现如下报错：</p>
<pre><code>RuntimeError: each element in list of batch should be of equal size
</code></pre>
<p>这是因为dataset中的每个文本分词后长度不一致，而DataLoader需要长度一致的文本.</p>
<p>修改代码，定义collate_fn方法.这个方法用于对每个batch进行处理，最终输出一个batch的返回值：</p>
<pre><code>def collate_fn(batch):
    &quot;&quot;&quot;
    对batch数据进行处理
    :param batch: [一个getitem的结果，getitem的结果,getitem的结果]
    :return: 元组
    &quot;&quot;&quot;
    reviews,labels = zip(*batch)
    reviews = torch.LongTensor([config.ws.transform(i,max_len=config.max_len) for i in reviews])
    labels = torch.LongTensor(labels)

    return reviews,labels


def get_dataloader(train=True):
    dataset = ImdbDataset(train)
    batch_size = config.train_batch_size if train else config.test_batch_size
    return DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)

</code></pre>
<p><strong>经过collate_fn处理前的batch:([token, label], [token, label], [token, label]&hellip;)</strong></p>
<p><strong>经过collate_fn处理后的batch:([token, token, token&hellip;], [label, label, label])</strong></p>
<h3 id="33-文本序列化">3.3 文本序列化</h3>
<p>思路分析：</p>
<ol>
<li>
<p>对词语进行分词</p>
</li>
<li>
<p>分词后存入词典，并根据次数进行过滤，并统计次数</p>
</li>
<li>
<p>实现文本转数字序列</p>
</li>
<li>
<p>实现数字序列转文本</p>
</li>
</ol>
<p>需要解决的问题：</p>
<ol>
<li>
<p>每个batch的长度不同，需要对短句子进行填充.代码中用&rsquo;UKN&rsquo;表示</p>
</li>
<li>
<p>测试集冲出现新的词语，需要用特殊字符代理.代码中用&rsquo;PAD&rsquo;表示</p>
</li>
</ol>
<p>具体代码如下：</p>
<pre><code>&quot;&quot;&quot;
文本序列化
&quot;&quot;&quot;

class WordSequence:
    UNK_TAG = &quot;&lt;UNK&gt;&quot;  #表示未知字符
    PAD_TAG = &quot;&lt;PAD&gt;&quot;  #填充符
    PAD = 0
    UNK = 1

    def __init__(self):
        self.dict = {   #保存词语和对应的数字
            self.UNK_TAG:self.UNK,
            self.PAD_TAG:self.PAD
        }
        self.count = {}  #统计词频的


    def fit(self,sentence):
        &quot;&quot;&quot;
        接受句子，统计词频
        :param sentence:[str,str,str]
        :return:None
        &quot;&quot;&quot;
        for word in sentence:
            self.count[word] = self.count.get(word,0)  + 1  #所有的句子fit之后，self.count就有了所有词语的词频

    def build_vocab(self,min_count=5,max_count=None,max_features=None):
        &quot;&quot;&quot;
        根据条件构造 词典
        :param min_count:最小词频
        :param max_count: 最大词频
        :param max_features: 最大保留的词语数
        :return:
        &quot;&quot;&quot;
        if min_count is not None:
            self.count = {word:count for word,count in self.count.items() if count &gt;= min_count}
        if max_count is not None:
            self.count = {word:count for word,count in self.count.items() if count &lt;= max_count}
        if max_features is not None:
            # 先根据词频进行排序 保留前max_features个
            self.count = dict(sorted(self.count.items(),lambda x:x[-1],reverse=True)[:max_features])

        for word in self.count:
            self.dict[word]  = len(self.dict)  #每次word对应一个数字

        #把dict进行翻转
        self.inverse_dict = dict(zip(self.dict.values(),self.dict.keys()))

    def transform(self,sentence,max_len=None):
        &quot;&quot;&quot;
        把句子转化为数字序列
        :param sentence:[str,str,str]
        :return: [int,int,int]
        &quot;&quot;&quot;
        if len(sentence) &gt; max_len:
            sentence = sentence[:max_len]
        else:
            sentence = sentence + [self.PAD_TAG] *(max_len- len(sentence))  #填充PAD

        return [self.dict.get(i,1) for i in sentence]

    def inverse_transform(self,incides):
        &quot;&quot;&quot;
        把数字序列转化为字符
        :param incides: [int,int,int]
        :return: [str,str,str]
        &quot;&quot;&quot;
        # dict.get(a,b):a是键值key，如果存在dict存在键值a，则函数返回dict[a]；否则返回b，如果没有定义b参数，则返回None。
        return [self.inverse_dict.get(i,&quot;&lt;UNK&gt;&quot;) for i in incides]

    def __len__(self):
        return len(self.dict)

if __name__ == '__main__':
    # 保存模型
    import pickle
    import os
    from tqdm import tqdm
    from utils import tokenlize
    ws = WordSequence()
    path = r'D:\BaiduNetdiskDownload\阶段9-人工智能NLP项目\第四天\代码\data\aclImdb\train'
    temp_data_path = [os.path.join(path, &quot;pos&quot;), os.path.join(path, &quot;neg&quot;)]
    for data_path in temp_data_path:
        file_paths = [os.path.join(data_path, file_name) for file_name in os.listdir(data_path) if file_name.endswith('txt')]
        for file_path in tqdm(file_paths):
            sentence = tokenlize(open(file_path, encoding='utf-8').read())
            ws.fit(sentence)
    ws.build_vocab(min_count=10)
    pickle.dump(ws, open(&quot;./models/myws.pkl&quot;, &quot;wb&quot;))
    # torch.save(ws, &quot;./models/ws2.pkl&quot;)
    # ws = torch.load(&quot;./models/ws2.pkl&quot;)
    print(len(ws))
    pass

</code></pre>
<h3 id="34-构建模型">3.4 构建模型</h3>
<p>只练习word_embedding，所以模型只有一层.数据通过word_embedding后通弄个全连接层，计算log_softmax</p>
<p>对于线性模型，在__init__方法中定义一些函数，在forword中定义详细的处理过程.</p>
<pre><code>&quot;&quot;&quot;构建模型&quot;&quot;&quot;
import torch.nn as nn
import config
import torch.nn.functional as F
import pickle
import dataset
from word_sequece import WordSequence



class ImdbModel(nn.Module):
    def __init__(self):
        super(ImdbModel,self).__init__()
        # padding_idx:短句填充
        self.embedding = nn.Embedding(num_embeddings=len(config.ws),embedding_dim=200,padding_idx=config.ws.PAD)
        # 全连接层，将config.max_len*200转化为2维，即二分类
        self.fc = nn.Linear(config.max_len*200,2)

    def forward(self, input):
        &quot;&quot;&quot;
        :param input:[batch_size,max_len]
        :return:
        &quot;&quot;&quot;
        # word_embedding
        input_embeded = self.embedding(input) #input embeded :[batch_size,max_len,200]

        #变形.batch_size不能变，而全连接层不改变形状.因为上面定义了self.fc = nn.Linear(config.max_len*200,2)，所以也要将输入变形为2维
        input_embeded_viewed = input_embeded.view(input_embeded.size(0),-1)

        #全连接
        out = self.fc(input_embeded_viewed)
        return F.log_softmax(out,dim=-1)
</code></pre>
<h3 id="35-构建训练">3.5 构建训练</h3>
<p>具体过程依然是：</p>
<p>1.实例化模型，将模型设置为训练模式</p>
<p>2.实例化优化器类，实例化损失函数</p>
<p>3.获取，并遍历dataloader</p>
<p>4.梯度置为0</p>
<p>5.进行向前计算</p>
<p>6.计算损失</p>
<p>7.反向传播</p>
<p>8.更新参数.</p>
<pre><code>&quot;&quot;&quot;进行模型的训练&quot;&quot;&quot;
import config
from model import  ImdbModel
from dataset import get_dataloader
from torch.optim import Adam
from tqdm import tqdm
import torch.nn.functional as F

model = ImdbModel()
optimizer = Adam(model.parameters())


def train(epoch):
    train_dataloader = get_dataloader(train=True)
    bar = tqdm(train_dataloader,total=len(train_dataloader))
    for idx,(input,target) in enumerate(bar):
        optimizer.zero_grad()
        output = model(input)
        loss = F.nll_loss(output,target)
        loss.backward()
        optimizer.step()
        bar.set_description(&quot;epcoh:{}  idx:{}   loss:{:.6f}&quot;.format(epoch,idx,loss.item()))


if __name__ == '__main__':
    for i in range(10):
        train(i)
</code></pre>
<h2 id="4-总结a-idconclusiona">4. 总结<a id="conclusion"></a></h2>
<p>本章讲解了word_embedding的基本原理，API的实现并进行了关于情感分析的实战</p>
<p>实战过程中，在文本序列化时，需要使用pickle保存训练好的模型，以方便下次直接调用.</p>
<p>必须注意的是，使用pickle保存模型时，需要在原文件的主函数中导入自己的模型.因为不这么做的话，后面的文件在使用该模型时会不知道按照什么方式反序列化</p>
<p>如：在word_sequence.py的主函数中，我对该文件内的类进行了导入后再pickle.</p>
<p>这个问题把我从下午困惑到了晚上.找了很多方法都不尽如人意.现在终于解决了.</p>
<pre><code>if __name__ == '__main__':
    import pickle
    import os
    from tqdm import tqdm
    from utils import tokenlize
    # 在主函数中导入类再pickle
    from word_sequece import WordSequence
    ws = WordSequence()
    path = r'D:\BaiduNetdiskDownload\阶段9-人工智能NLP项目\第四天\代码\data\aclImdb\train'
    temp_data_path = [os.path.join(path, &quot;pos&quot;), os.path.join(path, &quot;neg&quot;)]
    for data_path in temp_data_path:
        file_paths = [os.path.join(data_path, file_name) for file_name in os.listdir(data_path) if file_name.endswith('txt')]
        for file_path in tqdm(file_paths):
            sentence = tokenlize(open(file_path, encoding='utf-8').read())
            ws.fit(sentence)
    ws.build_vocab(min_count=10)
    pickle.dump(ws, open(&quot;./models/finalws.pkl&quot;, &quot;wb&quot;))
    print(len(ws))
</code></pre>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-序a-id1a">1. 序<a id="1"></a></a></li>
    <li><a href="#2-rnn-nlp基础a-id2a">2. RNN-NLP基础<a id="2"></a></a>
      <ul>
        <li><a href="#21-n-gram">2.1 N-gram</a></li>
        <li><a href="#21-文本向量化">2.1 文本向量化</a></li>
      </ul>
    </li>
    <li><a href="#3-实战-文本情感分类a-id3a">3. 实战-文本情感分类<a id="3"></a></a>
      <ul>
        <li><a href="#31-数据的预处理--数据类的定义">3.1 数据的预处理&ndash;数据类的定义</a></li>
        <li><a href="#32-数据的预处理--dataloader">3.2 数据的预处理&ndash;dataloader</a></li>
        <li><a href="#33-文本序列化">3.3 文本序列化</a></li>
        <li><a href="#34-构建模型">3.4 构建模型</a></li>
        <li><a href="#35-构建训练">3.5 构建训练</a></li>
      </ul>
    </li>
    <li><a href="#4-总结a-idconclusiona">4. 总结<a id="conclusion"></a></a></li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/439482884/" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algo/">algo</a>
            </span>
            
            <span>
                <a href="/tags/css/">css</a>
            </span>
            
            <span>
                <a href="/tags/linux/">linux</a>
            </span>
            
            <span>
                <a href="/tags/markdown/">markdown</a>
            </span>
            
            <span>
                <a href="/tags/nlp/">NLP</a>
            </span>
            
            <span>
                <a href="/tags/others/">others</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/test/">test</a>
            </span>
            
            <span>
                <a href="/tags/text/">text</a>
            </span>
            
            <span>
                <a href="/tags/web/">web</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#1-序a-id1a">1. 序<a id="1"></a></a></li>
    <li><a href="#2-rnn-nlp基础a-id2a">2. RNN-NLP基础<a id="2"></a></a>
      <ul>
        <li><a href="#21-n-gram">2.1 N-gram</a></li>
        <li><a href="#21-文本向量化">2.1 文本向量化</a></li>
      </ul>
    </li>
    <li><a href="#3-实战-文本情感分类a-id3a">3. 实战-文本情感分类<a id="3"></a></a>
      <ul>
        <li><a href="#31-数据的预处理--数据类的定义">3.1 数据的预处理&ndash;数据类的定义</a></li>
        <li><a href="#32-数据的预处理--dataloader">3.2 数据的预处理&ndash;dataloader</a></li>
        <li><a href="#33-文本序列化">3.3 文本序列化</a></li>
        <li><a href="#34-构建模型">3.4 构建模型</a></li>
        <li><a href="#35-构建训练">3.5 构建训练</a></li>
      </ul>
    </li>
    <li><a href="#4-总结a-idconclusiona">4. 总结<a id="conclusion"></a></a></li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2022
                <a href="https://wudaore.github.io/">DSRKafuU</a>
                 | <a href="https://github.com/dsrkafuu/hugo-theme-fuji">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>



</body>

</html>
